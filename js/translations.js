diff --git a/js/translations.js b/js/translations.js
index c78d6d3eb5877c26b726f6e90d928d5e01b47cb9..b83f68733b82463887ea891c1648fa86671cf26a 100644
--- a/js/translations.js
+++ b/js/translations.js
@@ -117,54 +117,57 @@ const translations = {
         qLearningKeyDiff: "Der entscheidende Unterschied ist der Term $\\max_{a'} Q(S_{t+1}, a')$, der den maximalen $Q$-Wert im Folgezustand verwendet, was der Aktion entspricht, die eine gierige Policy wählen würde (Zielpolicy).",
         doubleQLearningTitle: "Double Q-Learning",
         doubleQLearningDesc: "Standard Q-Learning kann unter bestimmten Umständen zu einer Überschätzung der Q-Werte führen (Maximierungs-Bias), da der $\\max$-Operator sowohl zur Auswahl als auch zur Bewertung der Aktion verwendet wird. Double Q-Learning adressiert dies, indem es zwei separate Q-Wert-Schätzungen ($Q_A$ und $Q_B$) pflegt. Eine wird zur Auswahl der besten Aktion im nächsten Zustand verwendet, die andere zur Bewertung dieser Aktion.",
         doubleQLearningUpdate: "Z.B. Update für $Q_A$ (mit 50% Wahrscheinlichkeit): $Q_A(S_t,A_t) \\leftarrow Q_A(S_t,A_t) + \\alpha [R_{t+1} + \\gamma Q_B(S_{t+1}, \\text{argmax}_{a'} Q_A(S_{t+1},a')) - Q_A(S_t,A_t)]$ (und umgekehrt für $Q_B$).",
         onOffPolicyCompareTitle: "Vergleich der Update-Pfade: SARSA vs. Q-Learning",
         sarsaPathTitle: "SARSA (On-Policy)",
         sarsaPathDesc: "Update für $Q(S_t, A_t)$ verwendet $Q(S_{t+1}, A_{t+1})$. Lernt den Wert der tatsächlich ausgeführten Policy (inkl. Exploration).",
         qLearningPathTitle: "Q-Learning (Off-Policy)",
         qLearningPathDesc: "Update für $Q(S_t, A_t)$ verwendet $\\max_{a'} Q(S_{t+1}, a')$. Lernt den Wert der optimalen (gierigen) Policy, auch wenn die Aktionen anders (z.B. explorativ) gewählt wurden.",
         epsilonGreedyTitle: "Bedeutung von $\\epsilon$-greedy Exploration:",
         epsilonGreedyDesc: "Um sicherzustellen, dass der Agent alle relevanten Zustands-Aktions-Paare erkundet und nicht in lokalen Optima stecken bleibt, wird oft eine $\\epsilon$-greedy Strategie verwendet. Mit einer kleinen Wahrscheinlichkeit $\\epsilon$ wählt der Agent eine zufällige Aktion, anstatt der aktuell als optimal erachteten (gierigen) Aktion. Der Wert von $\\epsilon$ wird oft im Laufe des Trainings reduziert (Annealing), um von Exploration zu Exploitation überzugehen.",
         vfaTitle: "Ausblick: Wertfunktionsapproximation (VFA)",
         vfaDesc: "Für Probleme mit sehr großen oder kontinuierlichen Zustandsräumen sind tabellarische Darstellungen von $Q(s,a)$ (eine Tabelle für jedes Zustands-Aktions-Paar) nicht mehr praktikabel. Hier kommt die Wertfunktionsapproximation ins Spiel: Anstatt die Q-Werte exakt zu speichern, werden sie durch eine parametrisierte Funktion approximiert, z.B. eine lineare Funktion $\\hat{q}(s,a;\\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{x}(s,a)$ oder ein neuronales Netz $Q(s,a; \\mathbf{w}) \\approx Q^*(s,a)$. Die Parameter $\\mathbf{w}$ werden dann gelernt, oft mittels Gradientenverfahren.",
        vfaExample: "Ein bekanntes Beispiel ist das Deep Q-Network (DQN), das ein tiefes neuronales Netz zur Approximation der Q-Funktion verwendet und bemerkenswerte Erfolge in komplexen Umgebungen wie Atari-Spielen erzielt hat. Dies eröffnet die Anwendung von RL auf Probleme mit hochdimensionalen sensorischen Eingaben.",
        vfaCheat1: "Tabellarische Methoden scheitern bei sehr großen Zustandsräumen.",
        vfaCheat2: "Nur Parameter werden gespeichert; Konvergenz nicht garantiert.",
        vfaCheat3: "Lineare Approximation mit Feature-Vektoren, z.B. Polynombasis.",
        vfaCheat4: "Deep Q-Networks nutzen Experience Replay.",
        vfaCheat5: "Target Networks stabilisieren die Updates.",
         quizSectionTitle: "Wissens-Quiz zur Zwischenprüfung",
         quizJsonLoading: "Lade Quizdaten...",
         quizLoading: "Frage wird geladen...",
         quiz_nextButton: "Nächste Frage",
         quiz_resultsButton: "Ergebnisse anzeigen",
         quiz_finishedTitle: "Quiz beendet!",
         quiz_scorePrefix: "Dein Ergebnis: ",
         quiz_scoreSuffix_singular: "Punkt",
         quiz_scoreSuffix_plural: "Punkte",
         quiz_scoreOutOf: "von",
         quiz_restartButton: "Quiz neu starten",
+        quiz_startButton: "Quiz starten",
         quiz_feedbackCorrect: "Richtig!",
         quiz_feedbackIncorrectPrefix: "Falsch. Die richtige Antwort war: ",
         quiz_questionCounterPrefix: "Frage ",
         quiz_questionCounterOf: "von",
+        quiz_timerLabel: "Zeit:",
+        quiz_finalTimeLabel: "Benötigte Zeit:",
         footerText1: "&copy; 2025 Detaillierte Reinforcement Learning Infografik.",
         footerText2: "Basierend auf Vorlesungsmaterialien und Standard-RL-Konzepten.",
         chart_mcVsTd_title: "Vergleich: MC vs. TD Eigenschaften (Skala 1=Niedrig/Nein, 5=Hoch/Ja)",
         chart_mcVsTd_dataset_mc: "Monte Carlo (MC)",
         chart_mcVsTd_dataset_td: "Temporal Difference (TD)",
         chart_mcVsTd_label_bias: "Bias",
         chart_mcVsTd_label_variance: "Varianz",
         chart_mcVsTd_label_updateFreq: "Update Häufigkeit",
         chart_mcVsTd_label_incompleteEp: "Nutzung unvollst. Episoden",
         chart_mcVsTd_label_initSens: "Sensitivität Initialisierung",
         chart_mcVsTd_yAxis_lowNo: "1 (Niedrig/Nein)",
         chart_mcVsTd_yAxis_medium: "3 (Mittel)",
         chart_mcVsTd_yAxis_highYes: "5 (Hoch/Ja)",
         tooltip_bias_low_mc: "Niedrig (Unverzerrt)",
         tooltip_bias_medium_td: "Mittel (Verzerrt)",
         tooltip_variance_high_mc: "Hoch",
         tooltip_variance_low_td: "Niedrig",
         tooltip_update_episode_mc: "Am Episodenende",
         tooltip_update_step_td: "Nach jedem Schritt",
         tooltip_incomplete_no_mc: "Nein",
         tooltip_incomplete_yes_td: "Ja (Bootstrapping)",
         tooltip_init_insensitive_mc: "Unempfindlich",
         tooltip_init_sensitive_td: "Empfindlich",
        quiz_numQuestionsLabel: "Anzahl der Fragen:",
        quiz_numQuestionsAll: "Alle",
        quiz_reviewTitle: "Fehleranalyse",
        quiz_reviewQuestion: "Frage:",
        quiz_reviewYourAnswer: "Deine Antwort:",
        quiz_reviewCorrectAnswer: "Richtige Antwort:",
        quiz_reviewExplanation: "Erläuterung:",
        quiz_reviewIncorrectTag: "(Falsch)",
diff --git a/js/translations.js b/js/translations.js
index c78d6d3eb5877c26b726f6e90d928d5e01b47cb9..b83f68733b82463887ea891c1648fa86671cf26a 100644
--- a/js/translations.js
+++ b/js/translations.js
@@ -303,54 +306,57 @@ const translations = {
         qLearningKeyDiff: "The key difference is the term $\\max_{a'} Q(S_{t+1}, a')$, which uses the maximum $Q$-value in the next state, corresponding to the action a greedy policy would choose (target policy).",
         doubleQLearningTitle: "Double Q-Learning",
         doubleQLearningDesc: "Standard Q-Learning can, under certain circumstances, lead to an overestimation of Q-values (maximization bias), as the $\\max$ operator is used for both selecting and evaluating an action. Double Q-Learning addresses this by maintaining two separate Q-value estimates ($Q_A$ and $Q_B$). One is used to select the best action in the next state, and the other is used to evaluate that action.",
         doubleQLearningUpdate: "E.g., Update for $Q_A$ (with 50% probability): $Q_A(S_t,A_t) \\leftarrow Q_A(S_t,A_t) + \\alpha [R_{t+1} + \\gamma Q_B(S_{t+1}, \\text{argmax}_{a'} Q_A(S_{t+1},a')) - Q_A(S_t,A_t)]$ (and vice versa for $Q_B$).",
         onOffPolicyCompareTitle: "Comparison of Update Paths: SARSA vs. Q-Learning",
         sarsaPathTitle: "SARSA (On-Policy)",
         sarsaPathDesc: "Update for $Q(S_t, A_t)$ uses $Q(S_{t+1}, A_{t+1})$. Learns the value of the policy actually being executed (incl. exploration).",
         qLearningPathTitle: "Q-Learning (Off-Policy)",
         qLearningPathDesc: "Update for $Q(S_t, A_t)$ uses $\\max_{a'} Q(S_{t+1}, a')$. Learns the value of the optimal (greedy) policy, even if actions were chosen differently (e.g., exploratively).",
         epsilonGreedyTitle: "Importance of $\\epsilon$-greedy Exploration:",
         epsilonGreedyDesc: "To ensure the agent explores all relevant state-action pairs and doesn't get stuck in local optima, an $\\epsilon$-greedy strategy is often used. With a small probability $\\epsilon$, the agent chooses a random action instead of the one currently considered optimal (greedy). The value of $\\epsilon$ is often reduced over time (annealing) to shift from exploration to exploitation.",
         vfaTitle: "Outlook: Value Function Approximation (VFA)",
         vfaDesc: "For problems with very large or continuous state spaces, tabular representations of $Q(s,a)$ (a table for each state-action pair) are no longer practical. This is where value function approximation comes into play: Instead of storing Q-values exactly, they are approximated by a parameterized function, e.g., a linear function $\\hat{q}(s,a;\\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{x}(s,a)$ or a neural network $Q(s,a; \\mathbf{w}) \\approx Q^*(s,a)$. The parameters $\\mathbf{w}$ are then learned, often using gradient methods.",
        vfaExample: "A well-known example is the Deep Q-Network (DQN), which uses a deep neural network to approximate the Q-function and has achieved remarkable success in complex environments like Atari games. This opens up the application of RL to problems with high-dimensional sensory inputs.",
        vfaCheat1: "Tabular methods fail for huge state spaces.",
        vfaCheat2: "Only parameters are stored; convergence is not guaranteed.",
        vfaCheat3: "Linear approximation uses feature vectors, e.g., polynomial basis.",
        vfaCheat4: "Deep Q-Networks employ experience replay.",
        vfaCheat5: "Target networks stabilize the updates.",
         quizSectionTitle: "Knowledge Quiz for Midterm",
         quizJsonLoading: "Loading quiz data...",
         quizLoading: "Loading question...",
         quiz_nextButton: "Next Question",
         quiz_resultsButton: "Show Results",
         quiz_finishedTitle: "Quiz Finished!",
         quiz_scorePrefix: "Your score: ",
         quiz_scoreSuffix_singular: "point",
         quiz_scoreSuffix_plural: "points",
         quiz_scoreOutOf: "out of",
         quiz_restartButton: "Restart Quiz",
+        quiz_startButton: "Start Quiz",
         quiz_feedbackCorrect: "Correct!",
         quiz_feedbackIncorrectPrefix: "Incorrect. The correct answer was: ",
         quiz_questionCounterPrefix: "Question ",
         quiz_questionCounterOf: "of",
+        quiz_timerLabel: "Time:",
+        quiz_finalTimeLabel: "Time taken:",
         footerText1: "&copy; 2025 Detailed Reinforcement Learning Infographic.",
         footerText2: "Based on lecture materials and standard RL concepts.",
         chart_mcVsTd_title: "Comparison: MC vs. TD Properties (Scale 1=Low/No, 5=High/Yes)",
         chart_mcVsTd_dataset_mc: "Monte Carlo (MC)",
         chart_mcVsTd_dataset_td: "Temporal Difference (TD)",
         chart_mcVsTd_label_bias: "Bias",
         chart_mcVsTd_label_variance: "Variance",
         chart_mcVsTd_label_updateFreq: "Update Frequency",
         chart_mcVsTd_label_incompleteEp: "Uses Incomplete Episodes",
         chart_mcVsTd_label_initSens: "Initialization Sensitivity",
         chart_mcVsTd_yAxis_lowNo: "1 (Low/No)",
         chart_mcVsTd_yAxis_medium: "3 (Medium)",
         chart_mcVsTd_yAxis_highYes: "5 (High/Yes)",
         tooltip_bias_low_mc: "Low (Unbiased)",
         tooltip_bias_medium_td: "Medium (Biased)",
         tooltip_variance_high_mc: "High",
         tooltip_variance_low_td: "Low",
         tooltip_update_episode_mc: "At episode end",
         tooltip_update_step_td: "After each step",
         tooltip_incomplete_no_mc: "No",
         tooltip_incomplete_yes_td: "Yes (Bootstrapping)",
         tooltip_init_insensitive_mc: "Insensitive",
         tooltip_init_sensitive_td: "Sensitive",
                 // ... (alle bisherigen englischen Übersetzungen) ...
        quiz_numQuestionsLabel: "Number of Questions:",
        quiz_numQuestionsAll: "All",
        quiz_reviewTitle: "Incorrect Answers Review",
        quiz_reviewQuestion: "Question:",
        quiz_reviewYourAnswer: "Your Answer:",
        quiz_reviewCorrectAnswer: "Correct Answer:",
        quiz_reviewExplanation: "Explanation:",
        quiz_reviewIncorrectTag: "(Incorrect)",
        // Chart Tooltips (already present, check if needed)
        tooltip_bias_low_mc: "Low (Unbiased)",
        tooltip_bias_medium_td: "Medium (Biased)",
        tooltip_variance_high_mc: "High",
        tooltip_variance_low_td: "Low",
        tooltip_update_episode_mc: "At episode end",
        tooltip_update_step_td: "After each step",
        tooltip_incomplete_no_mc: "No",
        tooltip_incomplete_yes_td: "Yes (Bootstrapping)",
        tooltip_init_insensitive_mc: "Insensitive",
        tooltip_init_sensitive_td: "Sensitive",
    }
};
