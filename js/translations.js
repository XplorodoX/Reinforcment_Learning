// js/translations.js
const translations = {
    de: {
        pageTitle: "Reinforcement Learning Infografik - Detailliert mit Quiz",
        headerTitle: "Reinforcement Learning: Detaillierte Einblicke & Quiz",
        headerSubtitle: "Vertiefte Konzepte, Algorithmen und interaktives Quiz",
        section1Title: "1. Grundlagen: RL & Markov-Entscheidungsprozesse (MDPs)",
        section1Intro: "Reinforcement Learning (RL) ist ein Paradigma des maschinellen Lernens, bei dem ein autonomer <strong>Agent</strong> lernt, optimale Sequenzen von Entscheidungen (Aktionen) in einer komplexen und oft unsicheren <strong>Umgebung</strong> zu treffen. Das primäre Ziel des Agenten ist es, die Summe der erhaltenen <strong>Belohnungen</strong> über die Zeit zu maximieren. Dieser Lernprozess ist fundamental iterativ und erfahrungsbasiert. RL unterscheidet sich von anderen Lernparadigmen wie überwachtem Lernen (wo gelabelte Beispiele gegeben sind) und unüberwachtem Lernen (wo es darum geht, versteckte Strukturen zu finden) durch seinen Fokus auf zielgerichtetes Lernen durch Interaktion und evaluatives Feedback.",
        agentEnvCycleTitle: "Der Agent-Umgebung-Interaktionszyklus",
        cycleStep1: "1. Agent beobachtet aktuellen Zustand $S_t$ der Umgebung.",
        cycleStep2: "2. Agent wählt eine Aktion $A_t$ basierend auf seiner aktuellen Strategie (Policy $\\pi$).",
        cycleStep3: "3. Umgebung reagiert: Agent erhält eine Belohnung $R_{t+1}$ und beobachtet den neuen Zustand $S_{t+1}$.",
        cycleStep4: "4. Agent aktualisiert seine Policy $\\pi$ basierend auf der Erfahrung $(S_t, A_t, R_{t+1}, S_{t+1})$.",
        cycleDesc: "Dieser Zyklus wiederholt sich, wodurch der Agent schrittweise lernt, bessere Entscheidungen zu treffen.",
        goalTitle: "Maximierung des kumulativen Returns",
        goalDesc: "Der Return $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ ist die Summe der diskontierten zukünftigen Belohnungen. Die <strong data-i18n=\"rewardHypothesis\">Reward Hypothesis</strong> besagt, dass alle Ziele durch die Maximierung des erwarteten kumulativen Rewards formuliert werden können.",
        rewardHypothesis: "Reward Hypothesis",
        gammaFactorTitle: "Der Diskontierungsfaktor $\\gamma$",
        gammaDesc: "Steuert die Wichtigkeit zukünftiger Belohnungen:",
        gammaShortSighted: "$\\gamma \\approx 0$: Agent ist \"kurzsichtig\", fokussiert auf unmittelbare Belohnungen.",
        gammaLongSighted: "$\\gamma \\approx 1$: Agent ist \"weitsichtig\", berücksichtigt zukünftige Belohnungen stark.",
        gammaContinuous: "Notwendig für kontinuierliche Aufgaben, um unendliche Returns zu vermeiden und mathematische Konvergenz sicherzustellen.",
        mdpTitle: "Der Markov-Entscheidungsprozess (MDP)",
        mdpIntro: "Ein MDP ist der Standardformalismus für RL-Probleme, die die Markov-Eigenschaft erfüllen. Er wird durch ein Tupel $(S, A, P, R, \\gamma)$ definiert:",
        mdpS: "<strong>S (Zustandsraum):</strong> Eine Menge aller möglichen Zustände. Z.B. Positionen auf einem Schachbrett, Gelenkwinkel eines Roboters. Kann endlich oder unendlich sein.",
        mdpA: "<strong>A (Aktionsraum):</strong> Eine Menge aller möglichen Aktionen. Z.B. Züge im Schach, Motorkommandos für einen Roboter. Kann endlich oder unendlich sein.",
        mdpP: "<strong>P (Zustandsübergangsmodell):</strong> $P(s'|s, a) = Pr\\{S_{t+1}=s' | S_t=s, A_t=a\\}$. Definiert die Dynamik der Umgebung. Für jede Zustands-Aktions-Paar $(s,a)$ gibt $P$ eine Wahrscheinlichkeitsverteilung über mögliche Nachfolgezustände $s'$ an.",
        mdpR: "<strong>R (Belohnungsfunktion):</strong> $R(s, a, s') = E[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$. Definiert das Ziel des Agenten. Gibt die erwartete unmittelbare Belohnung an, die beim Übergang von $s$ zu $s'$ durch Aktion $a$ erhalten wird. Manchmal auch als $R(s,a)$ oder $R(s)$ definiert.",
        markovPropertyTitle: "Die Markov-Eigenschaft",
        markovPropertyQuote: "\"Die Zukunft ist unabhängig von der Vergangenheit, gegeben die Gegenwart.\"",
        markovPropertyDesc: "Der aktuelle Zustand $S_t$ enthält alle relevanten Informationen aus der Historie, um die nächste Zustands- und Belohnungswahrscheinlichkeit zu bestimmen. Dies ist eine starke Vereinfachung, die viele RL-Algorithmen ermöglicht. Wenn diese Eigenschaft nicht perfekt erfüllt ist, spricht man von partiell beobachtbaren MDPs (POMDPs), die komplexer sind.",
        coreChallengesTitle: "Kernherausforderungen und Konzepte",
        creditAssignmentTitle: "Credit Assignment",
        creditAssignmentDesc: "Wie können Belohnungen (oder Bestrafungen), die erst spät in einer Sequenz von Aktionen auftreten, korrekt den früheren Aktionen zugeordnet werden, die ursächlich dafür waren? Dies ist besonders schwierig bei langen Verzögerungen zwischen Aktion und Konsequenz.",
        exploreExploitTitle: "Exploration vs. Exploitation",
        exploreExploitDesc: "Der Agent muss entscheiden, ob er neue Aktionen ausprobiert, um mehr über die Umgebung zu lernen und potenziell bessere, unbekannte Belohnungen zu finden (Exploration), oder ob er die aktuell besten bekannten Aktionen ausführt, um die bereits bekannte Belohnung zu maximieren (Exploitation).",
        policyTitle: "Policy ($\\pi$): Die Strategie des Agenten",
        policyDesc: "Eine Abbildung von Zuständen zu Aktionen. Deterministisch: $\\pi(s) = a$. Stochastisch: $\\pi(a|s) = Pr\\{A_t=a | S_t=s\\}$. Das Ziel ist es, eine optimale Policy $\\pi_*$ zu finden.",
        valueFunctionTitle: "Wertfunktionen ($V^\\pi, Q^\\pi$): Wie gut ist ein Zustand/Aktion?",
        valueFunctionDesc: "$V^\\pi(s) = E_\\pi[G_t | S_t=s]$: Erwarteter Return, wenn man in Zustand $s$ startet und Policy $\\pi$ folgt. $Q^\\pi(s,a) = E_\\pi[G_t | S_t=s, A_t=a]$: Erwarteter Return, wenn man in Zustand $s$ Aktion $a$ ausführt und danach Policy $\\pi$ folgt.",
        section2Title: "2. Dynamische Programmierung (DP): Planung mit perfektem Modell",
        section2Intro: "Dynamische Programmierung (DP) umfasst Algorithmen, die eine optimale Policy $\\pi^*$ für einen gegebenen MDP berechnen können, vorausgesetzt, das Modell der Umgebung (Übergangswahrscheinlichkeiten $P$ und Belohnungen $R$) ist vollständig bekannt. DP-Methoden nutzen Wertfunktionen, um die Suche nach guten Policies zu strukturieren und basieren auf den Bellman-Gleichungen. Sie sind fundamental für das Verständnis von RL, auch wenn ihre direkte Anwendbarkeit durch die Modellannahme und den Rechenaufwand (\"Fluch der Dimensionalität\") begrenzt ist.",
        bellmanOptimalityPrincipleTitle: "Bellman-Optimalitätsprinzip",
        bellmanOptimalityPrincipleQuote: "\"Eine optimale Policy hat die Eigenschaft, dass, was auch immer der Anfangszustand und die erste Entscheidung sind, die verbleibenden Entscheidungen eine optimale Policy bezüglich des aus der ersten Entscheidung resultierenden Zustands darstellen müssen.\" (Bellman, 1957)",
        bellmanOptimalityPrincipleDesc: "Einfach gesagt: Wenn der beste Pfad von A nach C über B führt, dann muss der Teilpfad von B nach C auch der beste Pfad von B nach C sein.",
        policyIterationTitle: "Policy Iteration (Politikiteration)",
        policyIterationDesc: "Ein iterativer Algorithmus, der zwischen zwei Schritten wechselt, bis die Policy nicht mehr verbessert werden kann:",
        policyEvalTitle: "1. Policy Evaluation (Bewertung):",
        policyEvalDesc: "Berechne die Zustands-Wertfunktion $V^\\pi(s)$ für die aktuelle Policy $\\pi$. Dies geschieht durch iteratives Anwenden der Bellman-Erwartungsgleichung bis zur Konvergenz:",
        policyImprovTitle: "2. Policy Improvement (Verbesserung):",
        policyImprovDesc: "Verbessere die Policy, indem für jeden Zustand $s$ gierig die Aktion gewählt wird, die $Q^\\pi(s,a)$ maximiert:",
        policyIterationConvergence: "Dieser Prozess konvergiert garantiert zur optimalen Policy $\\pi^*$. \"Sweeps\" beziehen sich auf vollständige Durchläufe über alle Zustände während der Evaluation oder Iteration.",
        valueIterationTitle: "Value Iteration (Wertiteration)",
        valueIterationDesc: "Kombiniert im Wesentlichen einen Schritt der Policy Evaluation und einen Schritt der Policy Improvement in einer einzigen Update-Regel. Sie iteriert direkt über die Bellman-Optimalitätsgleichung für $V^*(s)$:",
        valueIterationConvergence: "Konvergiert ebenfalls zur optimalen Wertfunktion $V^*$, aus der die optimale Policy $\\pi^*$ dann durch eine einmalige gierige Auswahl extrahiert werden kann.",
        vQRelationTitle: "Beziehung zwischen $V^\\pi$ und $Q^\\pi$",
        vQRelationDesc: "Die Zustands-Wertfunktion und die Aktions-Wertfunktion sind eng miteinander verbunden:",
        vQStochastic: "(wenn $\\pi$ stochastisch ist)",
        vQDeterministic: "(wenn $\\pi$ deterministisch ist)",
        dpApplicabilityTitle: "Wann ist DP anwendbar?",
        dpCond1: "Wenn ein vollständiges und genaues Modell der Umgebung ($P$ und $R$) verfügbar ist.",
        dpCond2: "Eher für Planungsprobleme als für direktes Lernen aus Interaktion.",
        dpCond3: "Obwohl rechenintensiv für sehr große Probleme, bilden DP-Konzepte die theoretische Grundlage für viele andere RL-Algorithmen.",
        section3Title: "3. Modellfreie Vorhersage: Lernen ohne Modell",
        section3Intro: "In vielen realen Szenarien ist ein exaktes Modell der Umgebung nicht verfügbar. Modellfreie Vorhersagemethoden ermöglichen es, Wertfunktionen ($V^\\pi$ oder $Q^\\pi$) für eine gegebene Policy $\\pi$ direkt aus Erfahrungsepisoden zu schätzen, die durch Interaktion mit der Umgebung generiert werden.",
        mcMethodsTitle: "a) Monte Carlo (MC) Methoden",
        mcMethodsDesc: "MC-Methoden lernen, indem sie den durchschnittlichen Return nach Besuchen eines Zustands über viele vollständige Episoden berechnen.<br><strong>First-Visit MC:</strong> Berücksichtigt nur den Return nach dem ersten Besuch eines Zustands $s$ in einer Episode.<br><strong>Every-Visit MC:</strong> Berücksichtigt den Return nach jedem Besuch eines Zustands $s$ in einer Episode.",
        firstVisitMC: "First-Visit MC:",
        everyVisitMC: "Every-Visit MC:",
        mcUpdateRuleTitle: "Update-Regel (inkrementell für $V(S_t)$):",
        mcUpdateRuleDesc: "Hier ist $G_t$ der tatsächlich beobachtete (vollständige) Return ab Zeitschritt $t$ bis zum Ende der Episode. $\\alpha$ ist die Lernrate.",
        mcAdvantages: "Vorteile:",
        mcAdv1: "Unverzerrt (kein Bootstrapping-Bias).",
        mcAdv2: "Einfach zu verstehen und zu implementieren.",
        mcDisadvantages: "Nachteile:",
        mcDisadv1: "Hohe Varianz, da der Return von vielen zufälligen Aktionen und Übergängen abhängt.",
        mcDisadv2: "Funktioniert nur für episodische (terminierende) Aufgaben.",
        mcDisadv3: "Updates erfolgen erst am Ende einer Episode.",
        tdLearningTitle: "b) Temporal Difference (TD) Learning",
        tdLearningDesc: "TD-Methoden lernen von jedem Schritt, indem sie ihre aktuelle Schätzung basierend auf der Schätzung des nächsten Zustands anpassen – ein Prozess, der als <strong data-i18n=\"bootstrapping\">Bootstrapping</strong> bezeichnet wird. Sie benötigen keine vollständigen Episoden.",
        bootstrapping: "Bootstrapping",
        tdUpdateRuleTitle: "TD(0) Update-Regel für $V(S_t)$:",
        tdUpdateRuleDesc: "Der Term $R_{t+1} + \\gamma V(S_{t+1})$ ist das <strong data-i18n=\"tdTarget\">TD-Target</strong> (eine Schätzung des Returns), und $[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$ ist der <strong data-i18n=\"tdError\">TD-Fehler</strong>.",
        tdTarget: "TD-Target",
        tdError: "TD-Fehler",
        tdAdvantages: "Vorteile:",
        tdAdv1: "Geringere Varianz als MC.",
        tdAdv2: "Kann online lernen (nach jedem Schritt).",
        tdAdv3: "Funktioniert auch für kontinuierliche Aufgaben.",
        tdDisadvantages: "Nachteile:",
        tdDisadv1: "Verzerrt, da die Schätzung auf einer anderen Schätzung ($V(S_{t+1})$) basiert.",
        tdDisadv2: "Empfindlicher gegenüber der Initialisierung der Werte.",
        nStepTDTitle: "N-Step TD Methoden",
        nStepTDDesc1: "Diese Methoden schlagen eine Brücke zwischen MC und TD(0). Anstatt nur einen Schritt (TD(0)) oder die gesamte Episode (MC) für das Update zu betrachten, verwenden n-Step TD Methoden einen Return über $n$ Schritte:",
        nStepTDDesc2: "Die Update-Regel lautet dann:",
        nStepTDDesc3: "Der Parameter $n$ erlaubt einen Trade-off zwischen dem Bias von TD(0) und der Varianz von MC.",
        mcTdChartTitle: "Visueller Vergleich: MC vs. TD Update",
        mcTdChartDesc: "Das Diagramm illustriert die unterschiedlichen Stärken und Schwächen von MC- und TD-Lernansätzen bezüglich Bias, Varianz und Update-Frequenz.",
        section4Title: "4. Modellfreie Steuerung: Optimale Policies ohne Modell finden",
        section4Intro: "Modellfreie Steuerungsmethoden erweitern die Ideen der modellfreien Vorhersage, um nicht nur Wertfunktionen zu schätzen, sondern auch aktiv eine (nahezu) optimale Policy $\\pi^*$ zu finden. Da kein Umgebungsmodell vorhanden ist, lernen diese Methoden typischerweise die Aktions-Wertfunktion $Q(s,a)$, da diese direkt angibt, wie gut es ist, eine bestimmte Aktion $a$ in einem Zustand $s$ auszuführen. Die Policy wird dann oft $\\epsilon$-gierig auf Basis dieser $Q$-Werte abgeleitet.",
        gpiTitle: "Generalisierte Policy Iteration (GPI) für Modellfreie Steuerung",
        gpiEvalTitle: "1. Policy Evaluation (Schätzung)",
        gpiEvalDesc: "Schätze $Q^\\pi(s,a)$ für die aktuelle Policy $\\pi$ (z.B. mittels MC oder TD Updates auf $Q$-Werte).",
        gpiImprovTitle: "2. Policy Improvement (Verbesserung)",
        gpiImprovDesc: "Aktualisiere $\\pi$, indem $\\epsilon$-gierig bezüglich der aktuellen $Q$-Werte agiert wird:",
        gpiEpsilon: "(mit $\\epsilon$ W'keit zufällig).",
        gpiConvergence: "Dieser iterative Prozess konvergiert typischerweise gegen die optimale Aktions-Wertfunktion $Q^*$ und die optimale Policy $\\pi^*$. Die Notwendigkeit von $Q(s,a)$ ergibt sich daraus, dass ohne Modell $P(s'|s,a)$ die $V(s)$-Funktion allein nicht ausreicht, um eine gierige Aktion zu bestimmen.",
        sarsaTitle: "SARSA (On-Policy TD Control)",
        sarsaDesc: "SARSA ist ein On-Policy TD-Algorithmus. \"On-Policy\" bedeutet, dass die $Q$-Werte für die Policy geschätzt und verbessert werden, die der Agent tatsächlich ausführt (inklusive explorativer Aktionen).",
        sarsaUpdateRuleTitle: "Update-Regel für $Q(S_t, A_t)$:",
        sarsaNameOrigin: "Der Name SARSA kommt von der Sequenz der Ereignisse im Update: $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$. $A_{t+1}$ wird gemäß der aktuellen (oft $\\epsilon$-greedy) Policy gewählt.",
        expectedSarsaTitle: "Expected SARSA",
        expectedSarsaDesc: "Eine Variante, die den Erwartungswert über alle möglichen Aktionen $A_{t+1}$ gemäß der aktuellen Policy $\\pi$ nimmt, anstatt die spezifisch gewählte Aktion $A_{t+1}$ zu verwenden. Dies kann die Varianz reduzieren und ist oft stabiler.",
        qLearningTitle: "Q-Learning (Off-Policy TD Control)",
        qLearningDesc: "Q-Learning ist ein Off-Policy TD-Algorithmus. \"Off-Policy\" bedeutet, dass es die optimale Aktions-Wertfunktion $Q^*(s,a)$ lernt, unabhängig von der Policy, die der Agent zur Exploration verwendet (Verhaltenspolicy), solange diese alle Zustands-Aktions-Paare ausreichend oft besucht.",
        qLearningUpdateRuleTitle: "Update-Regel für $Q(S_t, A_t)$:",
        qLearningKeyDiff: "Der entscheidende Unterschied ist der Term $\\max_{a'} Q(S_{t+1}, a')$, der den maximalen $Q$-Wert im Folgezustand verwendet, was der Aktion entspricht, die eine gierige Policy wählen würde (Zielpolicy).",
        doubleQLearningTitle: "Double Q-Learning",
        doubleQLearningDesc: "Standard Q-Learning kann unter bestimmten Umständen zu einer Überschätzung der Q-Werte führen (Maximierungs-Bias), da der $\\max$-Operator sowohl zur Auswahl als auch zur Bewertung der Aktion verwendet wird. Double Q-Learning adressiert dies, indem es zwei separate Q-Wert-Schätzungen ($Q_A$ und $Q_B$) pflegt. Eine wird zur Auswahl der besten Aktion im nächsten Zustand verwendet, die andere zur Bewertung dieser Aktion.",
        doubleQLearningUpdate: "Z.B. Update für $Q_A$ (mit 50% Wahrscheinlichkeit): $Q_A(S_t,A_t) \\leftarrow Q_A(S_t,A_t) + \\alpha [R_{t+1} + \\gamma Q_B(S_{t+1}, \\text{argmax}_{a'} Q_A(S_{t+1},a')) - Q_A(S_t,A_t)]$ (und umgekehrt für $Q_B$).",
        onOffPolicyCompareTitle: "Vergleich der Update-Pfade: SARSA vs. Q-Learning",
        sarsaPathTitle: "SARSA (On-Policy)",
        sarsaPathDesc: "Update für $Q(S_t, A_t)$ verwendet $Q(S_{t+1}, A_{t+1})$. Lernt den Wert der tatsächlich ausgeführten Policy (inkl. Exploration).",
        qLearningPathTitle: "Q-Learning (Off-Policy)",
        qLearningPathDesc: "Update für $Q(S_t, A_t)$ verwendet $\\max_{a'} Q(S_{t+1}, a')$. Lernt den Wert der optimalen (gierigen) Policy, auch wenn die Aktionen anders (z.B. explorativ) gewählt wurden.",
        epsilonGreedyTitle: "Bedeutung von $\\epsilon$-greedy Exploration:",
        epsilonGreedyDesc: "Um sicherzustellen, dass der Agent alle relevanten Zustands-Aktions-Paare erkundet und nicht in lokalen Optima stecken bleibt, wird oft eine $\\epsilon$-greedy Strategie verwendet. Mit einer kleinen Wahrscheinlichkeit $\\epsilon$ wählt der Agent eine zufällige Aktion, anstatt der aktuell als optimal erachteten (gierigen) Aktion. Der Wert von $\\epsilon$ wird oft im Laufe des Trainings reduziert (Annealing), um von Exploration zu Exploitation überzugehen.",
        vfaTitle: "Ausblick: Wertfunktionsapproximation (VFA)",
        vfaDesc: "Für Probleme mit sehr großen oder kontinuierlichen Zustandsräumen sind tabellarische Darstellungen von $Q(s,a)$ (eine Tabelle für jedes Zustands-Aktions-Paar) nicht mehr praktikabel. Hier kommt die Wertfunktionsapproximation ins Spiel: Anstatt die Q-Werte exakt zu speichern, werden sie durch eine parametrisierte Funktion approximiert, z.B. eine lineare Funktion $\\hat{q}(s,a;\\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{x}(s,a)$ oder ein neuronales Netz $Q(s,a; \\mathbf{w}) \\approx Q^*(s,a)$. Die Parameter $\\mathbf{w}$ werden dann gelernt, oft mittels Gradientenverfahren.",
        vfaExample: "Ein bekanntes Beispiel ist das Deep Q-Network (DQN), das ein tiefes neuronales Netz zur Approximation der Q-Funktion verwendet und bemerkenswerte Erfolge in komplexen Umgebungen wie Atari-Spielen erzielt hat. Dies eröffnet die Anwendung von RL auf Probleme mit hochdimensionalen sensorischen Eingaben.",
        quizSectionTitle: "Wissens-Quiz zur Zwischenprüfung",
        quizJsonLoading: "Lade Quizdaten...",
        quizLoading: "Frage wird geladen...",
        quiz_nextButton: "Nächste Frage",
        quiz_resultsButton: "Ergebnisse anzeigen",
        quiz_finishedTitle: "Quiz beendet!",
        quiz_scorePrefix: "Dein Ergebnis: ",
        quiz_scoreSuffix_singular: "Punkt",
        quiz_scoreSuffix_plural: "Punkte",
        quiz_scoreOutOf: "von",
        quiz_restartButton: "Quiz neu starten",
        quiz_feedbackCorrect: "Richtig!",
        quiz_feedbackIncorrectPrefix: "Falsch. Die richtige Antwort war: ",
        quiz_explainButton: "Erklärung generieren",
        quiz_explanationLoading: "Erklärung wird geladen...",
        quiz_explanationError: "Fehler beim Laden der Erklärung.",
        quiz_questionCounterPrefix: "Frage ",
        quiz_questionCounterOf: "von",
        quiz_timerLabel: "Zeit:",
        quiz_finalTimeLabel: "Benötigte Zeit:",
        githubPatLabel: "GitHub Token (optional):",
        githubPatPlaceholder: "ghp_...",
        githubPatSaveButton: "Speichern",
        githubPatSaved: "Token gespeichert!",
        footerText1: "&copy; 2025 Detaillierte Reinforcement Learning Infografik.",
        footerText2: "Basierend auf Vorlesungsmaterialien und Standard-RL-Konzepten.",
        chart_mcVsTd_title: "Vergleich: MC vs. TD Eigenschaften (Skala 1=Niedrig/Nein, 5=Hoch/Ja)",
        chart_mcVsTd_dataset_mc: "Monte Carlo (MC)",
        chart_mcVsTd_dataset_td: "Temporal Difference (TD)",
        chart_mcVsTd_label_bias: "Bias",
        chart_mcVsTd_label_variance: "Varianz",
        chart_mcVsTd_label_updateFreq: "Update Häufigkeit",
        chart_mcVsTd_label_incompleteEp: "Nutzung unvollst. Episoden",
        chart_mcVsTd_label_initSens: "Sensitivität Initialisierung",
        chart_mcVsTd_yAxis_lowNo: "1 (Niedrig/Nein)",
        chart_mcVsTd_yAxis_medium: "3 (Mittel)",
        chart_mcVsTd_yAxis_highYes: "5 (Hoch/Ja)",
        tooltip_bias_low_mc: "Niedrig (Unverzerrt)",
        tooltip_bias_medium_td: "Mittel (Verzerrt)",
        tooltip_variance_high_mc: "Hoch",
        tooltip_variance_low_td: "Niedrig",
        tooltip_update_episode_mc: "Am Episodenende",
        tooltip_update_step_td: "Nach jedem Schritt",
        tooltip_incomplete_no_mc: "Nein",
        tooltip_incomplete_yes_td: "Ja (Bootstrapping)",
        tooltip_init_insensitive_mc: "Unempfindlich",
        tooltip_init_sensitive_td: "Empfindlich",
        quiz_numQuestionsLabel: "Anzahl der Fragen:",
        quiz_numQuestionsAll: "Alle",
        quiz_startButton: "Quiz starten",
        quiz_reviewTitle: "Fehleranalyse",
        quiz_reviewQuestion: "Frage:",
        quiz_reviewYourAnswer: "Deine Antwort:",
        quiz_reviewCorrectAnswer: "Richtige Antwort:",
        quiz_reviewExplanation: "Erläuterung:",
        quiz_reviewIncorrectTag: "(Falsch)",
        // Chart Tooltips (bereits vorhanden, ggf. prüfen)
        tooltip_bias_low_mc: "Niedrig (Unverzerrt)",
        tooltip_bias_medium_td: "Mittel (Verzerrt)",
        tooltip_variance_high_mc: "Hoch",
        tooltip_variance_low_td: "Niedrig",
        tooltip_update_episode_mc: "Am Episodenende",
        tooltip_update_step_td: "Nach jedem Schritt",
        tooltip_incomplete_no_mc: "Nein",
        tooltip_incomplete_yes_td: "Ja (Bootstrapping)",
        tooltip_init_insensitive_mc: "Unempfindlich",
        tooltip_init_sensitive_td: "Empfindlich",
    },
    en: {
        pageTitle: "Reinforcement Learning Infographic - Detailed with Quiz",
        headerTitle: "Reinforcement Learning: Detailed Insights & Quiz",
        headerSubtitle: "In-depth Concepts, Algorithms, and Interactive Quiz",
        section1Title: "1. Basics: RL & Markov Decision Processes (MDPs)",
        section1Intro: "Reinforcement Learning (RL) is a paradigm of machine learning where an autonomous <strong>agent</strong> learns to make optimal sequences of decisions (actions) in a complex and often uncertain <strong>environment</strong>. The primary goal of the agent is to maximize the sum of received <strong>rewards</strong> over time. This learning process is fundamentally iterative and experience-based. RL differs from other learning paradigms like supervised learning (where labeled examples are given) and unsupervised learning (which is about finding hidden structures) through its focus on goal-directed learning via interaction and evaluative feedback.",
        agentEnvCycleTitle: "The Agent-Environment Interaction Cycle",
        cycleStep1: "1. Agent observes current state $S_t$ of the environment.",
        cycleStep2: "2. Agent selects an action $A_t$ based on its current strategy (policy $\\pi$).",
        cycleStep3: "3. Environment reacts: Agent receives a reward $R_{t+1}$ and observes the new state $S_{t+1}$.",
        cycleStep4: "4. Agent updates its policy $\\pi$ based on the experience $(S_t, A_t, R_{t+1}, S_{t+1})$.",
        cycleDesc: "This cycle repeats, allowing the agent to gradually learn to make better decisions.",
        goalTitle: "Maximizing Cumulative Return",
        goalDesc: "The return $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ is the sum of discounted future rewards. The <strong>Reward Hypothesis</strong> states that all goals can be formulated as maximizing the expected cumulative reward.",
        rewardHypothesis: "Reward Hypothesis",
        gammaFactorTitle: "The Discount Factor $\\gamma$",
        gammaDesc: "Controls the importance of future rewards:",
        gammaShortSighted: "$\\gamma \\approx 0$: Agent is \"myopic\", focusing on immediate rewards.",
        gammaLongSighted: "$\\gamma \\approx 1$: Agent is \"far-sighted\", strongly considering future rewards.",
        gammaContinuous: "Necessary for continuous tasks to avoid infinite returns and ensure mathematical convergence.",
        mdpTitle: "The Markov Decision Process (MDP)",
        mdpIntro: "An MDP is the standard formalism for RL problems that satisfy the Markov property. It is defined by a tuple $(S, A, P, R, \\gamma)$:",
        mdpS: "<strong>S (State Space):</strong> A set of all possible states. E.g., positions on a chessboard, joint angles of a robot. Can be finite or infinite.",
        mdpA: "<strong>A (Action Space):</strong> A set of all possible actions. E.g., moves in chess, motor commands for a robot. Can be finite or infinite.",
        mdpP: "<strong>P (State Transition Model):</strong> $P(s'|s, a) = Pr\\{S_{t+1}=s' | S_t=s, A_t=a\\}$. Defines the dynamics of the environment. For each state-action pair $(s,a)$, $P$ gives a probability distribution over possible successor states $s'$.",
        mdpR: "<strong>R (Reward Function):</strong> $R(s, a, s') = E[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$. Defines the agent's goal. Gives the expected immediate reward received when transitioning from $s$ to $s'$ via action $a$. Sometimes also defined as $R(s,a)$ or $R(s)$.",
        markovPropertyTitle: "The Markov Property",
        markovPropertyQuote: "\"The future is independent of the past, given the present.\"",
        markovPropertyDesc: "The current state $S_t$ contains all relevant information from the history to determine the next state and reward probabilities. This is a strong simplification that enables many RL algorithms. If this property is not perfectly met, one speaks of partially observable MDPs (POMDPs), which are more complex.",
        coreChallengesTitle: "Core Challenges and Concepts",
        creditAssignmentTitle: "Credit Assignment",
        creditAssignmentDesc: "How can rewards (or punishments) that occur late in a sequence of actions be correctly attributed to the earlier actions that caused them? This is particularly difficult with long delays between action and consequence.",
        exploreExploitTitle: "Exploration vs. Exploitation",
        exploreExploitDesc: "The agent must decide whether to try new actions to learn more about the environment and potentially find better, unknown rewards (exploration), or to execute the currently known best actions to maximize known rewards (exploitation).",
        policyTitle: "Policy ($\\pi$): The Agent's Strategy",
        policyDesc: "A mapping from states to actions. Deterministic: $\\pi(s) = a$. Stochastic: $\\pi(a|s) = Pr\\{A_t=a | S_t=s\\}$. The goal is to find an optimal policy $\\pi_*$.",
        valueFunctionTitle: "Value Functions ($V^\\pi, Q^\\pi$): How good is a state/action?",
        valueFunctionDesc: "$V^\\pi(s) = E_\\pi[G_t | S_t=s]$: Expected return starting in state $s$ and following policy $\\pi$. $Q^\\pi(s,a) = E_\\pi[G_t | S_t=s, A_t=a]$: Expected return starting in state $s$, taking action $a$, and thereafter following policy $\\pi$.",
        section2Title: "2. Dynamic Programming (DP): Planning with a Perfect Model",
        section2Intro: "Dynamic Programming (DP) comprises algorithms that can compute an optimal policy $\\pi^*$ for a given MDP, provided that the model of the environment (transition probabilities $P$ and rewards $R$) is fully known. DP methods use value functions to structure the search for good policies and are based on Bellman equations. They are fundamental to understanding RL, even if their direct applicability is limited by the model assumption and computational cost (\"curse of dimensionality\").",
        bellmanOptimalityPrincipleTitle: "Bellman's Principle of Optimality",
        bellmanOptimalityPrincipleQuote: "\"An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.\" (Bellman, 1957)",
        bellmanOptimalityPrincipleDesc: "Simply put: If the best path from A to C goes through B, then the subpath from B to C must also be the best path from B to C.",
        policyIterationTitle: "Policy Iteration",
        policyIterationDesc: "An iterative algorithm that alternates between two steps until the policy no longer improves:",
        policyEvalTitle: "1. Policy Evaluation:",
        policyEvalDesc: "Compute the state-value function $V^\\pi(s)$ for the current policy $\\pi$. This is done by iteratively applying the Bellman expectation equation until convergence:",
        policyImprovTitle: "2. Policy Improvement:",
        policyImprovDesc: "Improve the policy by choosing greedily for each state $s$ the action that maximizes $Q^\\pi(s,a)$:",
        policyIterationConvergence: "This process is guaranteed to converge to the optimal policy $\\pi^*$. \"Sweeps\" refer to complete passes through all states during evaluation or iteration.",
        valueIterationTitle: "Value Iteration",
        valueIterationDesc: "Essentially combines one step of policy evaluation and one step of policy improvement into a single update rule. It directly iterates on the Bellman optimality equation for $V^*(s)$:",
        valueIterationConvergence: "Also converges to the optimal value function $V^*$, from which the optimal policy $\\pi^*$ can then be extracted by a single greedy selection.",
        vQRelationTitle: "Relationship between $V^\\pi$ and $Q^\\pi$",
        vQRelationDesc: "The state-value function and action-value function are closely related:",
        vQStochastic: "(if $\\pi$ is stochastic)",
        vQDeterministic: "(if $\\pi$ is deterministic)",
        dpApplicabilityTitle: "When is DP Applicable?",
        dpCond1: "When a complete and accurate model of the environment ($P$ and $R$) is available.",
        dpCond2: "More for planning problems than for direct learning from interaction.",
        dpCond3: "Although computationally intensive for very large problems, DP concepts form the theoretical basis for many other RL algorithms.",
        section3Title: "3. Model-Free Prediction: Learning without a Model",
        section3Intro: "In many real-world scenarios, an exact model of the environment is not available. Model-free prediction methods allow estimating value functions ($V^\\pi$ or $Q^\\pi$) for a given policy $\\pi$ directly from experience episodes generated through interaction with the environment.",
        mcMethodsTitle: "a) Monte Carlo (MC) Methods",
        mcMethodsDesc: "MC methods learn by averaging returns observed after visits to a state over many complete episodes.<br><strong>First-Visit MC:</strong> Considers only the return after the first visit to state $s$ in an episode.<br><strong>Every-Visit MC:</strong> Considers the return after every visit to state $s$ in an episode.",
        firstVisitMC: "First-Visit MC:",
        everyVisitMC: "Every-Visit MC:",
        mcUpdateRuleTitle: "Update Rule (incremental for $V(S_t)$):",
        mcUpdateRuleDesc: "Here, $G_t$ is the actual observed (full) return from time step $t$ to the end of the episode. $\\alpha$ is the learning rate.",
        mcAdvantages: "Advantages:",
        mcAdv1: "Unbiased (no bootstrapping bias).",
        mcAdv2: "Easy to understand and implement.",
        mcDisadvantages: "Disadvantages:",
        mcDisadv1: "High variance, as the return depends on many random actions and transitions.",
        mcDisadv2: "Only works for episodic (terminating) tasks.",
        mcDisadv3: "Updates are made only at the end of an episode.",
        tdLearningTitle: "b) Temporal Difference (TD) Learning",
        tdLearningDesc: "TD methods learn from each step, updating their current estimate based on the estimate of the next state—a process called <strong data-i18n=\"bootstrapping\">bootstrapping</strong>. They do not require complete episodes.",
        bootstrapping: "bootstrapping",
        tdUpdateRuleTitle: "TD(0) Update Rule for $V(S_t)$:",
        tdUpdateRuleDesc: "The term $R_{t+1} + \\gamma V(S_{t+1})$ is the <strong data-i18n=\"tdTarget\">TD target</strong> (an estimate of the return), and $[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$ is the <strong data-i18n=\"tdError\">TD error</strong>.",
        tdTarget: "TD target",
        tdError: "TD error",
        tdAdvantages: "Advantages:",
        tdAdv1: "Lower variance than MC.",
        tdAdv2: "Can learn online (after each step).",
        tdAdv3: "Also works for continuous tasks.",
        tdDisadvantages: "Disadvantages:",
        tdDisadv1: "Biased, as the estimate is based on another estimate ($V(S_{t+1})$).",
        tdDisadv2: "More sensitive to the initialization of values.",
        nStepTDTitle: "N-Step TD Methods",
        nStepTDDesc1: "These methods bridge the gap between MC and TD(0). Instead of looking only one step ahead (TD(0)) or at the entire episode (MC), n-step TD methods use a return over $n$ steps:",
        nStepTDDesc2: "The update rule is then:",
        nStepTDDesc3: "The parameter $n$ allows a trade-off between the bias of TD(0) and the variance of MC.",
        mcTdChartTitle: "Visual Comparison: MC vs. TD Update",
        mcTdChartDesc: "The diagram illustrates the different strengths and weaknesses of MC and TD learning approaches regarding bias, variance, and update frequency.",
        section4Title: "4. Model-Free Control: Finding Optimal Policies without a Model",
        section4Intro: "Model-free control methods extend the ideas of model-free prediction to not only estimate value functions but also to actively find a (near) optimal policy $\\pi^*$. Since no environment model is available, these methods typically learn the action-value function $Q(s,a)$, as it directly indicates how good it is to take a specific action $a$ in a state $s$. The policy is then often derived $\\epsilon$-greedily based on these $Q$-values.",
        gpiTitle: "Generalized Policy Iteration (GPI) for Model-Free Control",
        gpiEvalTitle: "1. Policy Evaluation (Estimation)",
        gpiEvalDesc: "Estimate $Q^\\pi(s,a)$ for the current policy $\\pi$ (e.g., using MC or TD updates on $Q$-values).",
        gpiImprovTitle: "2. Policy Improvement",
        gpiImprovDesc: "Update $\\pi$ by acting $\\epsilon$-greedily with respect to the current $Q$-values:",
        gpiEpsilon: "(with $\\epsilon$ probability random).",
        gpiConvergence: "This iterative process typically converges towards the optimal action-value function $Q^*$ and the optimal policy $\\pi^*$. The need for $Q(s,a)$ arises because, without a model $P(s'|s,a)$, the $V(s)$ function alone is insufficient to determine a greedy action.",
        sarsaTitle: "SARSA (On-Policy TD Control)",
        sarsaDesc: "SARSA is an on-policy TD algorithm. \"On-policy\" means that the $Q$-values are estimated and improved for the policy the agent is actually executing (including exploratory actions).",
        sarsaUpdateRuleTitle: "Update Rule for $Q(S_t, A_t)$:",
        sarsaNameOrigin: "The name SARSA comes from the sequence of events in the update: $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$. $A_{t+1}$ is chosen according to the current (often $\\epsilon$-greedy) policy.",
        expectedSarsaTitle: "Expected SARSA",
        expectedSarsaDesc: "A variant that takes the expectation over all possible actions $A_{t+1}$ according to the current policy $\\pi$, instead of using the specifically chosen action $A_{t+1}$. This can reduce variance and is often more stable.",
        qLearningTitle: "Q-Learning (Off-Policy TD Control)",
        qLearningDesc: "Q-Learning is an off-policy TD algorithm. \"Off-policy\" means it learns the optimal action-value function $Q^*(s,a)$ independently of the policy used by the agent for exploration (behavior policy), as long as it visits all state-action pairs sufficiently often.",
        qLearningUpdateRuleTitle: "Update Rule for $Q(S_t, A_t)$:",
        qLearningKeyDiff: "The key difference is the term $\\max_{a'} Q(S_{t+1}, a')$, which uses the maximum $Q$-value in the next state, corresponding to the action a greedy policy would choose (target policy).",
        doubleQLearningTitle: "Double Q-Learning",
        doubleQLearningDesc: "Standard Q-Learning can, under certain circumstances, lead to an overestimation of Q-values (maximization bias), as the $\\max$ operator is used for both selecting and evaluating an action. Double Q-Learning addresses this by maintaining two separate Q-value estimates ($Q_A$ and $Q_B$). One is used to select the best action in the next state, and the other is used to evaluate that action.",
        doubleQLearningUpdate: "E.g., Update for $Q_A$ (with 50% probability): $Q_A(S_t,A_t) \\leftarrow Q_A(S_t,A_t) + \\alpha [R_{t+1} + \\gamma Q_B(S_{t+1}, \\text{argmax}_{a'} Q_A(S_{t+1},a')) - Q_A(S_t,A_t)]$ (and vice versa for $Q_B$).",
        onOffPolicyCompareTitle: "Comparison of Update Paths: SARSA vs. Q-Learning",
        sarsaPathTitle: "SARSA (On-Policy)",
        sarsaPathDesc: "Update for $Q(S_t, A_t)$ uses $Q(S_{t+1}, A_{t+1})$. Learns the value of the policy actually being executed (incl. exploration).",
        qLearningPathTitle: "Q-Learning (Off-Policy)",
        qLearningPathDesc: "Update for $Q(S_t, A_t)$ uses $\\max_{a'} Q(S_{t+1}, a')$. Learns the value of the optimal (greedy) policy, even if actions were chosen differently (e.g., exploratively).",
        epsilonGreedyTitle: "Importance of $\\epsilon$-greedy Exploration:",
        epsilonGreedyDesc: "To ensure the agent explores all relevant state-action pairs and doesn't get stuck in local optima, an $\\epsilon$-greedy strategy is often used. With a small probability $\\epsilon$, the agent chooses a random action instead of the one currently considered optimal (greedy). The value of $\\epsilon$ is often reduced over time (annealing) to shift from exploration to exploitation.",
        vfaTitle: "Outlook: Value Function Approximation (VFA)",
        vfaDesc: "For problems with very large or continuous state spaces, tabular representations of $Q(s,a)$ (a table for each state-action pair) are no longer practical. This is where value function approximation comes into play: Instead of storing Q-values exactly, they are approximated by a parameterized function, e.g., a linear function $\\hat{q}(s,a;\\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{x}(s,a)$ or a neural network $Q(s,a; \\mathbf{w}) \\approx Q^*(s,a)$. The parameters $\\mathbf{w}$ are then learned, often using gradient methods.",
        vfaExample: "A well-known example is the Deep Q-Network (DQN), which uses a deep neural network to approximate the Q-function and has achieved remarkable success in complex environments like Atari games. This opens up the application of RL to problems with high-dimensional sensory inputs.",
        quizSectionTitle: "Knowledge Quiz for Midterm",
        quizJsonLoading: "Loading quiz data...",
        quizLoading: "Loading question...",
        quiz_nextButton: "Next Question",
        quiz_resultsButton: "Show Results",
        quiz_finishedTitle: "Quiz Finished!",
        quiz_scorePrefix: "Your score: ",
        quiz_scoreSuffix_singular: "point",
        quiz_scoreSuffix_plural: "points",
        quiz_scoreOutOf: "out of",
        quiz_restartButton: "Restart Quiz",
        quiz_feedbackCorrect: "Correct!",
        quiz_feedbackIncorrectPrefix: "Incorrect. The correct answer was: ",
        quiz_explainButton: "Generate explanation",
        quiz_explanationLoading: "Loading explanation...",
        quiz_explanationError: "Failed to load explanation.",
        quiz_questionCounterPrefix: "Question ",
        quiz_questionCounterOf: "of",
        quiz_timerLabel: "Time:",
        quiz_finalTimeLabel: "Time taken:",
        githubPatLabel: "GitHub token (optional):",
        githubPatPlaceholder: "ghp_...",
        githubPatSaveButton: "Save",
        githubPatSaved: "Token saved!",
        footerText1: "&copy; 2025 Detailed Reinforcement Learning Infographic.",
        footerText2: "Based on lecture materials and standard RL concepts.",
        chart_mcVsTd_title: "Comparison: MC vs. TD Properties (Scale 1=Low/No, 5=High/Yes)",
        chart_mcVsTd_dataset_mc: "Monte Carlo (MC)",
        chart_mcVsTd_dataset_td: "Temporal Difference (TD)",
        chart_mcVsTd_label_bias: "Bias",
        chart_mcVsTd_label_variance: "Variance",
        chart_mcVsTd_label_updateFreq: "Update Frequency",
        chart_mcVsTd_label_incompleteEp: "Uses Incomplete Episodes",
        chart_mcVsTd_label_initSens: "Initialization Sensitivity",
        chart_mcVsTd_yAxis_lowNo: "1 (Low/No)",
        chart_mcVsTd_yAxis_medium: "3 (Medium)",
        chart_mcVsTd_yAxis_highYes: "5 (High/Yes)",
        tooltip_bias_low_mc: "Low (Unbiased)",
        tooltip_bias_medium_td: "Medium (Biased)",
        tooltip_variance_high_mc: "High",
        tooltip_variance_low_td: "Low",
        tooltip_update_episode_mc: "At episode end",
        tooltip_update_step_td: "After each step",
        tooltip_incomplete_no_mc: "No",
        tooltip_incomplete_yes_td: "Yes (Bootstrapping)",
        tooltip_init_insensitive_mc: "Insensitive",
        tooltip_init_sensitive_td: "Sensitive",
                // ... (alle bisherigen englischen Übersetzungen) ...
        quiz_numQuestionsLabel: "Number of Questions:",
        quiz_numQuestionsAll: "All",
        quiz_startButton: "Start Quiz",
        quiz_reviewTitle: "Incorrect Answers Review",
        quiz_reviewQuestion: "Question:",
        quiz_reviewYourAnswer: "Your Answer:",
        quiz_reviewCorrectAnswer: "Correct Answer:",
        quiz_reviewExplanation: "Explanation:",
        quiz_reviewIncorrectTag: "(Incorrect)",
        // Chart Tooltips (already present, check if needed)
        tooltip_bias_low_mc: "Low (Unbiased)",
        tooltip_bias_medium_td: "Medium (Biased)",
        tooltip_variance_high_mc: "High",
        tooltip_variance_low_td: "Low",
        tooltip_update_episode_mc: "At episode end",
        tooltip_update_step_td: "After each step",
        tooltip_incomplete_no_mc: "No",
        tooltip_incomplete_yes_td: "Yes (Bootstrapping)",
        tooltip_init_insensitive_mc: "Insensitive",
        tooltip_init_sensitive_td: "Sensitive",
    }
};
