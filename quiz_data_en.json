[
  {
    "id": "ch1_q1_rl_definition",
    "type": "multiple-choice",
    "question": "What is the primary goal of Reinforcement Learning?",
    "options": [
      "To accurately model the environment.",
      "To maximize a numerical reward signal over time.",
      "To find hidden structures in unlabeled data.",
      "To correctly classify input data based on examples."
    ],
    "answer": 1
  },
  {
    "id": "ch1_q2_rl_characteristics",
    "type": "multiple-choice",
    "question": "Which of the following characteristics is NOT typical of Reinforcement Learning problems?",
    "options": [
      "The learner is not explicitly told which actions to take.",
      "The learner's actions influence its later inputs (closed-loop).",
      "There is always an external supervisor providing correct actions.",
      "The consequences of actions can extend over long periods."
    ],
    "answer": 2
  },
  {
    "id": "ch1_q3_exploration_exploitation",
    "type": "multiple-choice",
    "question": "The exploration-exploitation dilemma describes the conflict between:",
    "options": [
      "Trying new actions to discover better strategies and applying known good actions to obtain rewards.",
      "Learning quickly and learning accurately.",
      "Building a model and learning values directly.",
      "Short-term and long-term goals."
    ],
    "answer": 0
  },
  {
    "id": "ch1_q4_rl_elements_policy",
    "type": "multiple-choice",
    "question": "Which element of a Reinforcement Learning system defines the agent's behavior at a given time?",
    "options": [
      "The reward signal",
      "The value function",
      "The policy",
      "The model of the environment"
    ],
    "answer": 2
  },
  {
    "id": "ch1_q5_value_function_purpose",
    "type": "multiple-choice",
    "question": "What does a value function specify in Reinforcement Learning?",
    "options": [
      "The immediate reward for an action.",
      "What is good in the long run, i.e., the expected cumulative future reward.",
      "The probability of reaching a certain state.",
      "The optimal action in each state."
    ],
    "answer": 1
  },
  {
    "id": "ch2_q1_bandit_problem",
    "type": "multiple-choice",
    "question": "What is the core problem in the n-armed bandit problem?",
    "options": [
      "Modeling the dynamics of a complex system.",
      "Balancing exploration (testing new options) and exploitation (choosing the current best-known option) to maximize total return.",
      "Predicting the next state based on the current action.",
      "Learning a policy for a sequence of decisions."
    ],
    "answer": 1
  },
  {
    "id": "ch2_q2_action_value_estimation",
    "type": "multiple-choice",
    "question": "How is the value of an action $Q_t(a)$ estimated using the sample-average method in the bandit problem?",
    "options": [
      "As the maximum reward received so far for that action.",
      "As the average of all rewards received after selecting that action.",
      "As the most recent reward received for that action.",
      "As a weighted sum of rewards, with more recent rewards weighted more heavily."
    ],
    "answer": 1
  },
  {
    "id": "ch2_q3_epsilon_greedy",
    "type": "multiple-choice",
    "question": "How does an $\\epsilon$-greedy action selection strategy behave?",
    "options": [
      "It always selects the action with the highest estimated value.",
      "It always selects a random action.",
      "It mostly selects the action with the highest estimated value, but with a small probability $\\epsilon$, it selects a random action.",
      "It always selects the action that has been tried least often."
    ],
    "answer": 2
  },
  {
    "id": "ch2_q4_incremental_update",
    "type": "multiple-choice",
    "question": "The incremental update rule $Q_{k+1} = Q_k + \\frac{1}{k}[R_k - Q_k]$ is used to compute the:",
    "options": [
      "Weighted average of rewards.",
      "Simple average of the first $k$ rewards.",
      "Maximum value of the first $k$ rewards.",
      "TD-error."
    ],
    "answer": 1
  },
  {
    "id": "ch2_q5_optimistic_initialization",
    "type": "multiple-choice",
    "question": "What is the main purpose of \"optimistic initial values\" in bandit problems?",
    "options": [
      "To increase the speed of convergence.",
      "To encourage exploration in the early stages of learning.",
      "To reduce the bias of the estimates.",
      "To decrease the complexity of the algorithm."
    ],
    "answer": 1
  },
  {
    "id": "ch2_q6_ucb_selection",
    "type": "multiple-choice",
    "question": "Upper-Confidence-Bound (UCB) action selection...",
    "options": [
      "Selects actions purely randomly to ensure exploration.",
      "Favors actions that have been chosen infrequently or whose value estimates are uncertain but potentially high.",
      "Is identical to the $\\epsilon$-greedy method.",
      "Requires a complete model of reward probabilities."
    ],
    "answer": 1
  },
  {
    "id": "ch2_q7_gradient_bandit_baseline",
    "type": "yes-no",
    "question": "Is the use of a baseline (e.g., the average reward $\\overline{R}_t$) in gradient bandit algorithms important for reducing the variance of the updates?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch3_q1_agent_interface",
    "type": "multiple-choice",
    "question": "What primarily defines the interaction between agent and environment in the RL framework?",
    "options": [
      "Only the agent's policy.",
      "States, actions, and rewards.",
      "The agent's internal model.",
      "The learning rate and discount factor."
    ],
    "answer": 1
  },
  {
    "id": "ch3_q2_reward_hypothesis",
    "type": "yes-no",
    "question": "Does the \"Reward Hypothesis\" state that all goals can be thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch3_q3_return_episodic",
    "type": "multiple-choice",
    "question": "How is the return $G_t$ typically defined in simple episodic tasks?",
    "options": [
      "As the immediate reward $R_{t+1}$.",
      "As the sum of all rewards from time step $t$ until the end of the episode.",
      "As the average of all rewards in the episode.",
      "As the maximum reward achieved in the episode."
    ],
    "answer": 1
  },
  {
    "id": "ch3_q4_discounting",
    "type": "multiple-choice",
    "question": "What is the purpose of the discount factor $\\gamma$ in the definition of return for continuing tasks?",
    "options": [
      "It weights short-term rewards more heavily than long-term rewards.",
      "It ensures that the sum of future rewards remains finite, even if the task lasts indefinitely.",
      "It increases the agent's exploration rate.",
      "It simplifies the policy calculation."
    ],
    "answer": 1
  },
  {
    "id": "ch3_q5_markov_property",
    "type": "multiple-choice",
    "question": "The Markov property states that the future is...",
    "options": [
      "Completely dependent on the entire history of states and actions.",
      "Dependent only on the current state and action, given the past.",
      "Deterministic if the current state is known.",
      "Always leading to maximum reward."
    ],
    "answer": 1
  },
  {
    "id": "ch3_q6_mdp_definition",
    "type": "multiple-choice",
    "question": "A finite Markov Decision Process (MDP) is completely defined by:",
    "options": [
      "Only the state and action spaces.",
      "The state and action spaces, the transition probabilities $p(s',r|s,a)$, and the discount factor $\\gamma$.",
      "The agent's policy and the value function.",
      "The reward function and the number of episodes."
    ],
    "answer": 1
  },
  {
    "id": "ch3_q7_state_value_function",
    "type": "multiple-choice",
    "question": "The state-value function $v_\\pi(s)$ is defined as:",
    "options": [
      "The immediate reward in state $s$.",
      "The expected return when starting in state $s$ and thereafter following policy $\\pi$.",
      "The probability of reaching state $s$ under policy $\\pi$.",
      "The best action that can be taken in state $s$."
    ],
    "answer": 1
  },
  {
    "id": "ch3_q8_action_value_function",
    "type": "multiple-choice",
    "question": "The action-value function $q_\\pi(s,a)$ is defined as:",
    "options": [
      "The value of action $a$, regardless of the state.",
      "The expected return when starting in state $s$, taking action $a$, and thereafter following policy $\\pi$.",
      "The probability of taking action $a$ in state $s$.",
      "The immediate reward after taking action $a$ in state $s$."
    ],
    "answer": 1
  },
  {
    "id": "ch3_q9_bellman_equation_v",
    "type": "yes-no",
    "question": "Does the Bellman equation for $v_\\pi$ express a relationship between the value of a state and the values of its successor states?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch3_q10_optimal_policy_definition",
    "type": "multiple-choice",
    "question": "An optimal policy $\\pi_*$ is a policy that...",
    "options": [
      "Guarantees the shortest episode.",
      "Achieves the highest expected return for all states compared to all other policies.",
      "Converges the fastest.",
      "Is always deterministic."
    ],
    "answer": 1
  },
  {
    "id": "ch3_q11_bellman_optimality_v",
    "type": "yes-no",
    "question": "Does the Bellman optimality equation for $v_*$ refer to a specific policy?",
    "options": ["No", "Yes"],
    "answer": 0
  },
  {
    "id": "ch3_q12_greedy_optimal",
    "type": "yes-no",
    "question": "Is a policy that acts greedily with respect to the optimal value function $v_*$ always an optimal policy?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch4_q1_dp_assumption",
    "type": "multiple-choice",
    "question": "What fundamental assumption do classical Dynamic Programming (DP) algorithms make about the environment?",
    "options": [
      "The environment does not need to be known.",
      "A perfect model of the environment (MDP) is required.",
      "The environment must be deterministic.",
      "Only sample episodes are needed."
    ],
    "answer": 1
  },
  {
    "id": "ch4_q2_policy_evaluation",
    "type": "multiple-choice",
    "question": "What is the goal of Policy Evaluation (the prediction problem) in DP?",
    "options": [
      "To find the optimal policy directly.",
      "To compute the state-value function $v_\\pi$ for a given policy $\\pi$.",
      "To determine the best action for each state.",
      "To learn a model of the environment."
    ],
    "answer": 1
  },
  {
    "id": "ch4_q3_iterative_policy_evaluation",
    "type": "yes-no",
    "question": "Does iterative policy evaluation use the Bellman equation for $v_\\pi$ as an update rule to obtain successive approximations of the value function?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch4_q4_policy_improvement_theorem",
    "type": "multiple-choice",
    "question": "What does the Policy Improvement Theorem essentially state?",
    "options": [
      "Any policy can be improved by random changes.",
      "If a policy $\\pi'$ is greedy with respect to $q_\\pi$, then $\\pi'$ is at least as good as $\\pi$.",
      "The optimal policy is always deterministic.",
      "Policy evaluation is always necessary before policy improvement."
    ],
    "answer": 1
  },
  {
    "id": "ch4_q5_policy_iteration_process",
    "type": "multiple-choice",
    "question": "Policy Iteration is a process that repeatedly alternates between which two steps?",
    "options": [
      "Model learning and planning.",
      "Exploration and exploitation.",
      "Policy evaluation and policy improvement.",
      "Action selection and reward reception."
    ],
    "answer": 2
  },
  {
    "id": "ch4_q6_value_iteration_update",
    "type": "multiple-choice",
    "question": "Value Iteration is essentially applying which equation as an update rule?",
    "options": [
      "The Bellman expectation equation for $v_\\pi$.",
      "The Bellman optimality equation for $v_*$.",
      "The update rule for Monte Carlo methods.",
      "The update rule for TD-Learning."
    ],
    "answer": 1
  },
  {
    "id": "ch4_q7_asynchronous_dp",
    "type": "multiple-choice",
    "question": "What characterizes asynchronous DP algorithms?",
    "options": [
      "They perform backups for all states simultaneously in each sweep.",
      "They perform backups for states in any order, possibly using outdated values of other states.",
      "They do not require an environment model.",
      "They are always slower than synchronous DP methods."
    ],
    "answer": 1
  },
  {
    "id": "ch4_q8_gpi",
    "type": "yes-no",
    "question": "Is Generalized Policy Iteration (GPI) the general idea of interacting processes of policy evaluation and policy improvement?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch4_q9_bootstrapping_dp",
    "type": "yes-no",
    "question": "Do DP methods update estimates of values based on other estimates (i.e., do they bootstrap)?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch5_q1_mc_model",
    "type": "yes-no",
    "question": "Do Monte Carlo (MC) methods require a complete model of the environment to estimate value functions?",
    "options": ["No", "Yes"],
    "answer": 0
  },
  {
    "id": "ch5_q2_mc_episodes",
    "type": "multiple-choice",
    "question": "For which type of tasks are Monte Carlo methods typically defined to ensure well-defined returns are available?",
    "options": [
      "Only for continuous tasks.",
      "Only for episodic tasks.",
      "For both episodic and continuous tasks without restriction.",
      "Only for tasks with small state spaces."
    ],
    "answer": 1
  },
  {
    "id": "ch5_q3_first_visit_mc",
    "type": "multiple-choice",
    "question": "What does the First-Visit MC method estimate for $v_\\pi(s)$?",
    "options": [
      "The average of returns following all visits to $s$.",
      "The average of returns following the first visit to $s$ in each episode.",
      "The maximum return observed after a visit to $s$.",
      "The return of the last episode in which $s$ was visited."
    ],
    "answer": 1
  },
  {
    "id": "ch5_q4_mc_action_values",
    "type": "yes-no",
    "question": "For MC methods operating without a model, is it particularly useful to estimate action-values $q_\\pi(s,a)$ instead of state-values $v_\\pi(s)$ to derive a policy?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch5_q5_exploring_starts",
    "type": "multiple-choice",
    "question": "What does the assumption of \"Exploring Starts\" mean in MC control methods?",
    "options": [
      "The agent always begins in the same initial state.",
      "Every state-action pair has a non-zero probability of being selected as the starting point of an episode.",
      "The agent only performs exploratory actions in the initial steps.",
      "Starting states are chosen based on their uncertainty."
    ],
    "answer": 1
  },
  {
    "id": "ch5_q6_on_policy_mc_control",
    "type": "multiple-choice",
    "question": "How does On-Policy Monte Carlo Control without exploring starts typically ensure continued exploration?",
    "options": [
      "By always making the policy deterministically greedy.",
      "By using an $\\epsilon$-soft policy (e.g., $\\epsilon$-greedy) which ensures all actions are selected with some probability.",
      "By learning an environment model and using it for planning.",
      "By learning only from the best episodes."
    ],
    "answer": 1
  },
  {
    "id": "ch5_q7_importance_sampling_purpose",
    "type": "multiple-choice",
    "question": "What is the purpose of Importance Sampling in Off-Policy Monte Carlo methods?",
    "options": [
      "To increase the variance of the estimates.",
      "To estimate value functions of a target policy $\\pi$ from episodes generated using a different behavior policy $\\mu$.",
      "To improve the convergence speed of on-policy methods.",
      "To eliminate the need for exploration."
    ],
    "answer": 1
  },
  {
    "id": "ch5_q8_weighted_importance_sampling",
    "type": "yes-no",
    "question": "Does weighted importance sampling often have lower variance in practice compared to ordinary importance sampling?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch5_q9_mc_bootstrap",
    "type": "yes-no",
    "question": "Do Monte Carlo methods bootstrap, i.e., do they update their value estimates based on other value estimates?",
    "options": ["No", "Yes"],
    "answer": 0
  },
  {
    "id": "ch6_q1_td_learning_combination",
    "type": "multiple-choice",
    "question": "Temporal-Difference (TD) Learning is a combination of which two fundamental ideas?",
    "options": [
      "Monte Carlo methods and Supervised Learning.",
      "Monte Carlo methods and Dynamic Programming.",
      "Dynamic Programming and Genetic Algorithms.",
      "Policy Gradient methods and Value-based methods."
    ],
    "answer": 1
  },
  {
    "id": "ch6_q2_td0_update_target",
    "type": "multiple-choice",
    "question": "What is the target for the update of $V(S_t)$ in the TD(0) algorithm?",
    "options": [
      "The full return $G_t$.",
      "$R_{t+1} + \\gamma V(S_{t+1})$.",
      "$R_{t+1}$.",
      "$\\max_a Q(S_{t+1}, a)$."
    ],
    "answer": 1
  },
  {
    "id": "ch6_q3_td_bootstrapping",
    "type": "yes-no",
    "question": "Do TD methods use bootstrapping, i.e., update estimates based on other learned estimates?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch6_q4_td_advantage_mc_online",
    "type": "yes-no",
    "question": "Can TD methods learn online and incrementally after each time step, unlike MC methods which must wait until the end of an episode?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch6_q5_batch_td0_convergence",
    "type": "multiple-choice",
    "question": "To what does Batch TD(0) converge with a sufficiently small $\\alpha$?",
    "options": [
      "To the estimates that minimize mean-squared error on the training set.",
      "To the estimates that would be exactly correct for the maximum-likelihood model of the Markov process (Certainty-Equivalence Estimate).",
      "Always to the optimal value function $v_*$.",
      "To the same values as Batch Monte Carlo."
    ],
    "answer": 1
  },
  {
    "id": "ch6_q6_sarsa_name",
    "type": "multiple-choice",
    "question": "What do the letters in the name of the SARSA algorithm stand for?",
    "options": [
      "State, Action, Reward, State, Algorithm",
      "State, Action, Reward, State, Action",
      "Sample, Action, Return, State, Action",
      "Stochastic, Action, Reward, Sample, Approximation"
    ],
    "answer": 1
  },
  {
    "id": "ch6_q7_sarsa_on_policy",
    "type": "yes-no",
    "question": "Is SARSA an on-policy algorithm, meaning it learns the action-value function for the policy it is currently using to make decisions?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch6_q8_qlearning_off_policy",
    "type": "yes-no",
    "question": "Is Q-Learning an off-policy algorithm, meaning it can learn the optimal action-value function $q_*$ regardless of the policy used for exploration?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch6_q9_qlearning_update_target",
    "type": "multiple-choice",
    "question": "Which term does Q-Learning use in its update target that distinguishes it from SARSA and enables off-policy learning?",
    "options": [
      "$Q(S_{t+1}, A_{t+1})$",
      "$\\sum_a \\pi(a|S_{t+1})Q(S_{t+1}, a)$",
      "$\\max_a Q(S_{t+1}, a)$",
      "$V(S_{t+1})$"
    ],
    "answer": 2
  },
  {
    "id": "ch6_q10_cliff_walking_sarsa_qlearning",
    "type": "multiple-choice",
    "question": "In the 'Cliff Walking' example, Q-Learning learns the optimal (risky) path, while SARSA learns a longer but safer path. Why is this?",
    "options": [
      "Q-Learning uses a higher learning rate.",
      "SARSA is a Monte Carlo algorithm.",
      "Q-Learning (off-policy) learns about the greedy policy, while SARSA (on-policy) incorporates the effects of $\\epsilon$-greedy exploration into its value estimates.",
      "SARSA converges to the optimum faster."
    ],
    "answer": 2
  },
  {
    "id": "ch1_q6_tictactoe_td_update",
    "type": "multiple-choice",
    "question": "In the Tic-Tac-Toe example, how is the value estimate of a state $V(s)$ typically updated after a greedy move to $s'$ (TD method)?",
    "options": [
      "$V(s) \\leftarrow V(s')$",
      "$V(s) \\leftarrow V(s) + \\alpha[V(s') - V(s)]$",
      "$V(s) \\leftarrow \\alpha V(s')$",
      "$V(s) \\leftarrow \\text{average of all previous } V(s')$"
    ],
    "answer": 1
  },
  {
    "id": "ch3_q13_episodic_vs_continuing",
    "type": "multiple-choice",
    "question": "What is the main difference between episodic and continuing tasks in RL?",
    "options": [
      "Episodic tasks always have deterministic transitions.",
      "Continuing tasks do not have clearly defined terminal states or natural breaks.",
      "Episodic tasks do not use a discount factor.",
      "Continuing tasks can only be solved with model-based methods."
    ],
    "answer": 1
  },
  {
    "id": "ch4_q10_dp_efficiency",
    "type": "yes-no",
    "question": "Are DP methods generally exponentially faster than a direct search in the space of all possible policies to find an optimal policy?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch5_q10_mc_vs_dp_model",
    "type": "yes-no",
    "question": "Can Monte Carlo methods learn without a model of the environment's dynamics, unlike DP methods?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch6_q11_td_vs_mc_efficiency",
    "type": "multiple-choice",
    "question": "Which of the following is often an advantage of TD methods over MC methods on stochastic tasks?",
    "options": [
      "TD methods are always unbiased.",
      "TD methods typically have lower variance and often converge faster.",
      "TD methods do not require exploration.",
      "MC methods can be used for continuous tasks without modification."
    ],
    "answer": 1
  },
   {
    "id": "ch6_q12_double_q_maximization_bias",
    "type": "yes-no",
    "question": "Does Double Q-Learning help to mitigate the maximization bias that can occur in standard Q-Learning?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch6_q13_expected_sarsa_variance",
    "type": "yes-no",
    "question": "Is a primary motivation for Expected SARSA to reduce variance compared to standard SARSA by taking an expectation over the next action?",
    "options": ["Yes", "No"],
    "answer": 0
  },
  {
    "id": "ch9_q1_vfa_need",
    "type": "multiple-choice",
    "question": "When is Value Function Approximation (VFA) primarily needed in Reinforcement Learning?",
    "options": [
      "When the reward signals are sparse.",
      "When the state and/or action spaces are too large for tabular methods.",
      "When an exact model of the environment is available.",
      "Only for off-policy learning methods."
    ],
    "answer": 1
  }
]
