[
  {
    "question": "What is the main goal of an RL agent?",
    "options": [
      "Minimizing computation time",
      "Maximizing cumulative reward",
      "Creating a perfect model of the environment",
      "Executing as many actions as possible"
    ],
    "answer": 1,
    "explanation": "The primary goal of the agent is to maximize the sum of received rewards over time."
  },
  {
    "question": "What does MDP stand for?",
    "options": [
      "Maximum Discounting Process",
      "Markov Decision Parameter",
      "Markov Decision Process",
      "Model Data Path"
    ],
    "answer": 2,
    "explanation": "MDP stands for Markov Decision Process, a standard formalism for RL problems."
  },
  {
    "question": "What is the value of the discount factor $\\gamma$ for an extremely \"myopic\" agent?",
    "options": [
      "$\\gamma \\approx 1$",
      "$\\gamma = 0.5$",
      "$\\gamma \\approx 0$",
      "$\\gamma = -1$"
    ],
    "answer": 2,
    "explanation": "A $\\gamma \\approx 0$ makes the agent myopic, as future rewards are heavily discounted."
  },
  {
      "question": "What does the Bellman optimality equation for $V^*(s)$ describe in Value Iteration?",
      "options": [
          "$V_{k+1}(s) \\leftarrow \\sum_a \\pi(a|s) \\sum_{s',r} P(s',r|s,a) [r + \\gamma V_k(s')]$",
          "$V_{k+1}(s) \\leftarrow \\max_a \\sum_{s',r} P(s',r|s,a) [r + \\gamma V_k(s')]$",
          "$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$",
          "$V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t))$"
      ],
      "answer": 1,
      "explanation": "Value Iteration uses the Bellman optimality equation: $V_{k+1}(s) \\leftarrow \\max_a \\sum_{s',r} P(s',r|s,a) [r + \\gamma V_k(s')]$."
  },
  {
      "question": "Which algorithm is \"On-Policy\"?",
      "options": [
          "Q-Learning",
          "SARSA",
          "Deep Q-Network (DQN)",
          "All of the above"
      ],
      "answer": 1,
      "explanation": "SARSA is an on-policy TD control algorithm because it learns the policy it is executing, including exploratory actions."
  }
]
