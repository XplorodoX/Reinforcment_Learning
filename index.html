<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reinforcement Learning Infografik - Vertieft mit Quiz</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          tags: 'ams'
        },
        svg: {
          fontCache: 'global'
        }
      };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #e0f2fe; /* Light blue background */
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 300px;
            max-height: 400px;
        }
        @media (min-width: 768px) {
            .chart-container {
                height: 350px;
            }
        }
        .stat-card {
            background-color: #ffffff;
            border-radius: 0.5rem;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            padding: 1.5rem;
            text-align: center;
        }
        .stat-value {
            font-size: 2.5rem;
            font-weight: 700;
            color: #0056B3;
        }
        .stat-label {
            font-size: 1rem;
            color: #007BFF;
        }
        .concept-box {
            background-color: #caf0f8;
            border: 1px solid #90e0ef;
            border-radius: 0.5rem;
            padding: 1.5rem; /* Increased padding */
            margin-bottom: 1.5rem; /* Increased margin */
        }
        .flow-step {
            background-color: #ffffff;
            border: 2px solid #007BFF;
            border-radius: 0.5rem;
            padding: 1rem;
            text-align: center;
            margin-bottom: 0.5rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }
        .flow-arrow {
            font-size: 1.5rem;
            color: #007BFF;
            margin: 0.25rem 0;
            text-align: center;
        }
        h1, h2, h3, h4 {
            color: #0056B3;
        }
        .math-formula {
            font-family: 'Inter', sans-serif;
            font-size: 1.05em;
            background-color: #f0f9ff;
            padding: 0.2em 0.4em;
            border-radius: 0.25rem;
            color: #022c43;
            display: inline-block;
        }
        .equation-block {
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            padding: 1rem;
            border-radius: 0.25rem;
            margin-top: 0.5rem;
            margin-bottom: 0.5rem;
            overflow-x: auto;
        }
        .explanation-point {
            margin-bottom: 0.75rem;
            padding-left: 1rem;
            position: relative;
        }
        .explanation-point::before {
            content: "üîπ";
            position: absolute;
            left: -0.25rem;
            color: #007BFF;
        }
        .quiz-container {
            background-color: #ffffff;
            border-radius: 0.75rem;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
            padding: 2rem;
        }
        .quiz-question {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
            color: #0056B3;
        }
        .quiz-option {
            display: block;
            background-color: #f0f9ff;
            border: 1px solid #90e0ef;
            border-radius: 0.375rem;
            padding: 0.75rem 1rem;
            margin-bottom: 0.75rem;
            cursor: pointer;
            transition: background-color 0.2s ease-in-out, border-color 0.2s ease-in-out;
        }
        .quiz-option:hover {
            background-color: #caf0f8;
            border-color: #007BFF;
        }
        .quiz-option.selected {
            background-color: #007BFF;
            color: white;
            border-color: #0056B3;
        }
        .quiz-feedback {
            margin-top: 1rem;
            padding: 0.75rem;
            border-radius: 0.375rem;
            font-weight: 500;
        }
        .feedback-correct {
            background-color: #d1fae5; /* Tailwind green-100 */
            color: #065f46; /* Tailwind green-800 */
            border: 1px solid #6ee7b7; /* Tailwind green-300 */
        }
        .feedback-incorrect {
            background-color: #fee2e2; /* Tailwind red-100 */
            color: #991b1b; /* Tailwind red-800 */
            border: 1px solid #fca5a5; /* Tailwind red-300 */
        }
        .quiz-navigation button {
            background-color: #007BFF;
            color: white;
            padding: 0.5rem 1.5rem;
            border-radius: 0.375rem;
            font-weight: 600;
            transition: background-color 0.2s;
        }
        .quiz-navigation button:hover {
            background-color: #0056B3;
        }
        .quiz-navigation button:disabled {
            background-color: #a0aec0; /* Tailwind gray-400 */
            cursor: not-allowed;
        }
    </style>
</head>
<body class="text-gray-800">

    <header class="bg-blue-600 text-white p-6 shadow-lg">
        <div class="container mx-auto text-center">
            <h1 class="text-4xl font-bold">Reinforcement Learning: Vertiefte Einblicke & Quiz</h1>
            <p class="mt-2 text-lg">Detaillierte Konzepte, Algorithmen und interaktives Quiz</p>
        </div>
    </header>

    <main class="container mx-auto p-4 md:p-8 space-y-16">

        <section id="intro-rl-mdp" class="bg-white p-8 rounded-lg shadow-xl">
            <h2 class="text-3xl font-semibold mb-8 text-center text-blue-700">1. Grundlagen: RL & Markov-Entscheidungsprozesse (MDPs)</h2>
            <p class="text-lg mb-6 text-gray-700 leading-relaxed">
                Reinforcement Learning (RL) ist ein Paradigma des maschinellen Lernens, bei dem ein autonomer <strong>Agent</strong> lernt, optimale Sequenzen von Entscheidungen (Aktionen) in einer komplexen und oft unsicheren <strong>Umgebung</strong> zu treffen. Das prim√§re Ziel des Agenten ist es, die Summe der erhaltenen <strong>Belohnungen</strong> √ºber die Zeit zu maximieren. Dieser Lernprozess ist fundamental iterativ und erfahrungsbasiert.
            </p>

            <div class="grid md:grid-cols-2 gap-8 mb-10 items-start">
                <div class="concept-box bg-blue-50 border-blue-200 p-6">
                    <h3 class="text-xl font-semibold mb-4 text-blue-600">Der Agent-Umgebung-Interaktionszyklus</h3>
                    <div class="space-y-3">
                        <div class="flow-step">1. Agent beobachtet aktuellen Zustand $S_t$ der Umgebung.</div>
                        <div class="flow-arrow">‚¨áÔ∏è</div>
                        <div class="flow-step">2. Agent w√§hlt eine Aktion $A_t$ basierend auf seiner aktuellen Strategie (Policy $\pi$).</div>
                        <div class="flow-arrow">‚û°Ô∏è</div>
                        <div class="flow-step">3. Umgebung reagiert: Agent erh√§lt eine Belohnung $R_{t+1}$ und beobachtet den neuen Zustand $S_{t+1}$.</div>
                        <div class="flow-arrow">üîÑ</div>
                        <div class="flow-step">4. Agent aktualisiert seine Policy $\pi$ basierend auf der Erfahrung $(S_t, A_t, R_{t+1}, S_{t+1})$.</div>
                    </div>
                    <p class="mt-5 text-sm text-gray-600">Dieser Zyklus wiederholt sich, wodurch der Agent schrittweise lernt, bessere Entscheidungen zu treffen.</p>
                </div>

                <div class="space-y-6">
                    <div class="stat-card p-6">
                        <div class="stat-value">üéØ</div>
                        <div class="stat-label mt-2">Maximierung des kumulativen Returns</div>
                        <p class="text-sm text-gray-600 mt-3">Der Return $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$ ist die Summe der diskontierten zuk√ºnftigen Belohnungen. Die <strong>Reward Hypothesis</strong> besagt, dass alle Ziele durch die Maximierung des erwarteten kumulativen Rewards formuliert werden k√∂nnen.</p>
                    </div>
                     <div class="concept-box bg-blue-50 border-blue-200 p-4">
                        <h4 class="text-lg font-semibold text-blue-600 mb-2">Der Diskontierungsfaktor $\gamma$</h4>
                        <p class="text-sm text-gray-700 mb-1">Steuert die Wichtigkeit zuk√ºnftiger Belohnungen:</p>
                        <ul class="list-none space-y-1 text-sm">
                            <li class="explanation-point">$\gamma \approx 0$: Agent ist "kurzsichtig", fokussiert auf unmittelbare Belohnungen.</li>
                            <li class="explanation-point">$\gamma \approx 1$: Agent ist "weitsichtig", ber√ºcksichtigt zuk√ºnftige Belohnungen stark.</li>
                        </ul>
                        <p class="text-xs text-gray-500 mt-2">Notwendig f√ºr kontinuierliche Aufgaben, um unendliche Returns zu vermeiden.</p>
                    </div>
                </div>
            </div>

            <h3 class="text-2xl font-semibold mb-4 text-blue-600">Der Markov-Entscheidungsprozess (MDP)</h3>
            <p class="mb-4 text-gray-700 leading-relaxed">Ein MDP ist der Standardformalismus f√ºr RL-Probleme, die die Markov-Eigenschaft erf√ºllen. Er wird durch ein Tupel $(S, A, P, R, \gamma)$ definiert:</p>
            <div class="grid md:grid-cols-2 gap-x-8 gap-y-4 mb-6">
                <div class="explanation-point"><strong>S (Zustandsraum):</strong> Eine Menge aller m√∂glichen Zust√§nde. Z.B. Positionen auf einem Schachbrett, Gelenkwinkel eines Roboters.</div>
                <div class="explanation-point"><strong>A (Aktionsraum):</strong> Eine Menge aller m√∂glichen Aktionen. Z.B. Z√ºge im Schach, Motorkommandos f√ºr einen Roboter.</div>
                <div class="explanation-point"><strong>P (Zustands√ºbergangsmodell):</strong> $P(s'|s, a) = Pr\{S_{t+1}=s' | S_t=s, A_t=a\}$. Definiert die Dynamik der Umgebung.</div>
                <div class="explanation-point"><strong>R (Belohnungsfunktion):</strong> $R(s, a, s') = E[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$. Definiert das Ziel des Agenten.</div>
            </div>
            <div class="concept-box bg-indigo-50 border-indigo-200 p-5">
                <h4 class="text-lg font-semibold text-indigo-700 mb-2">Die Markov-Eigenschaft</h4>
                <p class="text-gray-700">"Die Zukunft ist unabh√§ngig von der Vergangenheit, gegeben die Gegenwart."</p>
                <p class="mt-2 text-sm equation-block text-indigo-800">$Pr\{S_{t+1}=s', R_{t+1}=r | S_t, A_t, S_{t-1}, A_{t-1}, \dots\} = Pr\{S_{t+1}=s', R_{t+1}=r | S_t, A_t\}$</p>
                <p class="mt-2 text-sm text-gray-600">Der aktuelle Zustand $S_t$ enth√§lt alle relevanten Informationen aus der Historie, um die n√§chste Zustands- und Belohnungswahrscheinlichkeit zu bestimmen. Dies ist eine starke Vereinfachung, die viele RL-Algorithmen erm√∂glicht.</p>
            </div>

            <div class="mt-8 p-6 bg-yellow-50 border border-yellow-300 rounded-lg">
                <h4 class="text-xl font-semibold text-yellow-700 mb-3">Kernherausforderungen und Konzepte</h4>
                <div class="grid md:grid-cols-2 gap-6">
                    <div>
                        <h5 class="text-lg font-semibold text-yellow-600 mb-1">Credit Assignment</h5>
                        <p class="text-sm text-gray-700">Wie k√∂nnen Belohnungen (oder Bestrafungen), die erst sp√§t in einer Sequenz von Aktionen auftreten, korrekt den fr√ºheren Aktionen zugeordnet werden, die urs√§chlich daf√ºr waren?</p>
                    </div>
                    <div>
                        <h5 class="text-lg font-semibold text-yellow-600 mb-1">Exploration vs. Exploitation</h5>
                        <p class="text-sm text-gray-700">Der Agent muss entscheiden, ob er neue Aktionen ausprobiert, um mehr √ºber die Umgebung zu lernen (Exploration), oder ob er die aktuell besten bekannten Aktionen ausf√ºhrt, um die Belohnung zu maximieren (Exploitation).</p>
                    </div>
                    <div>
                        <h5 class="text-lg font-semibold text-yellow-600 mb-1">Policy ($\pi$): Die Strategie des Agenten</h5>
                        <p class="text-sm text-gray-700">Eine Abbildung von Zust√§nden zu Aktionen. Deterministisch: $\pi(s) = a$. Stochastisch: $\pi(a|s) = Pr\{A_t=a | S_t=s\}$.</p>
                    </div>
                    <div>
                        <h5 class="text-lg font-semibold text-yellow-600 mb-1">Wertfunktionen ($V^\pi, Q^\pi$): Wie gut ist ein Zustand/Aktion?</h5>
                        <p class="text-sm text-gray-700">$V^\pi(s)$: Erwarteter Return, wenn man in Zustand $s$ startet und Policy $\pi$ folgt. $Q^\pi(s,a)$: Erwarteter Return, wenn man in Zustand $s$ Aktion $a$ ausf√ºhrt und danach Policy $\pi$ folgt.</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="dp" class="bg-white p-8 rounded-lg shadow-xl">
            <h2 class="text-3xl font-semibold mb-8 text-center text-blue-700">2. Dynamische Programmierung (DP): Planung mit perfektem Modell</h2>
            <p class="text-lg mb-6 text-gray-700 leading-relaxed">
                Dynamische Programmierung (DP) umfasst Algorithmen, die eine optimale Policy $\pi^*$ f√ºr einen gegebenen MDP berechnen k√∂nnen, vorausgesetzt, das Modell der Umgebung (√úbergangswahrscheinlichkeiten $P$ und Belohnungen $R$) ist vollst√§ndig bekannt. DP-Methoden nutzen Wertfunktionen, um die Suche nach guten Policies zu strukturieren und basieren auf den Bellman-Gleichungen.
            </p>
            <div class="concept-box bg-blue-50 border-blue-200 p-6 mb-8">
                <h3 class="text-xl font-semibold mb-3 text-blue-600">Bellman-Optimalit√§tsprinzip</h3>
                <p class="text-gray-700 italic">"Eine optimale Policy hat die Eigenschaft, dass, was auch immer der Anfangszustand und die erste Entscheidung sind, die verbleibenden Entscheidungen eine optimale Policy bez√ºglich des aus der ersten Entscheidung resultierenden Zustands darstellen m√ºssen." (Bellman, 1957)</p>
                <p class="mt-3 text-sm text-gray-600">Einfach gesagt: Wenn der beste Pfad von A nach C √ºber B f√ºhrt, dann muss der Teilpfad von B nach C auch der beste Pfad von B nach C sein.</p>
            </div>

            <div class="grid md:grid-cols-2 gap-8 mb-8">
                <div class="concept-box bg-green-50 border-green-200 p-6">
                    <h3 class="text-xl font-semibold mb-4 text-green-700">Policy Iteration (Politikiteration)</h3>
                    <p class="text-sm text-gray-700 mb-3">Ein iterativer Algorithmus, der zwischen zwei Schritten wechselt:</p>
                    <div class="space-y-3">
                        <div class="flow-step bg-green-100 border-green-500"><strong>1. Policy Evaluation (Bewertung):</strong> Berechne die Zustands-Wertfunktion $V^\pi(s)$ f√ºr die aktuelle Policy $\pi$. Dies geschieht durch iteratives Anwenden der Bellman-Erwartungsgleichung bis zur Konvergenz: <br> <span class="math-formula text-xs">$V_{k+1}^\pi(s) \leftarrow \sum_a \pi(a|s) \sum_{s',r} P(s',r|s,a) [r + \gamma V_k^\pi(s')]$</span></div>
                        <div class="flow-arrow text-green-600">‚¨áÔ∏è</div>
                        <div class="flow-step bg-green-100 border-green-500"><strong>2. Policy Improvement (Verbesserung):</strong> Verbessere die Policy, indem f√ºr jeden Zustand $s$ gierig die Aktion gew√§hlt wird, die $Q^\pi(s,a)$ maximiert: <br> <span class="math-formula text-xs">$\pi'(s) \leftarrow \text{argmax}_a \sum_{s',r} P(s',r|s,a) [r + \gamma V^\pi(s')]$</span></div>
                    </div>
                    <p class="mt-4 text-sm text-gray-600">Dieser Prozess konvergiert garantiert zur optimalen Policy $\pi^*$. "Sweeps" beziehen sich auf vollst√§ndige Durchl√§ufe √ºber alle Zust√§nde w√§hrend der Evaluation oder Iteration.</p>
                </div>
                <div class="concept-box bg-purple-50 border-purple-200 p-6">
                    <h3 class="text-xl font-semibold mb-4 text-purple-700">Value Iteration (Wertiteration)</h3>
                    <p class="text-sm text-gray-700 mb-3">Kombiniert im Wesentlichen einen Schritt der Policy Evaluation und einen Schritt der Policy Improvement in einer einzigen Update-Regel. Sie iteriert direkt √ºber die Bellman-Optimalit√§tsgleichung f√ºr $V^*(s)$:</p>
                     <div class="flow-step bg-purple-100 border-purple-500 equation-block">
                        <span class="math-formula">$V_{k+1}(s) \leftarrow \max_a \sum_{s',r} P(s',r|s,a) [r + \gamma V_k(s')]$</span>
                     </div>
                     <p class="mt-4 text-sm text-gray-600">Konvergiert ebenfalls zur optimalen Wertfunktion $V^*$, aus der die optimale Policy $\pi^*$ dann durch eine einmalige gierige Auswahl extrahiert werden kann.</p>
                </div>
            </div>
            <div class="concept-box">
                <h3 class="text-xl font-semibold mb-2 text-blue-600">Beziehung zwischen $V^\pi$ und $Q^\pi$</h3>
                <p class="text-gray-700 mb-2">Die Zustands-Wertfunktion und die Aktions-Wertfunktion sind eng miteinander verbunden:</p>
                <div class="equation-block text-sm">
                    <span class="math-formula">$V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s,a)$</span> (wenn $\pi$ stochastisch ist)
                    <br>
                    <span class="math-formula">$V^\pi(s) = Q^\pi(s, \pi(s))$</span> (wenn $\pi$ deterministisch ist)
                    <br>
                    <span class="math-formula">$Q^\pi(s,a) = \sum_{s',r} P(s',r|s,a) [r + \gamma V^\pi(s')]$</span>
                </div>
            </div>
             <div class="mt-8 p-6 bg-red-50 border border-red-200 rounded-lg">
                <h4 class="text-xl font-semibold text-red-700 mb-3">Wann ist DP anwendbar?</h4>
                <ul class="list-disc list-inside text-gray-700 space-y-1">
                    <li>Wenn ein vollst√§ndiges und genaues Modell der Umgebung ($P$ und $R$) verf√ºgbar ist.</li>
                    <li>Eher f√ºr Planungsprobleme als f√ºr direktes Lernen aus Interaktion.</li>
                    <li>Obwohl rechenintensiv f√ºr sehr gro√üe Probleme, bilden DP-Konzepte die theoretische Grundlage f√ºr viele andere RL-Algorithmen.</li>
                </ul>
            </div>
        </section>

        <section id="model-free-prediction" class="bg-white p-8 rounded-lg shadow-xl">
            <h2 class="text-3xl font-semibold mb-8 text-center text-blue-700">3. Modellfreie Vorhersage: Lernen ohne Modell</h2>
            <p class="text-lg mb-6 text-gray-700 leading-relaxed">
                In vielen realen Szenarien ist ein exaktes Modell der Umgebung nicht verf√ºgbar. Modellfreie Vorhersagemethoden erm√∂glichen es, Wertfunktionen ($V^\pi$ oder $Q^\pi$) f√ºr eine gegebene Policy $\pi$ direkt aus Erfahrungsepisoden zu sch√§tzen, die durch Interaktion mit der Umgebung generiert werden.
            </p>
            <div class="grid md:grid-cols-2 gap-8">
                <div>
                    <h3 class="text-2xl font-semibold mb-4 text-blue-600">a) Monte Carlo (MC) Methoden</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">MC-Methoden lernen, indem sie den durchschnittlichen Return nach Besuchen eines Zustands √ºber viele vollst√§ndige Episoden berechnen.
                    <br><strong>First-Visit MC:</strong> Ber√ºcksichtigt nur den Return nach dem ersten Besuch eines Zustands $s$ in einer Episode.
                    <br><strong>Every-Visit MC:</strong> Ber√ºcksichtigt den Return nach jedem Besuch eines Zustands $s$ in einer Episode.</p>
                    <div class="concept-box">
                        <h4 class="text-lg font-semibold mb-1 text-blue-500">Update-Regel (inkrementell f√ºr $V(S_t)$):</h4>
                        <span class="math-formula">$V(S_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t))$</span>
                        <p class="text-xs text-gray-500 mt-1">Hier ist $G_t$ der tats√§chlich beobachtete (vollst√§ndige) Return ab Zeitschritt $t$ bis zum Ende der Episode. $\alpha$ ist die Lernrate.</p>
                    </div>
                    <div class="mt-4">
                        <h4 class="text-md font-semibold text-green-600">Vorteile:</h4>
                        <ul class="list-disc list-inside text-sm text-gray-600"><li>Unverzerrt (kein Bootstrapping-Bias).</li><li>Einfach zu verstehen und zu implementieren.</li></ul>
                        <h4 class="text-md font-semibold text-red-600 mt-2">Nachteile:</h4>
                        <ul class="list-disc list-inside text-sm text-gray-600"><li>Hohe Varianz, da der Return von vielen zuf√§lligen Aktionen und √úberg√§ngen abh√§ngt.</li><li>Funktioniert nur f√ºr episodische (terminierende) Aufgaben.</li><li>Updates erfolgen erst am Ende einer Episode.</li></ul>
                    </div>
                </div>
                <div>
                    <h3 class="text-2xl font-semibold mb-4 text-blue-600">b) Temporal Difference (TD) Learning</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed">TD-Methoden lernen von jedem Schritt, indem sie ihre aktuelle Sch√§tzung basierend auf der Sch√§tzung des n√§chsten Zustands anpassen ‚Äì ein Prozess, der als <strong>Bootstrapping</strong> bezeichnet wird. Sie ben√∂tigen keine vollst√§ndigen Episoden.</p>
                     <div class="concept-box">
                        <h4 class="text-lg font-semibold mb-1 text-blue-500">TD(0) Update-Regel f√ºr $V(S_t)$:</h4>
                        <span class="math-formula">$V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$</span>
                        <p class="text-xs text-gray-500 mt-1">Der Term $R_{t+1} + \gamma V(S_{t+1})$ ist das <strong>TD-Target</strong> (eine Sch√§tzung des Returns), und $[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$ ist der <strong>TD-Fehler</strong>.</p>
                    </div>
                    <div class="mt-4">
                        <h4 class="text-md font-semibold text-green-600">Vorteile:</h4>
                        <ul class="list-disc list-inside text-sm text-gray-600"><li>Geringere Varianz als MC.</li><li>Kann online lernen (nach jedem Schritt).</li><li>Funktioniert auch f√ºr kontinuierliche Aufgaben.</li></ul>
                        <h4 class="text-md font-semibold text-red-600 mt-2">Nachteile:</h4>
                        <ul class="list-disc list-inside text-sm text-gray-600"><li>Verzerrt, da die Sch√§tzung auf einer anderen Sch√§tzung ($V(S_{t+1})$) basiert.</li><li>Empfindlicher gegen√ºber der Initialisierung der Werte.</li></ul>
                    </div>
                </div>
            </div>
            <div class="mt-8 concept-box bg-teal-50 border-teal-200 p-6">
                <h3 class="text-xl font-semibold text-teal-700 mb-3">N-Step TD Methoden</h3>
                <p class="text-gray-700 mb-2">Diese Methoden schlagen eine Br√ºcke zwischen MC und TD(0). Anstatt nur einen Schritt (TD(0)) oder die gesamte Episode (MC) f√ºr das Update zu betrachten, verwenden n-Step TD Methoden einen Return √ºber $n$ Schritte:</p>
                <div class="equation-block text-sm">
                    <span class="math-formula">$G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n})$</span>
                </div>
                <p class="text-gray-700 mt-2">Die Update-Regel lautet dann:</p>
                 <div class="equation-block text-sm">
                    <span class="math-formula">$V(S_t) \leftarrow V(S_t) + \alpha [G_{t:t+n} - V(S_t)]$</span>
                </div>
                <p class="text-sm text-gray-600 mt-2">Der Parameter $n$ erlaubt einen Trade-off zwischen dem Bias von TD(0) und der Varianz von MC.</p>
            </div>
             <div class="mt-10 text-center">
                <h3 class="text-2xl font-semibold mb-4 text-blue-600">Visueller Vergleich: MC vs. TD Update</h3>
                 <div class="chart-container h-72 md:h-96">
                    <canvas id="mcVsTdChart"></canvas>
                </div>
                <p class="text-md text-gray-600 mt-4">Das Diagramm illustriert die unterschiedlichen St√§rken und Schw√§chen von MC- und TD-Lernans√§tzen bez√ºglich Bias, Varianz und Update-Frequenz.</p>
            </div>
        </section>

        <section id="model-free-control" class="bg-white p-8 rounded-lg shadow-xl">
            <h2 class="text-3xl font-semibold mb-8 text-center text-blue-700">4. Modellfreie Steuerung: Optimale Policies ohne Modell finden</h2>
            <p class="text-lg mb-6 text-gray-700 leading-relaxed">
                Modellfreie Steuerungsmethoden erweitern die Ideen der modellfreien Vorhersage, um nicht nur Wertfunktionen zu sch√§tzen, sondern auch aktiv eine (nahezu) optimale Policy $\pi^*$ zu finden. Da kein Umgebungsmodell vorhanden ist, lernen diese Methoden typischerweise die Aktions-Wertfunktion $Q(s,a)$, da diese direkt angibt, wie gut es ist, eine bestimmte Aktion $a$ in einem Zustand $s$ auszuf√ºhren. Die Policy wird dann oft $\epsilon$-gierig auf Basis dieser $Q$-Werte abgeleitet.
            </p>
            <div class="concept-box bg-yellow-50 border-yellow-200 p-6 mb-10">
                <h3 class="text-xl font-semibold mb-4 text-yellow-700 text-center">Generalisierte Policy Iteration (GPI) f√ºr Modellfreie Steuerung</h3>
                <div class="flex flex-col md:flex-row justify-around items-center space-y-4 md:space-y-0 md:space-x-6">
                    <div class="flow-step bg-yellow-100 border-yellow-500 p-5 text-center">
                        <h4 class="font-semibold mb-1">1. Policy Evaluation (Sch√§tzung)</h4>
                        Sch√§tze $Q^\pi(s,a)$ f√ºr die aktuelle Policy $\pi$ (z.B. mittels MC oder TD Updates auf $Q$-Werte).
                    </div>
                    <div class="text-3xl font-bold text-yellow-600 self-center">üîÑ</div>
                    <div class="flow-step bg-yellow-100 border-yellow-500 p-5 text-center">
                        <h4 class="font-semibold mb-1">2. Policy Improvement (Verbesserung)</h4>
                        Aktualisiere $\pi$, indem $\epsilon$-gierig bez√ºglich der aktuellen $Q$-Werte agiert wird: <br> $\pi(s) \leftarrow \text{argmax}_a Q(s,a)$ (mit $\epsilon$ W'keit zuf√§llig).
                    </div>
                </div>
                <p class="mt-5 text-sm text-gray-600 text-center">Dieser iterative Prozess konvergiert typischerweise gegen die optimale Aktions-Wertfunktion $Q^*$ und die optimale Policy $\pi^*$. Die Notwendigkeit von $Q(s,a)$ ergibt sich daraus, dass ohne Modell $P(s'|s,a)$ die $V(s)$-Funktion allein nicht ausreicht, um eine gierige Aktion zu bestimmen.</p>
            </div>

            <div class="grid md:grid-cols-2 gap-10">
                <div class="concept-box bg-sky-50 border-sky-200 p-6">
                    <h3 class="text-2xl font-semibold mb-4 text-sky-700">SARSA (On-Policy TD Control)</h3>
                    <p class="mb-3 text-gray-700 leading-relaxed">SARSA ist ein On-Policy TD-Algorithmus. "On-Policy" bedeutet, dass die $Q$-Werte f√ºr die Policy gesch√§tzt und verbessert werden, die der Agent tats√§chlich ausf√ºhrt (inklusive explorativer Aktionen).</p>
                    <div class="equation-block">
                        <h4 class="text-lg font-semibold mb-1 text-sky-600">Update-Regel f√ºr $Q(S_t, A_t)$:</h4>
                        <span class="math-formula block">$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$</span>
                    </div>
                    <p class="mt-3 text-sm text-gray-600">
                        Der Name SARSA kommt von der Sequenz der Ereignisse im Update: $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$. $A_{t+1}$ wird gem√§√ü der aktuellen (oft $\epsilon$-greedy) Policy gew√§hlt.
                    </p>
                </div>
                <div class="concept-box bg-lime-50 border-lime-200 p-6">
                    <h3 class="text-2xl font-semibold mb-4 text-lime-700">Q-Learning (Off-Policy TD Control)</h3>
                    <p class="mb-3 text-gray-700 leading-relaxed">Q-Learning ist ein Off-Policy TD-Algorithmus. "Off-Policy" bedeutet, dass es die optimale Aktions-Wertfunktion $Q^*(s,a)$ lernt, unabh√§ngig von der Policy, die der Agent zur Exploration verwendet (Verhaltenspolicy), solange diese alle Zustands-Aktions-Paare ausreichend oft besucht.</p>
                     <div class="equation-block">
                        <h4 class="text-lg font-semibold mb-1 text-lime-600">Update-Regel f√ºr $Q(S_t, A_t)$:</h4>
                        <span class="math-formula block">$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t)]$</span>
                    </div>
                    <p class="mt-3 text-sm text-gray-600">
                        Der entscheidende Unterschied ist der Term $\max_{a'} Q(S_{t+1}, a')$, der den maximalen $Q$-Wert im Folgezustand verwendet, was der Aktion entspricht, die eine gierige Policy w√§hlen w√ºrde (Zielpolicy).
                    </p>
                </div>
            </div>
            <div class="mt-10 p-6 bg-indigo-50 border border-indigo-200 rounded-lg">
                <h3 class="text-xl font-semibold text-indigo-700 mb-3">Vergleich der Update-Pfade: SARSA vs. Q-Learning</h3>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6 text-sm">
                    <div class="p-4 bg-white rounded shadow">
                        <h4 class="font-bold text-indigo-600 mb-2">SARSA (On-Policy)</h4>
                        <p>$S_t \xrightarrow{A_t (\text{aus } \pi)} R_{t+1}, S_{t+1} \xrightarrow{A_{t+1} (\text{aus } \pi)} \dots$</p>
                        <p class="mt-1">Update f√ºr $Q(S_t, A_t)$ verwendet $Q(S_{t+1}, A_{t+1})$. Lernt den Wert der tats√§chlich ausgef√ºhrten Policy.</p>
                    </div>
                    <div class="p-4 bg-white rounded shadow">
                        <h4 class="font-bold text-indigo-600 mb-2">Q-Learning (Off-Policy)</h4>
                        <p>$S_t \xrightarrow{A_t (\text{aus Verhaltens-}\pi)} R_{t+1}, S_{t+1} \quad (\text{Zielaktion ist } \text{argmax}_{a'} Q(S_{t+1}, a'))$</p>
                        <p class="mt-1">Update f√ºr $Q(S_t, A_t)$ verwendet $\max_{a'} Q(S_{t+1}, a')$. Lernt den Wert der optimalen (gierigen) Policy, auch wenn die Aktionen anders gew√§hlt wurden.</p>
                    </div>
                </div>
                 <p class="mt-5 text-md font-semibold text-indigo-600">Bedeutung von $\epsilon$-greedy Exploration:</p>
                <p class="text-sm text-gray-700 leading-relaxed">
                    Um sicherzustellen, dass der Agent alle relevanten Zustands-Aktions-Paare erkundet und nicht in lokalen Optima stecken bleibt, wird oft eine $\epsilon$-greedy Strategie verwendet. Mit einer kleinen Wahrscheinlichkeit $\epsilon$ w√§hlt der Agent eine zuf√§llige Aktion, anstatt der aktuell als optimal erachteten (gierigen) Aktion. Der Wert von $\epsilon$ wird oft im Laufe des Trainings reduziert (Annealing), um von Exploration zu Exploitation √ºberzugehen.
                </p>
            </div>
        </section>

        <section id="quiz" class="bg-white p-8 rounded-lg shadow-xl">
            <h2 class="text-3xl font-semibold mb-8 text-center text-blue-700">Wissens-Quiz zur Zwischenpr√ºfung</h2>
            <div id="quiz-area" class="quiz-container max-w-2xl mx-auto">
                <div id="quiz-question-container">
                    <p class="quiz-question" id="question-text">Frage wird geladen...</p>
                    <div id="options-container">
                        </div>
                </div>
                <div id="quiz-feedback-area" class="quiz-feedback" style="display: none;"></div>
                <div class="quiz-navigation mt-6 flex justify-between items-center">
                    <span id="question-counter" class="text-sm text-gray-600"></span>
                    <button id="next-question-btn" class="opacity-50 cursor-not-allowed" disabled>N√§chste Frage</button>
                </div>
                <div id="quiz-results-area" class="mt-6" style="display: none;">
                    <h3 class="text-2xl font-semibold text-center text-blue-600">Quiz beendet!</h3>
                    <p class="text-lg text-center mt-2">Dein Ergebnis: <span id="score-text" class="font-bold"></span></p>
                    <div class="text-center mt-4">
                        <button id="restart-quiz-btn" class="bg-green-500 hover:bg-green-600 text-white py-2 px-6 rounded-md font-semibold">Quiz neu starten</button>
                    </div>
                </div>
            </div>
        </section>


    </main>

    <footer class="bg-blue-700 text-white text-center p-8 mt-16">
        <p class="text-lg">&copy; 2025 Vertiefte Reinforcement Learning Infografik.</p>
        <p class="text-sm">Basierend auf Vorlesungsmaterialien und Standard-RL-Konzepten.</p>
    </footer>

    <script>
        function wrapLabels(label, maxWidth) {
            if (typeof label !== 'string') { 
                if (Array.isArray(label)) return label;
                return [String(label)]; 
            }
            const words = label.split(' ');
            let lines = [];
            let currentLine = '';
            for (const word of words) {
                if ((currentLine + word).length > maxWidth && currentLine.length > 0) {
                    lines.push(currentLine.trim());
                    currentLine = word + ' ';
                } else {
                    currentLine += word + ' ';
                }
            }
            lines.push(currentLine.trim());
            return lines;
        }
        
        const tooltipTitleCallback = (tooltipItems) => {
            const item = tooltipItems[0];
            let label = item.chart.data.labels[item.dataIndex];
            if (Array.isArray(label)) {
                return label.join(' ');
            }
            return label;
        };

        const mcVsTdCtx = document.getElementById('mcVsTdChart').getContext('2d');
        if (mcVsTdCtx) {
            new Chart(mcVsTdCtx, {
                type: 'bar',
                data: {
                    labels: ['Bias', 'Varianz', wrapLabels('Update H√§ufigkeit', 16), wrapLabels('Nutzung unvollst. Episoden', 16), wrapLabels('Sensitivit√§t Initialisierung', 16)],
                    datasets: [{
                        label: 'Monte Carlo (MC)',
                        data: [1, 5, 1, 1, 1],
                        backgroundColor: 'rgba(0, 123, 255, 0.7)',
                        borderColor: 'rgba(0, 123, 255, 1)',
                        borderWidth: 1
                    }, {
                        label: 'Temporal Difference (TD)',
                        data: [3, 2, 5, 5, 4],
                        backgroundColor: 'rgba(255, 193, 7, 0.7)',
                        borderColor: 'rgba(255, 193, 7, 1)',
                        borderWidth: 1
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: {
                        title: {
                            display: true,
                            text: 'Vergleich: MC vs. TD Eigenschaften (Skala 1=Niedrig/Nein, 5=Hoch/Ja)',
                            font: { size: 18, weight: 'bold' },
                            padding: { top: 10, bottom: 20 },
                            color: '#0056B3'
                        },
                        tooltip: {
                            callbacks: {
                               title: tooltipTitleCallback,
                                label: function(context) {
                                    let label = context.dataset.label || '';
                                    if (label) {
                                        label += ': ';
                                    }
                                    const a_value = context.raw;
                                    let characteristic = context.label;
                                    if (Array.isArray(characteristic)) characteristic = characteristic.join(' ');

                                    if (characteristic.includes('Bias')) {
                                       label += (a_value === 1 ? 'Niedrig (Unverzerrt)' : 'Mittel (Verzerrt)');
                                    } else if (characteristic.includes('Varianz')) {
                                       label += (a_value === 5 ? 'Hoch' : 'Niedrig');
                                    } else if (characteristic.includes('Update')) {
                                       label += (a_value === 1 ? 'Am Episodenende' : 'Nach jedem Schritt');
                                    } else if (characteristic.includes('Episoden')) {
                                       label += (a_value === 1 ? 'Nein' : 'Ja (Bootstrapping)');
                                    } else if (characteristic.includes('Initialisierung')) {
                                       label += (a_value === 1 ? 'Unempfindlich' : 'Empfindlich');
                                    } else {
                                       label += a_value;
                                    }
                                    return label;
                                }
                            }
                        }
                    },
                    scales: {
                        y: {
                            beginAtZero: true,
                            max: 5.5,
                            ticks: {
                                stepSize: 1,
                                callback: function(value) {
                                    switch(value) {
                                        case 1: return '1 (Niedrig/Nein)';
                                        case 2: return '2';
                                        case 3: return '3 (Mittel)';
                                        case 4: return '4';
                                        case 5: return '5 (Hoch/Ja)';
                                        default: return '';
                                    }
                                },
                               color: '#0056B3',
                               font: { weight: '600'}
                            },
                            grid: {
                                color: 'rgba(0, 86, 179, 0.15)'
                            }
                        },
                        x: {
                             ticks: { color: '#0056B3', font: { weight: '600'}},
                             grid: { display: false }
                        }
                    },
                    animation: {
                        duration: 1000,
                        easing: 'easeInOutQuart'
                    }
                }
            });
        }

        // Quiz Logic
        const quizData = [
            {
                question: "Was ist das prim√§re Ziel eines Reinforcement Learning Agenten?",
                options: [
                    "Die Dynamik der Umgebung perfekt zu verstehen.",
                    "Die kumulative Belohnung √ºber die Zeit zu maximieren.",
                    "M√∂glichst viele Zust√§nde zu explorieren.",
                    "Die Anzahl der ausgef√ºhrten Aktionen zu minimieren."
                ],
                answer: 1 // Index der korrekten Antwort
            },
            {
                question: "Welche der folgenden Komponenten geh√∂rt NICHT zu einem Markov Decision Process (MDP)?",
                options: [
                    "Zustandsraum (S)",
                    "Aktionsraum (A)",
                    "Belohnungsfunktion (R)",
                    "Die interne Speichergr√∂√üe des Agenten."
                ],
                answer: 3
            },
            {
                question: "Dynamische Programmierung (DP) Methoden ben√∂tigen zwingend:",
                options: [
                    "Kein Modell der Umgebung.",
                    "Ein perfektes Modell der Umgebung (P und R).",
                    "Nur Beispiel-Episoden aus Erfahrung.",
                    "Eine vordefinierte optimale Policy."
                ],
                answer: 1
            },
            {
                question: "Value Iteration berechnet direkt die...",
                options: [
                    "Optimale Policy $\\pi^*$.",
                    "Optimale Zustands-Wertfunktion $V^*$.",
                    "√úbergangswahrscheinlichkeiten P.",
                    "Belohnungsfunktion R."
                ],
                answer: 1
            },
            {
                question: "Welche Methode lernt aus vollst√§ndigen Episoden, ist unverzerrt, kann aber hohe Varianz aufweisen?",
                options: [
                    "Temporal Difference (TD) Learning.",
                    "Monte Carlo (MC) Methoden.",
                    "Dynamische Programmierung.",
                    "Q-Learning."
                ],
                answer: 1
            },
            {
                question: "TD(0) Learning aktualisiert seine Wertsch√§tzung basierend auf:",
                options: [
                    "Dem finalen Ergebnis der gesamten Episode.",
                    "Der beobachteten Belohnung und der gesch√§tzten Wertfunktion des *n√§chsten* Zustands (Bootstrapping).",
                    "Einem perfekten Modell der Zustands√ºberg√§nge.",
                    "Nur der unmittelbaren Belohnung, ignoriert zuk√ºnftige Zust√§nde."
                ],
                answer: 1
            },
            {
                question: "Was bedeutet \"On-Policy\" im Kontext von Algorithmen wie SARSA?",
                options: [
                    "Es lernt den Wert der optimalen Policy, unabh√§ngig von den Aktionen des Agenten.",
                    "Es lernt den Wert der Policy, die der Agent aktuell verfolgt.",
                    "Es ben√∂tigt keine Policy, um zu lernen.",
                    "Es kann nur verwendet werden, wenn die Policy deterministisch ist."
                ],
                answer: 1
            },
            {
                question: "Q-Learning ist ein \"Off-Policy\" Algorithmus, weil:",
                options: [
                    "Es immer zuf√§llige Aktionen w√§hlt.",
                    "Seine Updates die tats√§chlich im n√§chsten Schritt ausgef√ºhrte Aktion verwenden.",
                    "Es seine Q-Werte in Richtung des maximal m√∂glichen Q-Wertes des n√§chsten Zustands aktualisiert, unabh√§ngig davon, welche Aktion zur Exploration tats√§chlich ausgef√ºhrt wurde.",
                    "Es eine separate Policy f√ºr die Evaluation und eine andere f√ºr die Steuerung ben√∂tigt."
                ],
                answer: 2
            }
        ];

        const questionTextEl = document.getElementById('question-text');
        const optionsContainerEl = document.getElementById('options-container');
        const feedbackAreaEl = document.getElementById('quiz-feedback-area');
        const nextQuestionBtn = document.getElementById('next-question-btn');
        const quizResultsAreaEl = document.getElementById('quiz-results-area');
        const scoreTextEl = document.getElementById('score-text');
        const restartQuizBtn = document.getElementById('restart-quiz-btn');
        const questionCounterEl = document.getElementById('question-counter');
        const quizQuestionContainerEl = document.getElementById('quiz-question-container');


        let currentQuestionIndex = 0;
        let score = 0;
        let selectedOptionElement = null;
        let answerSubmitted = false;

        function loadQuestion() {
            answerSubmitted = false;
            selectedOptionElement = null;
            const currentQuestion = quizData[currentQuestionIndex];
            questionTextEl.innerHTML = currentQuestion.question; // Use innerHTML for MathJax
            
            optionsContainerEl.innerHTML = '';
            currentQuestion.options.forEach((option, index) => {
                const optionEl = document.createElement('button');
                optionEl.classList.add('quiz-option');
                optionEl.innerHTML = option; // Use innerHTML for MathJax
                optionEl.dataset.index = index;
                optionEl.addEventListener('click', selectOption);
                optionsContainerEl.appendChild(optionEl);
            });

            feedbackAreaEl.style.display = 'none';
            nextQuestionBtn.disabled = true;
            nextQuestionBtn.classList.add('opacity-50', 'cursor-not-allowed');
            nextQuestionBtn.classList.remove('opacity-100');
            questionCounterEl.textContent = `Frage ${currentQuestionIndex + 1} von ${quizData.length}`;
            if (window.MathJax && window.MathJax.typeset) {
                window.MathJax.typesetPromise([questionTextEl, optionsContainerEl]).catch(function (err) { console.error('MathJax typesetting error:', err); });
            }
        }

        function selectOption(event) {
            if (answerSubmitted) return;

            if (selectedOptionElement) {
                selectedOptionElement.classList.remove('selected');
            }
            selectedOptionElement = event.target.closest('.quiz-option');
            selectedOptionElement.classList.add('selected');
            
            submitAnswer(); // Automatically submit after selection or require a submit button
        }

        function submitAnswer() {
            if (!selectedOptionElement || answerSubmitted) return;
            answerSubmitted = true;

            const selectedAnswerIndex = parseInt(selectedOptionElement.dataset.index);
            const correctAnswerIndex = quizData[currentQuestionIndex].answer;

            optionsContainerEl.childNodes.forEach(child => child.disabled = true);


            if (selectedAnswerIndex === correctAnswerIndex) {
                score++;
                feedbackAreaEl.textContent = 'Richtig!';
                feedbackAreaEl.className = 'quiz-feedback feedback-correct';
                selectedOptionElement.classList.add('!bg-green-500', '!border-green-700', 'text-white');
            } else {
                feedbackAreaEl.textContent = `Falsch. Die richtige Antwort war: ${quizData[currentQuestionIndex].options[correctAnswerIndex]}`;
                feedbackAreaEl.className = 'quiz-feedback feedback-incorrect';
                selectedOptionElement.classList.add('!bg-red-500', '!border-red-700', 'text-white');
                // Highlight correct answer
                optionsContainerEl.childNodes[correctAnswerIndex].classList.add('!bg-green-300', '!border-green-500', '!text-black');
            }
            feedbackAreaEl.style.display = 'block';
            if (window.MathJax && window.MathJax.typeset) { // Retypeset if feedback contains math
                 window.MathJax.typesetPromise([feedbackAreaEl]).catch(function (err) { console.error('MathJax typesetting error:', err); });
            }

            nextQuestionBtn.disabled = false;
            nextQuestionBtn.classList.remove('opacity-50', 'cursor-not-allowed');
            nextQuestionBtn.classList.add('opacity-100');

            if (currentQuestionIndex === quizData.length - 1) {
                nextQuestionBtn.textContent = 'Ergebnisse anzeigen';
            }
        }


        nextQuestionBtn.addEventListener('click', () => {
            currentQuestionIndex++;
            if (currentQuestionIndex < quizData.length) {
                loadQuestion();
            } else {
                showResults();
            }
        });

        restartQuizBtn.addEventListener('click', () => {
            currentQuestionIndex = 0;
            score = 0;
            quizResultsAreaEl.style.display = 'none';
            quizQuestionContainerEl.style.display = 'block';
            optionsContainerEl.style.display = 'block';
            document.getElementById('quiz-navigation').style.display = 'flex';
            nextQuestionBtn.textContent = 'N√§chste Frage';
            loadQuestion();
        });

        function showResults() {
            quizQuestionContainerEl.style.display = 'none';
            optionsContainerEl.style.display = 'none';
            feedbackAreaEl.style.display = 'none';
            document.getElementById('quiz-navigation').style.display = 'none';
            quizResultsAreaEl.style.display = 'block';
            scoreTextEl.textContent = `${score} von ${quizData.length} (${((score / quizData.length) * 100).toFixed(0)}%)`;
        }

        // Initial load
        if (document.getElementById('quiz-area')) {
             loadQuestion();
        }

    </script>

</body>
</html>
