<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reinforcement Learning Infografik - Detailliert mit Quiz</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          tags: 'ams'
        },
        svg: {
          fontCache: 'global'
        }
      };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" async></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #e0f2fe; /* Light blue background */
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 300px;
            max-height: 400px;
        }
        @media (min-width: 768px) {
            .chart-container {
                height: 350px;
            }
        }
        .stat-card {
            background-color: #ffffff;
            border-radius: 0.5rem;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            padding: 1.5rem;
            text-align: center;
        }
        .stat-value {
            font-size: 2.5rem;
            font-weight: 700;
            color: #0056B3;
        }
        .stat-label {
            font-size: 1rem;
            color: #007BFF;
        }
        .concept-box {
            background-color: #caf0f8;
            border: 1px solid #90e0ef;
            border-radius: 0.5rem;
            padding: 1.5rem; 
            margin-bottom: 1.5rem; 
        }
        .flow-step {
            background-color: #ffffff;
            border: 2px solid #007BFF;
            border-radius: 0.5rem;
            padding: 1rem;
            text-align: center;
            margin-bottom: 0.5rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }
        .flow-arrow {
            font-size: 1.5rem;
            color: #007BFF;
            margin: 0.25rem 0;
            text-align: center;
        }
        h1, h2, h3, h4 {
            color: #0056B3;
        }
        .math-formula {
            font-family: 'Inter', sans-serif;
            font-size: 1.05em;
            background-color: #f0f9ff;
            padding: 0.2em 0.4em;
            border-radius: 0.25rem;
            color: #022c43;
            display: inline-block;
        }
        .equation-block {
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            padding: 1rem;
            border-radius: 0.25rem;
            margin-top: 0.5rem;
            margin-bottom: 0.5rem;
            overflow-x: auto;
        }
        .explanation-point {
            margin-bottom: 0.75rem;
            padding-left: 1rem;
            position: relative;
        }
        .explanation-point::before {
            content: "üîπ";
            position: absolute;
            left: -0.25rem;
            color: #007BFF;
        }
        .quiz-container {
            background-color: #ffffff;
            border-radius: 0.75rem;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
            padding: 2rem;
        }
        .quiz-question {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
            color: #0056B3;
        }
        .quiz-option {
            display: block;
            width: 100%; 
            background-color: #f0f9ff;
            border: 1px solid #90e0ef;
            border-radius: 0.375rem;
            padding: 0.75rem 1rem;
            margin-bottom: 0.75rem;
            cursor: pointer;
            transition: background-color 0.2s ease-in-out, border-color 0.2s ease-in-out;
            text-align: left; 
        }
        .quiz-option:hover {
            background-color: #caf0f8;
            border-color: #007BFF;
        }
        .quiz-option.selected {
            background-color: #007BFF !important; 
            color: white !important;
            border-color: #0056B3 !important;
        }
         .quiz-option.correct {
            background-color: #d1fae5 !important;
            color: #065f46 !important;
            border-color: #6ee7b7 !important;
        }
        .quiz-option.incorrect {
            background-color: #fee2e2 !important;
            color: #991b1b !important;
            border-color: #fca5a5 !important;
        }
        .quiz-feedback {
            margin-top: 1rem;
            padding: 0.75rem;
            border-radius: 0.375rem;
            font-weight: 500;
        }
        .feedback-correct {
            background-color: #d1fae5; 
            color: #065f46; 
            border: 1px solid #6ee7b7; 
        }
        .feedback-incorrect {
            background-color: #fee2e2; 
            color: #991b1b; 
            border: 1px solid #fca5a5; 
        }
        .quiz-navigation button {
            background-color: #007BFF;
            color: white;
            padding: 0.5rem 1.5rem;
            border-radius: 0.375rem;
            font-weight: 600;
            transition: background-color 0.2s;
        }
        .quiz-navigation button:hover {
            background-color: #0056B3;
        }
        .quiz-navigation button:disabled {
            background-color: #a0aec0; 
            cursor: not-allowed;
        }
        .lang-btn {
            padding: 0.5rem 1rem;
            border-radius: 0.375rem;
            text-sm font-medium;
            transition: background-color 0.2s;
        }
        .lang-btn.active {
            background-color: #0056B3; 
            color: white;
        }
        .lang-btn:not(.active) {
            background-color: #007BFF;
            color: white;
        }
         .lang-btn:not(.active):hover {
            background-color: #0069d9;
        }
        .sub-concept-box {
            background-color: #e6f7ff; /* Slightly different light blue */
            border: 1px solid #ade8f4; /* Lighter border */
            border-radius: 0.375rem;
            padding: 1rem;
            margin-top: 1rem;
        }

    </style>
</head>
<body class="text-gray-800">
    <div class="fixed top-4 right-4 z-50 space-x-2">
        <button id="lang-de-btn" data-lang="de" class="lang-btn">Deutsch</button>
        <button id="lang-en-btn" data-lang="en" class="lang-btn">English</button>
    </div>

    <header class="bg-blue-600 text-white p-6 shadow-lg">
        <div class="container mx-auto text-center">
            <h1 class="text-4xl font-bold" data-i18n="headerTitle">Reinforcement Learning: Detaillierte Einblicke & Quiz</h1>
            <p class="mt-2 text-lg" data-i18n="headerSubtitle">Vertiefte Konzepte, Algorithmen und interaktives Quiz</p>
        </div>
    </header>

    <main class="container mx-auto p-4 md:p-8 space-y-16">

        <section id="intro-rl-mdp" class="bg-white p-8 rounded-lg shadow-xl">
            <h2 class="text-3xl font-semibold mb-8 text-center text-blue-700" data-i18n="section1Title">1. Grundlagen: RL & Markov-Entscheidungsprozesse (MDPs)</h2>
            <p class="text-lg mb-6 text-gray-700 leading-relaxed" data-i18n="section1Intro">
                Reinforcement Learning (RL) ist ein Paradigma des maschinellen Lernens, bei dem ein autonomer <strong>Agent</strong> lernt, optimale Sequenzen von Entscheidungen (Aktionen) in einer komplexen und oft unsicheren <strong>Umgebung</strong> zu treffen. Das prim√§re Ziel des Agenten ist es, die Summe der erhaltenen <strong>Belohnungen</strong> √ºber die Zeit zu maximieren. Dieser Lernprozess ist fundamental iterativ und erfahrungsbasiert.
            </p>

            <div class="grid md:grid-cols-2 gap-8 mb-10 items-start">
                <div class="concept-box bg-blue-50 border-blue-200 p-6">
                    <h3 class="text-xl font-semibold mb-4 text-blue-600" data-i18n="agentEnvCycleTitle">Der Agent-Umgebung-Interaktionszyklus</h3>
                    <div class="space-y-3">
                        <div class="flow-step" data-i18n="cycleStep1">1. Agent beobachtet aktuellen Zustand $S_t$ der Umgebung.</div>
                        <div class="flow-arrow">‚¨áÔ∏è</div>
                        <div class="flow-step" data-i18n="cycleStep2">2. Agent w√§hlt eine Aktion $A_t$ basierend auf seiner aktuellen Strategie (Policy $\pi$).</div>
                        <div class="flow-arrow">‚û°Ô∏è</div>
                        <div class="flow-step" data-i18n="cycleStep3">3. Umgebung reagiert: Agent erh√§lt eine Belohnung $R_{t+1}$ und beobachtet den neuen Zustand $S_{t+1}$.</div>
                        <div class="flow-arrow">üîÑ</div>
                        <div class="flow-step" data-i18n="cycleStep4">4. Agent aktualisiert seine Policy $\pi$ basierend auf der Erfahrung $(S_t, A_t, R_{t+1}, S_{t+1})$.</div>
                    </div>
                    <p class="mt-5 text-sm text-gray-600" data-i18n="cycleDesc">Dieser Zyklus wiederholt sich, wodurch der Agent schrittweise lernt, bessere Entscheidungen zu treffen.</p>
                </div>

                <div class="space-y-6">
                    <div class="stat-card p-6">
                        <div class="stat-value">üéØ</div>
                        <div class="stat-label mt-2" data-i18n="goalTitle">Maximierung des kumulativen Returns</div>
                        <p class="text-sm text-gray-600 mt-3" data-i18n="goalDesc">Der Return $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$ ist die Summe der diskontierten zuk√ºnftigen Belohnungen. Die <strong data-i18n="rewardHypothesis">Reward Hypothesis</strong> besagt, dass alle Ziele durch die Maximierung des erwarteten kumulativen Rewards formuliert werden k√∂nnen.</p>
                    </div>
                     <div class="concept-box bg-blue-50 border-blue-200 p-4">
                        <h4 class="text-lg font-semibold text-blue-600 mb-2" data-i18n="gammaFactorTitle">Der Diskontierungsfaktor $\gamma$</h4>
                        <p class="text-sm text-gray-700 mb-1" data-i18n="gammaDesc">Steuert die Wichtigkeit zuk√ºnftiger Belohnungen:</p>
                        <ul class="list-none space-y-1 text-sm">
                            <li class="explanation-point" data-i18n="gammaShortSighted">$\gamma \approx 0$: Agent ist "kurzsichtig", fokussiert auf unmittelbare Belohnungen.</li>
                            <li class="explanation-point" data-i18n="gammaLongSighted">$\gamma \approx 1$: Agent ist "weitsichtig", ber√ºcksichtigt zuk√ºnftige Belohnungen stark.</li>
                        </ul>
                        <p class="text-xs text-gray-500 mt-2" data-i18n="gammaContinuous">Notwendig f√ºr kontinuierliche Aufgaben, um unendliche Returns zu vermeiden.</p>
                    </div>
                </div>
            </div>

            <h3 class="text-2xl font-semibold mb-4 text-blue-600" data-i18n="mdpTitle">Der Markov-Entscheidungsprozess (MDP)</h3>
            <p class="mb-4 text-gray-700 leading-relaxed" data-i18n="mdpIntro">Ein MDP ist der Standardformalismus f√ºr RL-Probleme, die die Markov-Eigenschaft erf√ºllen. Er wird durch ein Tupel $(S, A, P, R, \gamma)$ definiert:</p>
            <div class="grid md:grid-cols-2 gap-x-8 gap-y-4 mb-6">
                <div class="explanation-point" data-i18n="mdpS"><strong>S (Zustandsraum):</strong> Eine Menge aller m√∂glichen Zust√§nde. Z.B. Positionen auf einem Schachbrett, Gelenkwinkel eines Roboters.</div>
                <div class="explanation-point" data-i18n="mdpA"><strong>A (Aktionsraum):</strong> Eine Menge aller m√∂glichen Aktionen. Z.B. Z√ºge im Schach, Motorkommandos f√ºr einen Roboter.</div>
                <div class="explanation-point" data-i18n="mdpP"><strong>P (Zustands√ºbergangsmodell):</strong> $P(s'|s, a) = Pr\{S_{t+1}=s' | S_t=s, A_t=a\}$. Definiert die Dynamik der Umgebung.</div>
                <div class="explanation-point" data-i18n="mdpR"><strong>R (Belohnungsfunktion):</strong> $R(s, a, s') = E[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$. Definiert das Ziel des Agenten.</div>
            </div>
            <div class="concept-box bg-indigo-50 border-indigo-200 p-5">
                <h4 class="text-lg font-semibold text-indigo-700 mb-2" data-i18n="markovPropertyTitle">Die Markov-Eigenschaft</h4>
                <p class="text-gray-700" data-i18n="markovPropertyQuote">"Die Zukunft ist unabh√§ngig von der Vergangenheit, gegeben die Gegenwart."</p>
                <p class="mt-2 text-sm equation-block text-indigo-800">$Pr\{S_{t+1}=s', R_{t+1}=r | S_t, A_t, S_{t-1}, A_{t-1}, \dots\} = Pr\{S_{t+1}=s', R_{t+1}=r | S_t, A_t\}$</p>
                <p class="mt-2 text-sm text-gray-600" data-i18n="markovPropertyDesc">Der aktuelle Zustand $S_t$ enth√§lt alle relevanten Informationen aus der Historie, um die n√§chste Zustands- und Belohnungswahrscheinlichkeit zu bestimmen. Dies ist eine starke Vereinfachung, die viele RL-Algorithmen erm√∂glicht.</p>
            </div>

            <div class="mt-8 p-6 bg-yellow-50 border border-yellow-300 rounded-lg">
                <h4 class="text-xl font-semibold text-yellow-700 mb-3" data-i18n="coreChallengesTitle">Kernherausforderungen und Konzepte</h4>
                <div class="grid md:grid-cols-2 gap-6">
                    <div>
                        <h5 class="text-lg font-semibold text-yellow-600 mb-1" data-i18n="creditAssignmentTitle">Credit Assignment</h5>
                        <p class="text-sm text-gray-700" data-i18n="creditAssignmentDesc">Wie k√∂nnen Belohnungen (oder Bestrafungen), die erst sp√§t in einer Sequenz von Aktionen auftreten, korrekt den fr√ºheren Aktionen zugeordnet werden, die urs√§chlich daf√ºr waren?</p>
                    </div>
                    <div>
                        <h5 class="text-lg font-semibold text-yellow-600 mb-1" data-i18n="exploreExploitTitle">Exploration vs. Exploitation</h5>
                        <p class="text-sm text-gray-700" data-i18n="exploreExploitDesc">Der Agent muss entscheiden, ob er neue Aktionen ausprobiert, um mehr √ºber die Umgebung zu lernen (Exploration), oder ob er die aktuell besten bekannten Aktionen ausf√ºhrt, um die Belohnung zu maximieren (Exploitation).</p>
                    </div>
                    <div>
                        <h5 class="text-lg font-semibold text-yellow-600 mb-1" data-i18n="policyTitle">Policy ($\pi$): Die Strategie des Agenten</h5>
                        <p class="text-sm text-gray-700" data-i18n="policyDesc">Eine Abbildung von Zust√§nden zu Aktionen. Deterministisch: $\pi(s) = a$. Stochastisch: $\pi(a|s) = Pr\{A_t=a | S_t=s\}$.</p>
                    </div>
                    <div>
                        <h5 class="text-lg font-semibold text-yellow-600 mb-1" data-i18n="valueFunctionTitle">Wertfunktionen ($V^\pi, Q^\pi$): Wie gut ist ein Zustand/Aktion?</h5>
                        <p class="text-sm text-gray-700" data-i18n="valueFunctionDesc">$V^\pi(s)$: Erwarteter Return, wenn man in Zustand $s$ startet und Policy $\pi$ folgt. $Q^\pi(s,a)$: Erwarteter Return, wenn man in Zustand $s$ Aktion $a$ ausf√ºhrt und danach Policy $\pi$ folgt.</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="dp" class="bg-white p-8 rounded-lg shadow-xl">
            <h2 class="text-3xl font-semibold mb-8 text-center text-blue-700" data-i18n="section2Title">2. Dynamische Programmierung (DP): Planung mit perfektem Modell</h2>
            <p class="text-lg mb-6 text-gray-700 leading-relaxed" data-i18n="section2Intro">
                Dynamische Programmierung (DP) umfasst Algorithmen, die eine optimale Policy $\pi^*$ f√ºr einen gegebenen MDP berechnen k√∂nnen, vorausgesetzt, das Modell der Umgebung (√úbergangswahrscheinlichkeiten $P$ und Belohnungen $R$) ist vollst√§ndig bekannt. DP-Methoden nutzen Wertfunktionen, um die Suche nach guten Policies zu strukturieren und basieren auf den Bellman-Gleichungen.
            </p>
            <div class="concept-box bg-blue-50 border-blue-200 p-6 mb-8">
                <h3 class="text-xl font-semibold mb-3 text-blue-600" data-i18n="bellmanOptimalityPrincipleTitle">Bellman-Optimalit√§tsprinzip</h3>
                <p class="text-gray-700 italic" data-i18n="bellmanOptimalityPrincipleQuote">"Eine optimale Policy hat die Eigenschaft, dass, was auch immer der Anfangszustand und die erste Entscheidung sind, die verbleibenden Entscheidungen eine optimale Policy bez√ºglich des aus der ersten Entscheidung resultierenden Zustands darstellen m√ºssen." (Bellman, 1957)</p>
                <p class="mt-3 text-sm text-gray-600" data-i18n="bellmanOptimalityPrincipleDesc">Einfach gesagt: Wenn der beste Pfad von A nach C √ºber B f√ºhrt, dann muss der Teilpfad von B nach C auch der beste Pfad von B nach C sein.</p>
            </div>

            <div class="grid md:grid-cols-2 gap-8 mb-8">
                <div class="concept-box bg-green-50 border-green-200 p-6">
                    <h3 class="text-xl font-semibold mb-4 text-green-700" data-i18n="policyIterationTitle">Policy Iteration (Politikiteration)</h3>
                    <p class="text-sm text-gray-700 mb-3" data-i18n="policyIterationDesc">Ein iterativer Algorithmus, der zwischen zwei Schritten wechselt:</p>
                    <div class="space-y-3">
                        <div class="flow-step bg-green-100 border-green-500"><strong data-i18n="policyEvalTitle">1. Policy Evaluation (Bewertung):</strong> <span data-i18n="policyEvalDesc">Berechne die Zustands-Wertfunktion $V^\pi(s)$ f√ºr die aktuelle Policy $\pi$. Dies geschieht durch iteratives Anwenden der Bellman-Erwartungsgleichung bis zur Konvergenz:</span> <br> <span class="math-formula text-xs">$V_{k+1}^\pi(s) \leftarrow \sum_a \pi(a|s) \sum_{s',r} P(s',r|s,a) [r + \gamma V_k^\pi(s')]$</span></div>
                        <div class="flow-arrow text-green-600">‚¨áÔ∏è</div>
                        <div class="flow-step bg-green-100 border-green-500"><strong data-i18n="policyImprovTitle">2. Policy Improvement (Verbesserung):</strong> <span data-i18n="policyImprovDesc">Verbessere die Policy, indem f√ºr jeden Zustand $s$ gierig die Aktion gew√§hlt wird, die $Q^\pi(s,a)$ maximiert:</span> <br> <span class="math-formula text-xs">$\pi'(s) \leftarrow \text{argmax}_a \sum_{s',r} P(s',r|s,a) [r + \gamma V^\pi(s')]$</span></div>
                    </div>
                    <p class="mt-4 text-sm text-gray-600" data-i18n="policyIterationConvergence">Dieser Prozess konvergiert garantiert zur optimalen Policy $\pi^*$. "Sweeps" beziehen sich auf vollst√§ndige Durchl√§ufe √ºber alle Zust√§nde w√§hrend der Evaluation oder Iteration.</p>
                </div>
                <div class="concept-box bg-purple-50 border-purple-200 p-6">
                    <h3 class="text-xl font-semibold mb-4 text-purple-700" data-i18n="valueIterationTitle">Value Iteration (Wertiteration)</h3>
                    <p class="text-sm text-gray-700 mb-3" data-i18n="valueIterationDesc">Kombiniert im Wesentlichen einen Schritt der Policy Evaluation und einen Schritt der Policy Improvement in einer einzigen Update-Regel. Sie iteriert direkt √ºber die Bellman-Optimalit√§tsgleichung f√ºr $V^*(s)$:</p>
                     <div class="flow-step bg-purple-100 border-purple-500 equation-block">
                        <span class="math-formula">$V_{k+1}(s) \leftarrow \max_a \sum_{s',r} P(s',r|s,a) [r + \gamma V_k(s')]$</span>
                     </div>
                     <p class="mt-4 text-sm text-gray-600" data-i18n="valueIterationConvergence">Konvergiert ebenfalls zur optimalen Wertfunktion $V^*$, aus der die optimale Policy $\pi^*$ dann durch eine einmalige gierige Auswahl extrahiert werden kann.</p>
                </div>
            </div>
            <div class="concept-box">
                <h3 class="text-xl font-semibold mb-2 text-blue-600" data-i18n="vQRelationTitle">Beziehung zwischen $V^\pi$ und $Q^\pi$</h3>
                <p class="text-gray-700 mb-2" data-i18n="vQRelationDesc">Die Zustands-Wertfunktion und die Aktions-Wertfunktion sind eng miteinander verbunden:</p>
                <div class="equation-block text-sm">
                    <span class="math-formula">$V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s,a)$</span> <span data-i18n="vQStochastic">(wenn $\pi$ stochastisch ist)</span>
                    <br>
                    <span class="math-formula">$V^\pi(s) = Q^\pi(s, \pi(s))$</span> <span data-i18n="vQDeterministic">(wenn $\pi$ deterministisch ist)</span>
                    <br>
                    <span class="math-formula">$Q^\pi(s,a) = \sum_{s',r} P(s',r|s,a) [r + \gamma V^\pi(s')]$</span>
                </div>
            </div>
             <div class="mt-8 p-6 bg-red-50 border border-red-200 rounded-lg">
                <h4 class="text-xl font-semibold text-red-700 mb-3" data-i18n="dpApplicabilityTitle">Wann ist DP anwendbar?</h4>
                <ul class="list-disc list-inside text-gray-700 space-y-1">
                    <li data-i18n="dpCond1">Wenn ein vollst√§ndiges und genaues Modell der Umgebung ($P$ und $R$) verf√ºgbar ist.</li>
                    <li data-i18n="dpCond2">Eher f√ºr Planungsprobleme als f√ºr direktes Lernen aus Interaktion.</li>
                    <li data-i18n="dpCond3">Obwohl rechenintensiv f√ºr sehr gro√üe Probleme, bilden DP-Konzepte die theoretische Grundlage f√ºr viele andere RL-Algorithmen.</li>
                </ul>
            </div>
        </section>

        <section id="model-free-prediction" class="bg-white p-8 rounded-lg shadow-xl">
            <h2 class="text-3xl font-semibold mb-8 text-center text-blue-700" data-i18n="section3Title">3. Modellfreie Vorhersage: Lernen ohne Modell</h2>
            <p class="text-lg mb-6 text-gray-700 leading-relaxed" data-i18n="section3Intro">
                In vielen realen Szenarien ist ein exaktes Modell der Umgebung nicht verf√ºgbar. Modellfreie Vorhersagemethoden erm√∂glichen es, Wertfunktionen ($V^\pi$ oder $Q^\pi$) f√ºr eine gegebene Policy $\pi$ direkt aus Erfahrungsepisoden zu sch√§tzen, die durch Interaktion mit der Umgebung generiert werden.
            </p>
            <div class="grid md:grid-cols-2 gap-8">
                <div>
                    <h3 class="text-2xl font-semibold mb-4 text-blue-600" data-i18n="mcMethodsTitle">a) Monte Carlo (MC) Methoden</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed" data-i18n="mcMethodsDesc">MC-Methoden lernen, indem sie den durchschnittlichen Return nach Besuchen eines Zustands √ºber viele vollst√§ndige Episoden berechnen.
                    <br><strong data-i18n="firstVisitMC">First-Visit MC:</strong> Ber√ºcksichtigt nur den Return nach dem ersten Besuch eines Zustands $s$ in einer Episode.
                    <br><strong data-i18n="everyVisitMC">Every-Visit MC:</strong> Ber√ºcksichtigt den Return nach jedem Besuch eines Zustands $s$ in einer Episode.</p>
                    <div class="concept-box">
                        <h4 class="text-lg font-semibold mb-1 text-blue-500" data-i18n="mcUpdateRuleTitle">Update-Regel (inkrementell f√ºr $V(S_t)$):</h4>
                        <span class="math-formula">$V(S_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t))$</span>
                        <p class="text-xs text-gray-500 mt-1" data-i18n="mcUpdateRuleDesc">Hier ist $G_t$ der tats√§chlich beobachtete (vollst√§ndige) Return ab Zeitschritt $t$ bis zum Ende der Episode. $\alpha$ ist die Lernrate.</p>
                    </div>
                    <div class="mt-4">
                        <h4 class="text-md font-semibold text-green-600" data-i18n="mcAdvantages">Vorteile:</h4>
                        <ul class="list-disc list-inside text-sm text-gray-600"><li data-i18n="mcAdv1">Unverzerrt (kein Bootstrapping-Bias).</li><li data-i18n="mcAdv2">Einfach zu verstehen und zu implementieren.</li></ul>
                        <h4 class="text-md font-semibold text-red-600 mt-2" data-i18n="mcDisadvantages">Nachteile:</h4>
                        <ul class="list-disc list-inside text-sm text-gray-600"><li data-i18n="mcDisadv1">Hohe Varianz, da der Return von vielen zuf√§lligen Aktionen und √úberg√§ngen abh√§ngt.</li><li data-i18n="mcDisadv2">Funktioniert nur f√ºr episodische (terminierende) Aufgaben.</li><li data-i18n="mcDisadv3">Updates erfolgen erst am Ende einer Episode.</li></ul>
                    </div>
                </div>
                <div>
                    <h3 class="text-2xl font-semibold mb-4 text-blue-600" data-i18n="tdLearningTitle">b) Temporal Difference (TD) Learning</h3>
                    <p class="mb-4 text-gray-700 leading-relaxed" data-i18n="tdLearningDesc">TD-Methoden lernen von jedem Schritt, indem sie ihre aktuelle Sch√§tzung basierend auf der Sch√§tzung des n√§chsten Zustands anpassen ‚Äì ein Prozess, der als <strong data-i18n="bootstrapping">Bootstrapping</strong> bezeichnet wird. Sie ben√∂tigen keine vollst√§ndigen Episoden.</p>
                     <div class="concept-box">
                        <h4 class="text-lg font-semibold mb-1 text-blue-500" data-i18n="tdUpdateRuleTitle">TD(0) Update-Regel f√ºr $V(S_t)$:</h4>
                        <span class="math-formula">$V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$</span>
                        <p class="text-xs text-gray-500 mt-1" data-i18n="tdUpdateRuleDesc">Der Term $R_{t+1} + \gamma V(S_{t+1})$ ist das <strong data-i18n="tdTarget">TD-Target</strong> (eine Sch√§tzung des Returns), und $[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$ ist der <strong data-i18n="tdError">TD-Fehler</strong>.</p>
                    </div>
                    <div class="mt-4">
                        <h4 class="text-md font-semibold text-green-600" data-i18n="tdAdvantages">Vorteile:</h4>
                        <ul class="list-disc list-inside text-sm text-gray-600"><li data-i18n="tdAdv1">Geringere Varianz als MC.</li><li data-i18n="tdAdv2">Kann online lernen (nach jedem Schritt).</li><li data-i18n="tdAdv3">Funktioniert auch f√ºr kontinuierliche Aufgaben.</li></ul>
                        <h4 class="text-md font-semibold text-red-600 mt-2" data-i18n="tdDisadvantages">Nachteile:</h4>
                        <ul class="list-disc list-inside text-sm text-gray-600"><li data-i18n="tdDisadv1">Verzerrt, da die Sch√§tzung auf einer anderen Sch√§tzung ($V(S_{t+1})$) basiert.</li><li data-i18n="tdDisadv2">Empfindlicher gegen√ºber der Initialisierung der Werte.</li></ul>
                    </div>
                </div>
            </div>
            <div class="mt-8 concept-box bg-teal-50 border-teal-200 p-6">
                <h3 class="text-xl font-semibold text-teal-700 mb-3" data-i18n="nStepTDTitle">N-Step TD Methoden</h3>
                <p class="text-gray-700 mb-2" data-i18n="nStepTDDesc1">Diese Methoden schlagen eine Br√ºcke zwischen MC und TD(0). Anstatt nur einen Schritt (TD(0)) oder die gesamte Episode (MC) f√ºr das Update zu betrachten, verwenden n-Step TD Methoden einen Return √ºber $n$ Schritte:</p>
                <div class="equation-block text-sm">
                    <span class="math-formula">$G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n})$</span>
                </div>
                <p class="text-gray-700 mt-2" data-i18n="nStepTDDesc2">Die Update-Regel lautet dann:</p>
                 <div class="equation-block text-sm">
                    <span class="math-formula">$V(S_t) \leftarrow V(S_t) + \alpha [G_{t:t+n} - V(S_t)]$</span>
                </div>
                <p class="text-sm text-gray-600 mt-2" data-i18n="nStepTDDesc3">Der Parameter $n$ erlaubt einen Trade-off zwischen dem Bias von TD(0) und der Varianz von MC.</p>
            </div>
             <div class="mt-10 text-center">
                <h3 class="text-2xl font-semibold mb-4 text-blue-600" data-i18n="mcTdChartTitle">Visueller Vergleich: MC vs. TD Update</h3>
                 <div class="chart-container h-72 md:h-96">
                    <canvas id="mcVsTdChart"></canvas>
                </div>
                <p class="text-md text-gray-600 mt-4" data-i18n="mcTdChartDesc">Das Diagramm illustriert die unterschiedlichen St√§rken und Schw√§chen von MC- und TD-Lernans√§tzen bez√ºglich Bias, Varianz und Update-Frequenz.</p>
            </div>
        </section>

        <section id="model-free-control" class="bg-white p-8 rounded-lg shadow-xl">
            <h2 class="text-3xl font-semibold mb-8 text-center text-blue-700" data-i18n="section4Title">4. Modellfreie Steuerung: Optimale Policies ohne Modell finden</h2>
            <p class="text-lg mb-6 text-gray-700 leading-relaxed" data-i18n="section4Intro">
                Modellfreie Steuerungsmethoden erweitern die Ideen der modellfreien Vorhersage, um nicht nur Wertfunktionen zu sch√§tzen, sondern auch aktiv eine (nahezu) optimale Policy $\pi^*$ zu finden. Da kein Umgebungsmodell vorhanden ist, lernen diese Methoden typischerweise die Aktions-Wertfunktion $Q(s,a)$, da diese direkt angibt, wie gut es ist, eine bestimmte Aktion $a$ in einem Zustand $s$ auszuf√ºhren. Die Policy wird dann oft $\epsilon$-gierig auf Basis dieser $Q$-Werte abgeleitet.
            </p>
            <div class="concept-box bg-yellow-50 border-yellow-200 p-6 mb-10">
                <h3 class="text-xl font-semibold mb-4 text-yellow-700 text-center" data-i18n="gpiTitle">Generalisierte Policy Iteration (GPI) f√ºr Modellfreie Steuerung</h3>
                <div class="flex flex-col md:flex-row justify-around items-center space-y-4 md:space-y-0 md:space-x-6">
                    <div class="flow-step bg-yellow-100 border-yellow-500 p-5 text-center">
                        <h4 class="font-semibold mb-1" data-i18n="gpiEvalTitle">1. Policy Evaluation (Sch√§tzung)</h4>
                        <span data-i18n="gpiEvalDesc">Sch√§tze $Q^\pi(s,a)$ f√ºr die aktuelle Policy $\pi$ (z.B. mittels MC oder TD Updates auf $Q$-Werte).</span>
                    </div>
                    <div class="text-3xl font-bold text-yellow-600 self-center">üîÑ</div>
                    <div class="flow-step bg-yellow-100 border-yellow-500 p-5 text-center">
                        <h4 class="font-semibold mb-1" data-i18n="gpiImprovTitle">2. Policy Improvement (Verbesserung)</h4>
                        <span data-i18n="gpiImprovDesc">Aktualisiere $\pi$, indem $\epsilon$-gierig bez√ºglich der aktuellen $Q$-Werte agiert wird:</span> <br> $\pi(s) \leftarrow \text{argmax}_a Q(s,a)$ <span data-i18n="gpiEpsilon">(mit $\epsilon$ W'keit zuf√§llig).</span>
                    </div>
                </div>
                <p class="mt-5 text-sm text-gray-600 text-center" data-i18n="gpiConvergence">Dieser iterative Prozess konvergiert typischerweise gegen die optimale Aktions-Wertfunktion $Q^*$ und die optimale Policy $\pi^*$. Die Notwendigkeit von $Q(s,a)$ ergibt sich daraus, dass ohne Modell $P(s'|s,a)$ die $V(s)$-Funktion allein nicht ausreicht, um eine gierige Aktion zu bestimmen.</p>
            </div>

            <div class="grid md:grid-cols-2 gap-10">
                <div class="concept-box bg-sky-50 border-sky-200 p-6">
                    <h3 class="text-2xl font-semibold mb-4 text-sky-700" data-i18n="sarsaTitle">SARSA (On-Policy TD Control)</h3>
                    <p class="mb-3 text-gray-700 leading-relaxed" data-i18n="sarsaDesc">SARSA ist ein On-Policy TD-Algorithmus. "On-Policy" bedeutet, dass die $Q$-Werte f√ºr die Policy gesch√§tzt und verbessert werden, die der Agent tats√§chlich ausf√ºhrt (inklusive explorativer Aktionen).</p>
                    <div class="equation-block">
                        <h4 class="text-lg font-semibold mb-1 text-sky-600" data-i18n="sarsaUpdateRuleTitle">Update-Regel f√ºr $Q(S_t, A_t)$:</h4>
                        <span class="math-formula block">$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$</span>
                    </div>
                    <p class="mt-3 text-sm text-gray-600" data-i18n="sarsaNameOrigin">
                        Der Name SARSA kommt von der Sequenz der Ereignisse im Update: $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$. $A_{t+1}$ wird gem√§√ü der aktuellen (oft $\epsilon$-greedy) Policy gew√§hlt.
                    </p>
                     <div class="sub-concept-box mt-4">
                        <h4 class="text-lg font-semibold text-sky-600 mb-2" data-i18n="expectedSarsaTitle">Expected SARSA</h4>
                        <p class="text-sm text-gray-700" data-i18n="expectedSarsaDesc">Eine Variante, die den Erwartungswert √ºber alle m√∂glichen Aktionen $A_{t+1}$ gem√§√ü der aktuellen Policy $\pi$ nimmt, anstatt die spezifisch gew√§hlte Aktion $A_{t+1}$ zu verwenden. Dies kann die Varianz reduzieren.</p>
                        <span class="math-formula block mt-2 text-xs">$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \sum_{a'} \pi(a'|S_{t+1}) Q(S_{t+1}, a') - Q(S_t, A_t)]$</span>
                    </div>
                </div>
                <div class="concept-box bg-lime-50 border-lime-200 p-6">
                    <h3 class="text-2xl font-semibold mb-4 text-lime-700" data-i18n="qLearningTitle">Q-Learning (Off-Policy TD Control)</h3>
                    <p class="mb-3 text-gray-700 leading-relaxed" data-i18n="qLearningDesc">Q-Learning ist ein Off-Policy TD-Algorithmus. "Off-Policy" bedeutet, dass es die optimale Aktions-Wertfunktion $Q^*(s,a)$ lernt, unabh√§ngig von der Policy, die der Agent zur Exploration verwendet (Verhaltenspolicy), solange diese alle Zustands-Aktions-Paare ausreichend oft besucht.</p>
                     <div class="equation-block">
                        <h4 class="text-lg font-semibold mb-1 text-lime-600" data-i18n="qLearningUpdateRuleTitle">Update-Regel f√ºr $Q(S_t, A_t)$:</h4>
                        <span class="math-formula block">$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t)]$</span>
                    </div>
                    <p class="mt-3 text-sm text-gray-600" data-i18n="qLearningKeyDiff">
                        Der entscheidende Unterschied ist der Term $\max_{a'} Q(S_{t+1}, a')$, der den maximalen $Q$-Wert im Folgezustand verwendet, was der Aktion entspricht, die eine gierige Policy w√§hlen w√ºrde (Zielpolicy).
                    </p>
                    <div class="sub-concept-box mt-4">
                        <h4 class="text-lg font-semibold text-lime-600 mb-2" data-i18n="doubleQLearningTitle">Double Q-Learning</h4>
                        <p class="text-sm text-gray-700" data-i18n="doubleQLearningDesc">Standard Q-Learning kann unter bestimmten Umst√§nden zu einer √úbersch√§tzung der Q-Werte f√ºhren (Maximierungs-Bias). Double Q-Learning adressiert dies, indem es zwei separate Q-Wert-Sch√§tzungen ($Q_A$ und $Q_B$) pflegt. Eine wird zur Auswahl der besten Aktion im n√§chsten Zustand verwendet, die andere zur Bewertung dieser Aktion.</p>
                        <p class="text-sm text-gray-700 mt-1" data-i18n="doubleQLearningUpdate">Z.B. Update f√ºr $Q_A$: $Q_A(S_t,A_t) \leftarrow Q_A(S_t,A_t) + \alpha [R_{t+1} + \gamma Q_B(S_{t+1}, \text{argmax}_{a'} Q_A(S_{t+1},a')) - Q_A(S_t,A_t)]$ (und umgekehrt f√ºr $Q_B$).</p>
                    </div>
                </div>
            </div>
            <div class="mt-10 p-6 bg-indigo-50 border border-indigo-200 rounded-lg">
                <h3 class="text-xl font-semibold text-indigo-700 mb-3" data-i18n="onOffPolicyCompareTitle">Vergleich der Update-Pfade: SARSA vs. Q-Learning</h3>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6 text-sm">
                    <div class="p-4 bg-white rounded shadow">
                        <h4 class="font-bold text-indigo-600 mb-2" data-i18n="sarsaPathTitle">SARSA (On-Policy)</h4>
                        <p>$S_t \xrightarrow{A_t (\text{aus } \pi)} R_{t+1}, S_{t+1} \xrightarrow{A_{t+1} (\text{aus } \pi)} \dots$</p>
                        <p class="mt-1" data-i18n="sarsaPathDesc">Update f√ºr $Q(S_t, A_t)$ verwendet $Q(S_{t+1}, A_{t+1})$. Lernt den Wert der tats√§chlich ausgef√ºhrten Policy.</p>
                    </div>
                    <div class="p-4 bg-white rounded shadow">
                        <h4 class="font-bold text-indigo-600 mb-2" data-i18n="qLearningPathTitle">Q-Learning (Off-Policy)</h4>
                        <p>$S_t \xrightarrow{A_t (\text{aus Verhaltens-}\pi)} R_{t+1}, S_{t+1} \quad (\text{Zielaktion ist } \text{argmax}_{a'} Q(S_{t+1}, a'))$</p>
                        <p class="mt-1" data-i18n="qLearningPathDesc">Update f√ºr $Q(S_t, A_t)$ verwendet $\max_{a'} Q(S_{t+1}, a')$. Lernt den Wert der optimalen (gierigen) Policy, auch wenn die Aktionen anders gew√§hlt wurden.</p>
                    </div>
                </div>
                 <p class="mt-5 text-md font-semibold text-indigo-600" data-i18n="epsilonGreedyTitle">Bedeutung von $\epsilon$-greedy Exploration:</p>
                <p class="text-sm text-gray-700 leading-relaxed" data-i18n="epsilonGreedyDesc">
                    Um sicherzustellen, dass der Agent alle relevanten Zustands-Aktions-Paare erkundet und nicht in lokalen Optima stecken bleibt, wird oft eine $\epsilon$-greedy Strategie verwendet. Mit einer kleinen Wahrscheinlichkeit $\epsilon$ w√§hlt der Agent eine zuf√§llige Aktion, anstatt der aktuell als optimal erachteten (gierigen) Aktion. Der Wert von $\epsilon$ wird oft im Laufe des Trainings reduziert (Annealing), um von Exploration zu Exploitation √ºberzugehen.
                </p>
            </div>
             <div class="mt-10 p-6 bg-purple-50 border border-purple-200 rounded-lg">
                <h3 class="text-xl font-semibold text-purple-700 mb-3" data-i18n="vfaTitle">Ausblick: Wertfunktionsapproximation (VFA)</h3>
                <p class="text-gray-700 leading-relaxed" data-i18n="vfaDesc">
                    F√ºr Probleme mit sehr gro√üen oder kontinuierlichen Zustandsr√§umen sind tabellarische Darstellungen von $Q(s,a)$ (eine Tabelle f√ºr jedes Zustands-Aktions-Paar) nicht mehr praktikabel. Hier kommt die Wertfunktionsapproximation ins Spiel: Anstatt die Q-Werte exakt zu speichern, werden sie durch eine parametrisierte Funktion approximiert, z.B. eine lineare Funktion oder ein neuronales Netz: $Q(s,a; \mathbf{w}) \approx Q^*(s,a)$. Die Parameter $\mathbf{w}$ werden dann gelernt.
                </p>
                <p class="mt-2 text-sm text-gray-600" data-i18n="vfaExample">Ein bekanntes Beispiel ist das Deep Q-Network (DQN), das ein tiefes neuronales Netz zur Approximation der Q-Funktion verwendet und bemerkenswerte Erfolge in komplexen Umgebungen wie Atari-Spielen erzielt hat.</p>
            </div>
        </section>

        <section id="quiz" class="bg-white p-8 rounded-lg shadow-xl">
            <h2 class="text-3xl font-semibold mb-8 text-center text-blue-700" data-i18n="quizSectionTitle">Wissens-Quiz zur Zwischenpr√ºfung</h2>
            <div id="quiz-area" class="quiz-container max-w-2xl mx-auto">
                <div id="quiz-question-container">
                    <p class="quiz-question" id="question-text" data-i18n="quizLoading">Frage wird geladen...</p>
                    <div id="options-container">
                        </div>
                </div>
                <div id="quiz-feedback-area" class="quiz-feedback" style="display: none;"></div>
                <div id="quiz-navigation" class="quiz-navigation mt-6 flex justify-between items-center">
                    <span id="question-counter" class="text-sm text-gray-600"></span>
                    <button id="next-question-btn" class="opacity-50 cursor-not-allowed" disabled data-i18n="quiz_nextButton">N√§chste Frage</button>
                </div>
                <div id="quiz-results-area" class="mt-6" style="display: none;">
                    <h3 class="text-2xl font-semibold text-center text-blue-600" data-i18n="quiz_finishedTitle">Quiz beendet!</h3>
                    <p class="text-lg text-center mt-2"><span data-i18n="quiz_scorePrefix">Dein Ergebnis:</span> <span id="score-text" class="font-bold"></span></p>
                    <div class="text-center mt-4">
                        <button id="restart-quiz-btn" class="bg-green-500 hover:bg-green-600 text-white py-2 px-6 rounded-md font-semibold" data-i18n="quiz_restartButton">Quiz neu starten</button>
                    </div>
                </div>
            </div>
        </section>


    </main>

    <footer class="bg-blue-700 text-white text-center p-8 mt-16">
        <p class="text-lg" data-i18n="footerText1">&copy; 2025 Detaillierte Reinforcement Learning Infografik.</p>
        <p class="text-sm" data-i18n="footerText2">Basierend auf Vorlesungsmaterialien und Standard-RL-Konzepten.</p>
    </footer>

    <script>
        // --- I18N Data ---
        const translations = {
            de: {
                pageTitle: "Reinforcement Learning Infografik - Detailliert mit Quiz",
                headerTitle: "Reinforcement Learning: Detaillierte Einblicke & Quiz",
                headerSubtitle: "Vertiefte Konzepte, Algorithmen und interaktives Quiz",
                section1Title: "1. Grundlagen: RL & Markov-Entscheidungsprozesse (MDPs)",
                section1Intro: "Reinforcement Learning (RL) ist ein Paradigma des maschinellen Lernens, bei dem ein autonomer <strong>Agent</strong> lernt, optimale Sequenzen von Entscheidungen (Aktionen) in einer komplexen und oft unsicheren <strong>Umgebung</strong> zu treffen. Das prim√§re Ziel des Agenten ist es, die Summe der erhaltenen <strong>Belohnungen</strong> √ºber die Zeit zu maximieren. Dieser Lernprozess ist fundamental iterativ und erfahrungsbasiert.",
                agentEnvCycleTitle: "Der Agent-Umgebung-Interaktionszyklus",
                cycleStep1: "1. Agent beobachtet aktuellen Zustand $S_t$ der Umgebung.",
                cycleStep2: "2. Agent w√§hlt eine Aktion $A_t$ basierend auf seiner aktuellen Strategie (Policy $\\pi$).",
                cycleStep3: "3. Umgebung reagiert: Agent erh√§lt eine Belohnung $R_{t+1}$ und beobachtet den neuen Zustand $S_{t+1}$.",
                cycleStep4: "4. Agent aktualisiert seine Policy $\\pi$ basierend auf der Erfahrung $(S_t, A_t, R_{t+1}, S_{t+1})$.",
                cycleDesc: "Dieser Zyklus wiederholt sich, wodurch der Agent schrittweise lernt, bessere Entscheidungen zu treffen.",
                goalTitle: "Maximierung des kumulativen Returns",
                goalDesc: "Der Return $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ ist die Summe der diskontierten zuk√ºnftigen Belohnungen. Die <strong>Reward Hypothesis</strong> besagt, dass alle Ziele durch die Maximierung des erwarteten kumulativen Rewards formuliert werden k√∂nnen.",
                rewardHypothesis: "Reward Hypothesis",
                gammaFactorTitle: "Der Diskontierungsfaktor $\\gamma$",
                gammaDesc: "Steuert die Wichtigkeit zuk√ºnftiger Belohnungen:",
                gammaShortSighted: "$\\gamma \\approx 0$: Agent ist \"kurzsichtig\", fokussiert auf unmittelbare Belohnungen.",
                gammaLongSighted: "$\\gamma \\approx 1$: Agent ist \"weitsichtig\", ber√ºcksichtigt zuk√ºnftige Belohnungen stark.",
                gammaContinuous: "Notwendig f√ºr kontinuierliche Aufgaben, um unendliche Returns zu vermeiden.",
                mdpTitle: "Der Markov-Entscheidungsprozess (MDP)",
                mdpIntro: "Ein MDP ist der Standardformalismus f√ºr RL-Probleme, die die Markov-Eigenschaft erf√ºllen. Er wird durch ein Tupel $(S, A, P, R, \\gamma)$ definiert:",
                mdpS: "<strong>S (Zustandsraum):</strong> Eine Menge aller m√∂glichen Zust√§nde. Z.B. Positionen auf einem Schachbrett, Gelenkwinkel eines Roboters.",
                mdpA: "<strong>A (Aktionsraum):</strong> Eine Menge aller m√∂glichen Aktionen. Z.B. Z√ºge im Schach, Motorkommandos f√ºr einen Roboter.",
                mdpP: "<strong>P (Zustands√ºbergangsmodell):</strong> $P(s'|s, a) = Pr\\{S_{t+1}=s' | S_t=s, A_t=a\\}$. Definiert die Dynamik der Umgebung.",
                mdpR: "<strong>R (Belohnungsfunktion):</strong> $R(s, a, s') = E[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$. Definiert das Ziel des Agenten.",
                markovPropertyTitle: "Die Markov-Eigenschaft",
                markovPropertyQuote: "\"Die Zukunft ist unabh√§ngig von der Vergangenheit, gegeben die Gegenwart.\"",
                markovPropertyDesc: "Der aktuelle Zustand $S_t$ enth√§lt alle relevanten Informationen aus der Historie, um die n√§chste Zustands- und Belohnungswahrscheinlichkeit zu bestimmen. Dies ist eine starke Vereinfachung, die viele RL-Algorithmen erm√∂glicht.",
                coreChallengesTitle: "Kernherausforderungen und Konzepte",
                creditAssignmentTitle: "Credit Assignment",
                creditAssignmentDesc: "Wie k√∂nnen Belohnungen (oder Bestrafungen), die erst sp√§t in einer Sequenz von Aktionen auftreten, korrekt den fr√ºheren Aktionen zugeordnet werden, die urs√§chlich daf√ºr waren?",
                exploreExploitTitle: "Exploration vs. Exploitation",
                exploreExploitDesc: "Der Agent muss entscheiden, ob er neue Aktionen ausprobiert, um mehr √ºber die Umgebung zu lernen (Exploration), oder ob er die aktuell besten bekannten Aktionen ausf√ºhrt, um die Belohnung zu maximieren (Exploitation).",
                policyTitle: "Policy ($\\pi$): Die Strategie des Agenten",
                policyDesc: "Eine Abbildung von Zust√§nden zu Aktionen. Deterministisch: $\\pi(s) = a$. Stochastisch: $\\pi(a|s) = Pr\\{A_t=a | S_t=s\\}$.",
                valueFunctionTitle: "Wertfunktionen ($V^\\pi, Q^\\pi$): Wie gut ist ein Zustand/Aktion?",
                valueFunctionDesc: "$V^\\pi(s)$: Erwarteter Return, wenn man in Zustand $s$ startet und Policy $\\pi$ folgt. $Q^\\pi(s,a)$: Erwarteter Return, wenn man in Zustand $s$ Aktion $a$ ausf√ºhrt und danach Policy $\\pi$ folgt.",
                section2Title: "2. Dynamische Programmierung (DP): Planung mit perfektem Modell",
                section2Intro: "Dynamische Programmierung (DP) umfasst Algorithmen, die eine optimale Policy $\\pi^*$ f√ºr einen gegebenen MDP berechnen k√∂nnen, vorausgesetzt, das Modell der Umgebung (√úbergangswahrscheinlichkeiten $P$ und Belohnungen $R$) ist vollst√§ndig bekannt. DP-Methoden nutzen Wertfunktionen, um die Suche nach guten Policies zu strukturieren und basieren auf den Bellman-Gleichungen.",
                bellmanOptimalityPrincipleTitle: "Bellman-Optimalit√§tsprinzip",
                bellmanOptimalityPrincipleQuote: "\"Eine optimale Policy hat die Eigenschaft, dass, was auch immer der Anfangszustand und die erste Entscheidung sind, die verbleibenden Entscheidungen eine optimale Policy bez√ºglich des aus der ersten Entscheidung resultierenden Zustands darstellen m√ºssen.\" (Bellman, 1957)",
                bellmanOptimalityPrincipleDesc: "Einfach gesagt: Wenn der beste Pfad von A nach C √ºber B f√ºhrt, dann muss der Teilpfad von B nach C auch der beste Pfad von B nach C sein.",
                policyIterationTitle: "Policy Iteration (Politikiteration)",
                policyIterationDesc: "Ein iterativer Algorithmus, der zwischen zwei Schritten wechselt:",
                policyEvalTitle: "1. Policy Evaluation (Bewertung):",
                policyEvalDesc: "Berechne die Zustands-Wertfunktion $V^\\pi(s)$ f√ºr die aktuelle Policy $\\pi$. Dies geschieht durch iteratives Anwenden der Bellman-Erwartungsgleichung bis zur Konvergenz:",
                policyImprovTitle: "2. Policy Improvement (Verbesserung):",
                policyImprovDesc: "Verbessere die Policy, indem f√ºr jeden Zustand $s$ gierig die Aktion gew√§hlt wird, die $Q^\\pi(s,a)$ maximiert:",
                policyIterationConvergence: "Dieser Prozess konvergiert garantiert zur optimalen Policy $\\pi^*$. \"Sweeps\" beziehen sich auf vollst√§ndige Durchl√§ufe √ºber alle Zust√§nde w√§hrend der Evaluation oder Iteration.",
                valueIterationTitle: "Value Iteration (Wertiteration)",
                valueIterationDesc: "Kombiniert im Wesentlichen einen Schritt der Policy Evaluation und einen Schritt der Policy Improvement in einer einzigen Update-Regel. Sie iteriert direkt √ºber die Bellman-Optimalit√§tsgleichung f√ºr $V^*(s)$:",
                valueIterationConvergence: "Konvergiert ebenfalls zur optimalen Wertfunktion $V^*$, aus der die optimale Policy $\\pi^*$ dann durch eine einmalige gierige Auswahl extrahiert werden kann.",
                vQRelationTitle: "Beziehung zwischen $V^\\pi$ und $Q^\\pi$",
                vQRelationDesc: "Die Zustands-Wertfunktion und die Aktions-Wertfunktion sind eng miteinander verbunden:",
                vQStochastic: "(wenn $\\pi$ stochastisch ist)",
                vQDeterministic: "(wenn $\\pi$ deterministisch ist)",
                dpApplicabilityTitle: "Wann ist DP anwendbar?",
                dpCond1: "Wenn ein vollst√§ndiges und genaues Modell der Umgebung ($P$ und $R$) verf√ºgbar ist.",
                dpCond2: "Eher f√ºr Planungsprobleme als f√ºr direktes Lernen aus Interaktion.",
                dpCond3: "Obwohl rechenintensiv f√ºr sehr gro√üe Probleme, bilden DP-Konzepte die theoretische Grundlage f√ºr viele andere RL-Algorithmen.",
                section3Title: "3. Modellfreie Vorhersage: Lernen ohne Modell",
                section3Intro: "In vielen realen Szenarien ist ein exaktes Modell der Umgebung nicht verf√ºgbar. Modellfreie Vorhersagemethoden erm√∂glichen es, Wertfunktionen ($V^\\pi$ oder $Q^\\pi$) f√ºr eine gegebene Policy $\\pi$ direkt aus Erfahrungsepisoden zu sch√§tzen, die durch Interaktion mit der Umgebung generiert werden.",
                mcMethodsTitle: "a) Monte Carlo (MC) Methoden",
                mcMethodsDesc: "MC-Methoden lernen, indem sie den durchschnittlichen Return nach Besuchen eines Zustands √ºber viele vollst√§ndige Episoden berechnen.<br><strong>First-Visit MC:</strong> Ber√ºcksichtigt nur den Return nach dem ersten Besuch eines Zustands $s$ in einer Episode.<br><strong>Every-Visit MC:</strong> Ber√ºcksichtigt den Return nach jedem Besuch eines Zustands $s$ in einer Episode.",
                firstVisitMC: "First-Visit MC:",
                everyVisitMC: "Every-Visit MC:",
                mcUpdateRuleTitle: "Update-Regel (inkrementell f√ºr $V(S_t)$):",
                mcUpdateRuleDesc: "Hier ist $G_t$ der tats√§chlich beobachtete (vollst√§ndige) Return ab Zeitschritt $t$ bis zum Ende der Episode. $\\alpha$ ist die Lernrate.",
                mcAdvantages: "Vorteile:",
                mcAdv1: "Unverzerrt (kein Bootstrapping-Bias).",
                mcAdv2: "Einfach zu verstehen und zu implementieren.",
                mcDisadvantages: "Nachteile:",
                mcDisadv1: "Hohe Varianz, da der Return von vielen zuf√§lligen Aktionen und √úberg√§ngen abh√§ngt.",
                mcDisadv2: "Funktioniert nur f√ºr episodische (terminierende) Aufgaben.",
                mcDisadv3: "Updates erfolgen erst am Ende einer Episode.",
                tdLearningTitle: "b) Temporal Difference (TD) Learning",
                tdLearningDesc: "TD-Methoden lernen von jedem Schritt, indem sie ihre aktuelle Sch√§tzung basierend auf der Sch√§tzung des n√§chsten Zustands anpassen ‚Äì ein Prozess, der als <strong data-i18n=\"bootstrapping\">Bootstrapping</strong> bezeichnet wird. Sie ben√∂tigen keine vollst√§ndigen Episoden.",
                bootstrapping: "Bootstrapping",
                tdUpdateRuleTitle: "TD(0) Update-Regel f√ºr $V(S_t)$:",
                tdUpdateRuleDesc: "Der Term $R_{t+1} + \\gamma V(S_{t+1})$ ist das <strong data-i18n=\"tdTarget\">TD-Target</strong> (eine Sch√§tzung des Returns), und $[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$ ist der <strong data-i18n=\"tdError\">TD-Fehler</strong>.",
                tdTarget: "TD-Target",
                tdError: "TD-Fehler",
                tdAdvantages: "Vorteile:",
                tdAdv1: "Geringere Varianz als MC.",
                tdAdv2: "Kann online lernen (nach jedem Schritt).",
                tdAdv3: "Funktioniert auch f√ºr kontinuierliche Aufgaben.",
                tdDisadvantages: "Nachteile:",
                tdDisadv1: "Verzerrt, da die Sch√§tzung auf einer anderen Sch√§tzung ($V(S_{t+1})$) basiert.",
                tdDisadv2: "Empfindlicher gegen√ºber der Initialisierung der Werte.",
                nStepTDTitle: "N-Step TD Methoden",
                nStepTDDesc1: "Diese Methoden schlagen eine Br√ºcke zwischen MC und TD(0). Anstatt nur einen Schritt (TD(0)) oder die gesamte Episode (MC) f√ºr das Update zu betrachten, verwenden n-Step TD Methoden einen Return √ºber $n$ Schritte:",
                nStepTDDesc2: "Die Update-Regel lautet dann:",
                nStepTDDesc3: "Der Parameter $n$ erlaubt einen Trade-off zwischen dem Bias von TD(0) und der Varianz von MC.",
                mcTdChartTitle: "Visueller Vergleich: MC vs. TD Update",
                mcTdChartDesc: "Das Diagramm illustriert die unterschiedlichen St√§rken und Schw√§chen von MC- und TD-Lernans√§tzen bez√ºglich Bias, Varianz und Update-Frequenz.",
                section4Title: "4. Modellfreie Steuerung: Optimale Policies ohne Modell finden",
                section4Intro: "Modellfreie Steuerungsmethoden erweitern die Ideen der modellfreien Vorhersage, um nicht nur Wertfunktionen zu sch√§tzen, sondern auch aktiv eine (nahezu) optimale Policy $\\pi^*$ zu finden. Da kein Umgebungsmodell vorhanden ist, lernen diese Methoden typischerweise die Aktions-Wertfunktion $Q(s,a)$, da diese direkt angibt, wie gut es ist, eine bestimmte Aktion $a$ in einem Zustand $s$ auszuf√ºhren. Die Policy wird dann oft $\\epsilon$-gierig auf Basis dieser $Q$-Werte abgeleitet.",
                gpiTitle: "Generalisierte Policy Iteration (GPI) f√ºr Modellfreie Steuerung",
                gpiEvalTitle: "1. Policy Evaluation (Sch√§tzung)",
                gpiEvalDesc: "Sch√§tze $Q^\\pi(s,a)$ f√ºr die aktuelle Policy $\\pi$ (z.B. mittels MC oder TD Updates auf $Q$-Werte).",
                gpiImprovTitle: "2. Policy Improvement (Verbesserung)",
                gpiImprovDesc: "Aktualisiere $\\pi$, indem $\\epsilon$-gierig bez√ºglich der aktuellen $Q$-Werte agiert wird:",
                gpiEpsilon: "(mit $\\epsilon$ W'keit zuf√§llig).",
                gpiConvergence: "Dieser iterative Prozess konvergiert typischerweise gegen die optimale Aktions-Wertfunktion $Q^*$ und die optimale Policy $\\pi^*$. Die Notwendigkeit von $Q(s,a)$ ergibt sich daraus, dass ohne Modell $P(s'|s,a)$ die $V(s)$-Funktion allein nicht ausreicht, um eine gierige Aktion zu bestimmen.",
                sarsaTitle: "SARSA (On-Policy TD Control)",
                sarsaDesc: "SARSA ist ein On-Policy TD-Algorithmus. \"On-Policy\" bedeutet, dass die $Q$-Werte f√ºr die Policy gesch√§tzt und verbessert werden, die der Agent tats√§chlich ausf√ºhrt (inklusive explorativer Aktionen).",
                sarsaUpdateRuleTitle: "Update-Regel f√ºr $Q(S_t, A_t)$:",
                sarsaNameOrigin: "Der Name SARSA kommt von der Sequenz der Ereignisse im Update: $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$. $A_{t+1}$ wird gem√§√ü der aktuellen (oft $\\epsilon$-greedy) Policy gew√§hlt.",
                expectedSarsaTitle: "Expected SARSA",
                expectedSarsaDesc: "Eine Variante, die den Erwartungswert √ºber alle m√∂glichen Aktionen $A_{t+1}$ gem√§√ü der aktuellen Policy $\\pi$ nimmt, anstatt die spezifisch gew√§hlte Aktion $A_{t+1}$ zu verwenden. Dies kann die Varianz reduzieren.",
                qLearningTitle: "Q-Learning (Off-Policy TD Control)",
                qLearningDesc: "Q-Learning ist ein Off-Policy TD-Algorithmus. \"Off-Policy\" bedeutet, dass es die optimale Aktions-Wertfunktion $Q^*(s,a)$ lernt, unabh√§ngig von der Policy, die der Agent zur Exploration verwendet (Verhaltenspolicy), solange diese alle Zustands-Aktions-Paare ausreichend oft besucht.",
                qLearningUpdateRuleTitle: "Update-Regel f√ºr $Q(S_t, A_t)$:",
                qLearningKeyDiff: "Der entscheidende Unterschied ist der Term $\\max_{a'} Q(S_{t+1}, a')$, der den maximalen $Q$-Wert im Folgezustand verwendet, was der Aktion entspricht, die eine gierige Policy w√§hlen w√ºrde (Zielpolicy).",
                doubleQLearningTitle: "Double Q-Learning",
                doubleQLearningDesc: "Standard Q-Learning kann unter bestimmten Umst√§nden zu einer √úbersch√§tzung der Q-Werte f√ºhren (Maximierungs-Bias). Double Q-Learning adressiert dies, indem es zwei separate Q-Wert-Sch√§tzungen ($Q_A$ und $Q_B$) pflegt. Eine wird zur Auswahl der besten Aktion im n√§chsten Zustand verwendet, die andere zur Bewertung dieser Aktion.",
                doubleQLearningUpdate: "Z.B. Update f√ºr $Q_A$: $Q_A(S_t,A_t) \\leftarrow Q_A(S_t,A_t) + \\alpha [R_{t+1} + \\gamma Q_B(S_{t+1}, \\text{argmax}_{a'} Q_A(S_{t+1},a')) - Q_A(S_t,A_t)]$ (und umgekehrt f√ºr $Q_B$).",
                onOffPolicyCompareTitle: "Vergleich der Update-Pfade: SARSA vs. Q-Learning",
                sarsaPathTitle: "SARSA (On-Policy)",
                sarsaPathDesc: "Update f√ºr $Q(S_t, A_t)$ verwendet $Q(S_{t+1}, A_{t+1})$. Lernt den Wert der tats√§chlich ausgef√ºhrten Policy.",
                qLearningPathTitle: "Q-Learning (Off-Policy)",
                qLearningPathDesc: "Update f√ºr $Q(S_t, A_t)$ verwendet $\\max_{a'} Q(S_{t+1}, a')$. Lernt den Wert der optimalen (gierigen) Policy, auch wenn die Aktionen anders gew√§hlt wurden.",
                epsilonGreedyTitle: "Bedeutung von $\\epsilon$-greedy Exploration:",
                epsilonGreedyDesc: "Um sicherzustellen, dass der Agent alle relevanten Zustands-Aktions-Paare erkundet und nicht in lokalen Optima stecken bleibt, wird oft eine $\\epsilon$-greedy Strategie verwendet. Mit einer kleinen Wahrscheinlichkeit $\\epsilon$ w√§hlt der Agent eine zuf√§llige Aktion, anstatt der aktuell als optimal erachteten (gierigen) Aktion. Der Wert von $\\epsilon$ wird oft im Laufe des Trainings reduziert (Annealing), um von Exploration zu Exploitation √ºberzugehen.",
                vfaTitle: "Ausblick: Wertfunktionsapproximation (VFA)",
                vfaDesc: "F√ºr Probleme mit sehr gro√üen oder kontinuierlichen Zustandsr√§umen sind tabellarische Darstellungen von $Q(s,a)$ (eine Tabelle f√ºr jedes Zustands-Aktions-Paar) nicht mehr praktikabel. Hier kommt die Wertfunktionsapproximation ins Spiel: Anstatt die Q-Werte exakt zu speichern, werden sie durch eine parametrisierte Funktion approximiert, z.B. eine lineare Funktion oder ein neuronales Netz: $Q(s,a; \\mathbf{w}) \\approx Q^*(s,a)$. Die Parameter $\\mathbf{w}$ werden dann gelernt.",
                vfaExample: "Ein bekanntes Beispiel ist das Deep Q-Network (DQN), das ein tiefes neuronales Netz zur Approximation der Q-Funktion verwendet und bemerkenswerte Erfolge in komplexen Umgebungen wie Atari-Spielen erzielt hat.",
                quizSectionTitle: "Wissens-Quiz zur Zwischenpr√ºfung",
                quizLoading: "Frage wird geladen...",
                quiz_nextButton: "N√§chste Frage",
                quiz_resultsButton: "Ergebnisse anzeigen",
                quiz_finishedTitle: "Quiz beendet!",
                quiz_scorePrefix: "Dein Ergebnis: ",
                quiz_scoreSuffix_singular: "Punkt",
                quiz_scoreSuffix_plural: "Punkte",
                quiz_scoreOutOf: "von",
                quiz_restartButton: "Quiz neu starten",
                quiz_feedbackCorrect: "Richtig!",
                quiz_feedbackIncorrectPrefix: "Falsch. Die richtige Antwort war: ",
                quiz_questionCounterPrefix: "Frage ",
                quiz_questionCounterOf: "von",
                footerText1: "&copy; 2025 Detaillierte Reinforcement Learning Infografik.",
                footerText2: "Basierend auf Vorlesungsmaterialien und Standard-RL-Konzepten.",
                chart_mcVsTd_title: "Vergleich: MC vs. TD Eigenschaften (Skala 1=Niedrig/Nein, 5=Hoch/Ja)",
                chart_mcVsTd_dataset_mc: "Monte Carlo (MC)",
                chart_mcVsTd_dataset_td: "Temporal Difference (TD)",
                chart_mcVsTd_label_bias: "Bias",
                chart_mcVsTd_label_variance: "Varianz",
                chart_mcVsTd_label_updateFreq: "Update H√§ufigkeit",
                chart_mcVsTd_label_incompleteEp: "Nutzung unvollst. Episoden",
                chart_mcVsTd_label_initSens: "Sensitivit√§t Initialisierung",
                chart_mcVsTd_yAxis_lowNo: "1 (Niedrig/Nein)",
                chart_mcVsTd_yAxis_medium: "3 (Mittel)",
                chart_mcVsTd_yAxis_highYes: "5 (Hoch/Ja)",
                tooltip_bias_low_mc: "Niedrig (Unverzerrt)",
                tooltip_bias_medium_td: "Mittel (Verzerrt)",
                tooltip_variance_high_mc: "Hoch",
                tooltip_variance_low_td: "Niedrig",
                tooltip_update_episode_mc: "Am Episodenende",
                tooltip_update_step_td: "Nach jedem Schritt",
                tooltip_incomplete_no_mc: "Nein",
                tooltip_incomplete_yes_td: "Ja (Bootstrapping)",
                tooltip_init_insensitive_mc: "Unempfindlich",
                tooltip_init_sensitive_td: "Empfindlich",
            },
            en: {
                pageTitle: "Reinforcement Learning Infographic - Detailed with Quiz",
                headerTitle: "Reinforcement Learning: Detailed Insights & Quiz",
                headerSubtitle: "In-depth Concepts, Algorithms, and Interactive Quiz",
                section1Title: "1. Basics: RL & Markov Decision Processes (MDPs)",
                section1Intro: "Reinforcement Learning (RL) is a paradigm of machine learning where an autonomous <strong>agent</strong> learns to make optimal sequences of decisions (actions) in a complex and often uncertain <strong>environment</strong>. The primary goal of the agent is to maximize the sum of received <strong>rewards</strong> over time. This learning process is fundamentally iterative and experience-based.",
                agentEnvCycleTitle: "The Agent-Environment Interaction Cycle",
                cycleStep1: "1. Agent observes current state $S_t$ of the environment.",
                cycleStep2: "2. Agent selects an action $A_t$ based on its current strategy (policy $\\pi$).",
                cycleStep3: "3. Environment reacts: Agent receives a reward $R_{t+1}$ and observes the new state $S_{t+1}$.",
                cycleStep4: "4. Agent updates its policy $\\pi$ based on the experience $(S_t, A_t, R_{t+1}, S_{t+1})$.",
                cycleDesc: "This cycle repeats, allowing the agent to gradually learn to make better decisions.",
                goalTitle: "Maximizing Cumulative Return",
                goalDesc: "The return $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ is the sum of discounted future rewards. The <strong>Reward Hypothesis</strong> states that all goals can be formulated as maximizing the expected cumulative reward.",
                rewardHypothesis: "Reward Hypothesis",
                gammaFactorTitle: "The Discount Factor $\\gamma$",
                gammaDesc: "Controls the importance of future rewards:",
                gammaShortSighted: "$\\gamma \\approx 0$: Agent is \"myopic\", focusing on immediate rewards.",
                gammaLongSighted: "$\\gamma \\approx 1$: Agent is \"far-sighted\", strongly considering future rewards.",
                gammaContinuous: "Necessary for continuous tasks to avoid infinite returns.",
                mdpTitle: "The Markov Decision Process (MDP)",
                mdpIntro: "An MDP is the standard formalism for RL problems that satisfy the Markov property. It is defined by a tuple $(S, A, P, R, \\gamma)$:",
                mdpS: "<strong>S (State Space):</strong> A set of all possible states. E.g., positions on a chessboard, joint angles of a robot.",
                mdpA: "<strong>A (Action Space):</strong> A set of all possible actions. E.g., moves in chess, motor commands for a robot.",
                mdpP: "<strong>P (State Transition Model):</strong> $P(s'|s, a) = Pr\\{S_{t+1}=s' | S_t=s, A_t=a\\}$. Defines the dynamics of the environment.",
                mdpR: "<strong>R (Reward Function):</strong> $R(s, a, s') = E[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$. Defines the agent's goal.",
                markovPropertyTitle: "The Markov Property",
                markovPropertyQuote: "\"The future is independent of the past, given the present.\"",
                markovPropertyDesc: "The current state $S_t$ contains all relevant information from the history to determine the next state and reward probabilities. This is a strong simplification that enables many RL algorithms.",
                coreChallengesTitle: "Core Challenges and Concepts",
                creditAssignmentTitle: "Credit Assignment",
                creditAssignmentDesc: "How can rewards (or punishments) that occur late in a sequence of actions be correctly attributed to the earlier actions that caused them?",
                exploreExploitTitle: "Exploration vs. Exploitation",
                exploreExploitDesc: "The agent must decide whether to try new actions to learn more about the environment (exploration) or to execute the currently known best actions to maximize reward (exploitation).",
                policyTitle: "Policy ($\\pi$): The Agent's Strategy",
                policyDesc: "A mapping from states to actions. Deterministic: $\\pi(s) = a$. Stochastic: $\\pi(a|s) = Pr\\{A_t=a | S_t=s\\}$.",
                valueFunctionTitle: "Value Functions ($V^\\pi, Q^\\pi$): How good is a state/action?",
                valueFunctionDesc: "$V^\\pi(s)$: Expected return starting in state $s$ and following policy $\\pi$. $Q^\\pi(s,a)$: Expected return starting in state $s$, taking action $a$, and thereafter following policy $\\pi$.",
                section2Title: "2. Dynamic Programming (DP): Planning with a Perfect Model",
                section2Intro: "Dynamic Programming (DP) comprises algorithms that can compute an optimal policy $\\pi^*$ for a given MDP, provided that the model of the environment (transition probabilities $P$ and rewards $R$) is fully known. DP methods use value functions to structure the search for good policies and are based on Bellman equations.",
                bellmanOptimalityPrincipleTitle: "Bellman's Principle of Optimality",
                bellmanOptimalityPrincipleQuote: "\"An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.\" (Bellman, 1957)",
                bellmanOptimalityPrincipleDesc: "Simply put: If the best path from A to C goes through B, then the subpath from B to C must also be the best path from B to C.",
                policyIterationTitle: "Policy Iteration",
                policyIterationDesc: "An iterative algorithm that alternates between two steps:",
                policyEvalTitle: "1. Policy Evaluation:",
                policyEvalDesc: "Compute the state-value function $V^\\pi(s)$ for the current policy $\\pi$. This is done by iteratively applying the Bellman expectation equation until convergence:",
                policyImprovTitle: "2. Policy Improvement:",
                policyImprovDesc: "Improve the policy by choosing greedily for each state $s$ the action that maximizes $Q^\\pi(s,a)$:",
                policyIterationConvergence: "This process is guaranteed to converge to the optimal policy $\\pi^*$. \"Sweeps\" refer to complete passes through all states during evaluation or iteration.",
                valueIterationTitle: "Value Iteration",
                valueIterationDesc: "Essentially combines one step of policy evaluation and one step of policy improvement into a single update rule. It directly iterates on the Bellman optimality equation for $V^*(s)$:",
                valueIterationConvergence: "Also converges to the optimal value function $V^*$, from which the optimal policy $\\pi^*$ can then be extracted by a single greedy selection.",
                vQRelationTitle: "Relationship between $V^\\pi$ and $Q^\\pi$",
                vQRelationDesc: "The state-value function and action-value function are closely related:",
                vQStochastic: "(if $\\pi$ is stochastic)",
                vQDeterministic: "(if $\\pi$ is deterministic)",
                dpApplicabilityTitle: "When is DP Applicable?",
                dpCond1: "When a complete and accurate model of the environment ($P$ and $R$) is available.",
                dpCond2: "More for planning problems than for direct learning from interaction.",
                dpCond3: "Although computationally intensive for very large problems, DP concepts form the theoretical basis for many other RL algorithms.",
                section3Title: "3. Model-Free Prediction: Learning without a Model",
                section3Intro: "In many real-world scenarios, an exact model of the environment is not available. Model-free prediction methods allow estimating value functions ($V^\\pi$ or $Q^\\pi$) for a given policy $\\pi$ directly from experience episodes generated through interaction with the environment.",
                mcMethodsTitle: "a) Monte Carlo (MC) Methods",
                mcMethodsDesc: "MC methods learn by averaging returns observed after visits to a state over many complete episodes.<br><strong>First-Visit MC:</strong> Considers only the return after the first visit to state $s$ in an episode.<br><strong>Every-Visit MC:</strong> Considers the return after every visit to state $s$ in an episode.",
                firstVisitMC: "First-Visit MC:",
                everyVisitMC: "Every-Visit MC:",
                mcUpdateRuleTitle: "Update Rule (incremental for $V(S_t)$):",
                mcUpdateRuleDesc: "Here, $G_t$ is the actual observed (full) return from time step $t$ to the end of the episode. $\\alpha$ is the learning rate.",
                mcAdvantages: "Advantages:",
                mcAdv1: "Unbiased (no bootstrapping bias).",
                mcAdv2: "Easy to understand and implement.",
                mcDisadvantages: "Disadvantages:",
                mcDisadv1: "High variance, as the return depends on many random actions and transitions.",
                mcDisadv2: "Only works for episodic (terminating) tasks.",
                mcDisadv3: "Updates are made only at the end of an episode.",
                tdLearningTitle: "b) Temporal Difference (TD) Learning",
                tdLearningDesc: "TD methods learn from each step, updating their current estimate based on the estimate of the next state‚Äîa process called <strong data-i18n=\"bootstrapping\">bootstrapping</strong>. They do not require complete episodes.",
                bootstrapping: "bootstrapping",
                tdUpdateRuleTitle: "TD(0) Update Rule for $V(S_t)$:",
                tdUpdateRuleDesc: "The term $R_{t+1} + \\gamma V(S_{t+1})$ is the <strong data-i18n=\"tdTarget\">TD target</strong> (an estimate of the return), and $[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$ is the <strong data-i18n=\"tdError\">TD error</strong>.",
                tdTarget: "TD target",
                tdError: "TD error",
                tdAdvantages: "Advantages:",
                tdAdv1: "Lower variance than MC.",
                tdAdv2: "Can learn online (after each step).",
                tdAdv3: "Also works for continuous tasks.",
                tdDisadvantages: "Disadvantages:",
                tdDisadv1: "Biased, as the estimate is based on another estimate ($V(S_{t+1})$).",
                tdDisadv2: "More sensitive to the initialization of values.",
                nStepTDTitle: "N-Step TD Methods",
                nStepTDDesc1: "These methods bridge the gap between MC and TD(0). Instead of looking only one step ahead (TD(0)) or at the entire episode (MC), n-step TD methods use a return over $n$ steps:",
                nStepTDDesc2: "The update rule is then:",
                nStepTDDesc3: "The parameter $n$ allows a trade-off between the bias of TD(0) and the variance of MC.",
                mcTdChartTitle: "Visual Comparison: MC vs. TD Update",
                mcTdChartDesc: "The diagram illustrates the different strengths and weaknesses of MC and TD learning approaches regarding bias, variance, and update frequency.",
                section4Title: "4. Model-Free Control: Finding Optimal Policies without a Model",
                section4Intro: "Model-free control methods extend the ideas of model-free prediction to not only estimate value functions but also to actively find a (near) optimal policy $\\pi^*$. Since no environment model is available, these methods typically learn the action-value function $Q(s,a)$, as it directly indicates how good it is to take a specific action $a$ in a state $s$. The policy is then often derived $\\epsilon$-greedily based on these $Q$-values.",
                gpiTitle: "Generalized Policy Iteration (GPI) for Model-Free Control",
                gpiEvalTitle: "1. Policy Evaluation (Estimation)",
                gpiEvalDesc: "Estimate $Q^\\pi(s,a)$ for the current policy $\\pi$ (e.g., using MC or TD updates on $Q$-values).",
                gpiImprovTitle: "2. Policy Improvement",
                gpiImprovDesc: "Update $\\pi$ by acting $\\epsilon$-greedily with respect to the current $Q$-values:",
                gpiEpsilon: "(with $\\epsilon$ probability random).",
                gpiConvergence: "This iterative process typically converges towards the optimal action-value function $Q^*$ and the optimal policy $\\pi^*$. The need for $Q(s,a)$ arises because, without a model $P(s'|s,a)$, the $V(s)$ function alone is insufficient to determine a greedy action.",
                sarsaTitle: "SARSA (On-Policy TD Control)",
                sarsaDesc: "SARSA is an on-policy TD algorithm. \"On-policy\" means that the $Q$-values are estimated and improved for the policy the agent is actually executing (including exploratory actions).",
                sarsaUpdateRuleTitle: "Update Rule for $Q(S_t, A_t)$:",
                sarsaNameOrigin: "The name SARSA comes from the sequence of events in the update: $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$. $A_{t+1}$ is chosen according to the current (often $\\epsilon$-greedy) policy.",
                expectedSarsaTitle: "Expected SARSA",
                expectedSarsaDesc: "A variant that takes the expectation over all possible actions $A_{t+1}$ according to the current policy $\\pi$, instead of using the specifically chosen action $A_{t+1}$. This can reduce variance.",
                qLearningTitle: "Q-Learning (Off-Policy TD Control)",
                qLearningDesc: "Q-Learning is an off-policy TD algorithm. \"Off-policy\" means it learns the optimal action-value function $Q^*(s,a)$ independently of the policy used by the agent for exploration (behavior policy), as long as it visits all state-action pairs sufficiently often.",
                qLearningUpdateRuleTitle: "Update Rule for $Q(S_t, A_t)$:",
                qLearningKeyDiff: "The key difference is the term $\\max_{a'} Q(S_{t+1}, a')$, which uses the maximum $Q$-value in the next state, corresponding to the action a greedy policy would choose (target policy).",
                doubleQLearningTitle: "Double Q-Learning",
                doubleQLearningDesc: "Standard Q-Learning can, under certain circumstances, lead to an overestimation of Q-values (maximization bias). Double Q-Learning addresses this by maintaining two separate Q-value estimates ($Q_A$ and $Q_B$). One is used to select the best action in the next state, and the other is used to evaluate that action.",
                doubleQLearningUpdate: "E.g., Update for $Q_A$: $Q_A(S_t,A_t) \\leftarrow Q_A(S_t,A_t) + \\alpha [R_{t+1} + \\gamma Q_B(S_{t+1}, \\text{argmax}_{a'} Q_A(S_{t+1},a')) - Q_A(S_t,A_t)]$ (and vice versa for $Q_B$).",
                onOffPolicyCompareTitle: "Comparison of Update Paths: SARSA vs. Q-Learning",
                sarsaPathTitle: "SARSA (On-Policy)",
                sarsaPathDesc: "Update for $Q(S_t, A_t)$ uses $Q(S_{t+1}, A_{t+1})$. Learns the value of the policy actually being executed.",
                qLearningPathTitle: "Q-Learning (Off-Policy)",
                qLearningPathDesc: "Update for $Q(S_t, A_t)$ uses $\\max_{a'} Q(S_{t+1}, a')$. Learns the value of the optimal (greedy) policy, even if actions were chosen differently.",
                epsilonGreedyTitle: "Importance of $\\epsilon$-greedy Exploration:",
                epsilonGreedyDesc: "To ensure the agent explores all relevant state-action pairs and doesn't get stuck in local optima, an $\\epsilon$-greedy strategy is often used. With a small probability $\\epsilon$, the agent chooses a random action instead of the one currently considered optimal (greedy). The value of $\\epsilon$ is often reduced over time (annealing) to shift from exploration to exploitation.",
                vfaTitle: "Outlook: Value Function Approximation (VFA)",
                vfaDesc: "For problems with very large or continuous state spaces, tabular representations of $Q(s,a)$ (a table for each state-action pair) are no longer practical. This is where value function approximation comes into play: Instead of storing Q-values exactly, they are approximated by a parameterized function, e.g., a linear function or a neural network: $Q(s,a; \\mathbf{w}) \\approx Q^*(s,a)$. The parameters $\\mathbf{w}$ are then learned.",
                vfaExample: "A well-known example is the Deep Q-Network (DQN), which uses a deep neural network to approximate the Q-function and has achieved remarkable success in complex environments like Atari games.",
                quizSectionTitle: "Knowledge Quiz for Midterm",
                quizLoading: "Loading question...",
                quiz_nextButton: "Next Question",
                quiz_resultsButton: "Show Results",
                quiz_finishedTitle: "Quiz Finished!",
                quiz_scorePrefix: "Your score: ",
                quiz_scoreSuffix_singular: "point",
                quiz_scoreSuffix_plural: "points",
                quiz_scoreOutOf: "out of",
                quiz_restartButton: "Restart Quiz",
                quiz_feedbackCorrect: "Correct!",
                quiz_feedbackIncorrectPrefix: "Incorrect. The correct answer was: ",
                quiz_questionCounterPrefix: "Question ",
                quiz_questionCounterOf: "of",
                footerText1: "&copy; 2025 Detailed Reinforcement Learning Infographic.",
                footerText2: "Based on lecture materials and standard RL concepts.",
                chart_mcVsTd_title: "Comparison: MC vs. TD Properties (Scale 1=Low/No, 5=High/Yes)",
                chart_mcVsTd_dataset_mc: "Monte Carlo (MC)",
                chart_mcVsTd_dataset_td: "Temporal Difference (TD)",
                chart_mcVsTd_label_bias: "Bias",
                chart_mcVsTd_label_variance: "Variance",
                chart_mcVsTd_label_updateFreq: "Update Frequency",
                chart_mcVsTd_label_incompleteEp: "Uses Incomplete Episodes",
                chart_mcVsTd_label_initSens: "Initialization Sensitivity",
                chart_mcVsTd_yAxis_lowNo: "1 (Low/No)",
                chart_mcVsTd_yAxis_medium: "3 (Medium)",
                chart_mcVsTd_yAxis_highYes: "5 (High/Yes)",
                tooltip_bias_low_mc: "Low (Unbiased)",
                tooltip_bias_medium_td: "Medium (Biased)",
                tooltip_variance_high_mc: "High",
                tooltip_variance_low_td: "Low",
                tooltip_update_episode_mc: "At episode end",
                tooltip_update_step_td: "After each step",
                tooltip_incomplete_no_mc: "No",
                tooltip_incomplete_yes_td: "Yes (Bootstrapping)",
                tooltip_init_insensitive_mc: "Insensitive",
                tooltip_init_sensitive_td: "Sensitive",
            }
        };

        const quizDataStore = {
            de: [
                {
                    question: "Was ist das prim√§re Ziel eines Reinforcement Learning Agenten?",
                    options: [
                        "Die Dynamik der Umgebung perfekt zu verstehen.",
                        "Die kumulative Belohnung √ºber die Zeit zu maximieren.",
                        "M√∂glichst viele Zust√§nde zu explorieren.",
                        "Die Anzahl der ausgef√ºhrten Aktionen zu minimieren."
                    ],
                    answer: 1
                },
                {
                    question: "Welche der folgenden Komponenten geh√∂rt NICHT zu einem Markov Decision Process (MDP)?",
                    options: [
                        "Zustandsraum (S)",
                        "Aktionsraum (A)",
                        "Belohnungsfunktion (R)",
                        "Die interne Speichergr√∂√üe des Agenten."
                    ],
                    answer: 3
                },
                {
                    question: "Dynamische Programmierung (DP) Methoden ben√∂tigen zwingend:",
                    options: [
                        "Kein Modell der Umgebung.",
                        "Ein perfektes Modell der Umgebung (P und R).",
                        "Nur Beispiel-Episoden aus Erfahrung.",
                        "Eine vordefinierte optimale Policy."
                    ],
                    answer: 1
                },
                {
                    question: "Value Iteration berechnet direkt die...",
                    options: [
                        "Optimale Policy $\\pi^*$.",
                        "Optimale Zustands-Wertfunktion $V^*$.",
                        "√úbergangswahrscheinlichkeiten P.",
                        "Belohnungsfunktion R."
                    ],
                    answer: 1
                },
                {
                    question: "Welche Methode lernt aus vollst√§ndigen Episoden, ist unverzerrt, kann aber hohe Varianz aufweisen?",
                    options: [
                        "Temporal Difference (TD) Learning.",
                        "Monte Carlo (MC) Methoden.",
                        "Dynamische Programmierung.",
                        "Q-Learning."
                    ],
                    answer: 1
                },
                {
                    question: "TD(0) Learning aktualisiert seine Wertsch√§tzung basierend auf:",
                    options: [
                        "Dem finalen Ergebnis der gesamten Episode.",
                        "Der beobachteten Belohnung und der gesch√§tzten Wertfunktion des *n√§chsten* Zustands (Bootstrapping).",
                        "Einem perfekten Modell der Zustands√ºberg√§nge.",
                        "Nur der unmittelbaren Belohnung, ignoriert zuk√ºnftige Zust√§nde."
                    ],
                    answer: 1
                },
                {
                    question: "Was bedeutet \"On-Policy\" im Kontext von Algorithmen wie SARSA?",
                    options: [
                        "Es lernt den Wert der optimalen Policy, unabh√§ngig von den Aktionen des Agenten.",
                        "Es lernt den Wert der Policy, die der Agent aktuell verfolgt.",
                        "Es ben√∂tigt keine Policy, um zu lernen.",
                        "Es kann nur verwendet werden, wenn die Policy deterministisch ist."
                    ],
                    answer: 1
                },
                {
                    question: "Q-Learning ist ein \"Off-Policy\" Algorithmus, weil:",
                    options: [
                        "Es immer zuf√§llige Aktionen w√§hlt.",
                        "Seine Updates die tats√§chlich im n√§chsten Schritt ausgef√ºhrte Aktion verwenden.",
                        "Es seine Q-Werte in Richtung des maximal m√∂glichen Q-Wertes des n√§chsten Zustands aktualisiert, unabh√§ngig davon, welche Aktion zur Exploration tats√§chlich ausgef√ºhrt wurde.",
                        "Es eine separate Policy f√ºr die Evaluation und eine andere f√ºr die Steuerung ben√∂tigt."
                    ],
                    answer: 2
                },
                {
                    question: "Was ist der Hauptzweck von Double Q-Learning?",
                    options: [
                        "Die Lernrate zu verdoppeln.",
                        "Den Maximierungs-Bias (√úbersch√§tzung von Q-Werten) zu reduzieren.",
                        "Zwei Umgebungen gleichzeitig zu modellieren.",
                        "Die Exploration zu beschleunigen."
                    ],
                    answer: 1
                },
                {
                    question: "Wertfunktionsapproximation (VFA) wird typischerweise eingesetzt, wenn...",
                    options: [
                        "Das Umgebungsmodell perfekt bekannt ist.",
                        "Der Zustands- und/oder Aktionsraum sehr gro√ü oder kontinuierlich ist.",
                        "Man nur sehr wenige Trainingsdaten hat.",
                        "Man die Varianz der Sch√§tzungen erh√∂hen m√∂chte."
                    ],
                    answer: 1
                }
            ],
            en: [
                 {
                    question: "What is the primary goal of a Reinforcement Learning agent?",
                    options: [
                        "To perfectly understand the dynamics of the environment.",
                        "To maximize the cumulative reward over time.",
                        "To explore as many states as possible.",
                        "To minimize the number of actions taken."
                    ],
                    answer: 1
                },
                {
                    question: "Which of the following components is NOT part of a Markov Decision Process (MDP)?",
                    options: [
                        "State Space (S)",
                        "Action Space (A)",
                        "Reward Function (R)",
                        "The agent's internal memory size."
                    ],
                    answer: 3
                },
                {
                    question: "Dynamic Programming (DP) methods necessarily require:",
                    options: [
                        "No model of the environment.",
                        "A perfect model of the environment (P and R).",
                        "Only sample episodes from experience.",
                        "A predefined optimal policy."
                    ],
                    answer: 1
                },
                {
                    question: "Value Iteration directly computes the...",
                    options: [
                        "Optimal policy $\\pi^*$.",
                        "Optimal state-value function $V^*$.",
                        "Transition probabilities P.",
                        "Reward function R."
                    ],
                    answer: 1
                },
                {
                    question: "Which method learns from complete episodes, is unbiased, but can have high variance?",
                    options: [
                        "Temporal Difference (TD) Learning.",
                        "Monte Carlo (MC) Methods.",
                        "Dynamic Programming.",
                        "Q-Learning."
                    ],
                    answer: 1
                },
                {
                    question: "TD(0) Learning updates its value estimate based on:",
                    options: [
                        "The final outcome of the entire episode.",
                        "The observed reward and the estimated value function of the *next* state (bootstrapping).",
                        "A perfect model of state transitions.",
                        "Only the immediate reward, ignoring future states."
                    ],
                    answer: 1
                },
                {
                    question: "What does \"On-Policy\" mean in the context of algorithms like SARSA?",
                    options: [
                        "It learns the value of the optimal policy, regardless of the agent's actions.",
                        "It learns the value of the policy the agent is currently following.",
                        "It does not require a policy to learn.",
                        "It can only be used if the policy is deterministic."
                    ],
                    answer: 1
                },
                {
                    question: "Q-Learning is an \"Off-Policy\" algorithm because:",
                    options: [
                        "It always chooses random actions.",
                        "Its updates use the action actually taken in the next step.",
                        "It updates its Q-values towards the maximum possible Q-value of the next state, regardless of which action was actually taken for exploration.",
                        "It requires a separate policy for evaluation and another for control."
                    ],
                    answer: 2
                },
                {
                    question: "What is the main purpose of Double Q-Learning?",
                    options: [
                        "To double the learning rate.",
                        "To reduce maximization bias (overestimation of Q-values).",
                        "To model two environments simultaneously.",
                        "To speed up exploration."
                    ],
                    answer: 1
                },
                {
                    question: "Value Function Approximation (VFA) is typically used when...",
                    options: [
                        "The environment model is perfectly known.",
                        "The state and/or action space is very large or continuous.",
                        "There is very little training data.",
                        "One wants to increase the variance of the estimates."
                    ],
                    answer: 1
                }
            ]
        };

        let currentLanguage = 'de'; // Default language
        let currentQuizData = quizDataStore[currentLanguage];
        let chartInstance = null;


        // --- I18N Functions ---
        function setLanguage(lang) {
            currentLanguage = lang;
            document.documentElement.lang = lang;
            currentQuizData = quizDataStore[lang];

            document.querySelectorAll('[data-i18n]').forEach(el => {
                const key = el.dataset.i18n;
                if (translations[lang] && translations[lang][key]) {
                    el.innerHTML = translations[lang][key];
                }
            });
            
            document.title = translations[lang].pageTitle || "Reinforcement Learning Infographic";

            document.getElementById('lang-de-btn').classList.toggle('active', lang === 'de');
            document.getElementById('lang-en-btn').classList.toggle('active', lang === 'en');

            updateChartLanguage();
            if (document.getElementById('quiz-area').style.display !== 'none' && !quizResultsAreaEl.style.display !== 'none') { 
                 if (currentQuestionIndex < currentQuizData.length) { 
                    loadQuestion(); 
                 } else { 
                    showResults();
                 }
            } else if (quizResultsAreaEl.style.display !== 'none') { 
                 showResults(); 
            }


            if (window.MathJax && window.MathJax.typesetPromise) {
                window.MathJax.typesetPromise();
            }
        }

        document.getElementById('lang-de-btn').addEventListener('click', () => setLanguage('de'));
        document.getElementById('lang-en-btn').addEventListener('click', () => setLanguage('en'));


        // --- Chart Logic ---
        function wrapLabels(label, maxWidth) {
            const lang = currentLanguage || 'de'; 
            const originalLabel = (typeof label === 'object' && label !== null && label[lang]) ? label[lang] : String(label);

            if (typeof originalLabel !== 'string') {
                if (Array.isArray(originalLabel)) return originalLabel;
                return [String(originalLabel)];
            }
            const words = originalLabel.split(' ');
            let lines = [];
            let currentLine = '';
            for (const word of words) {
                if ((currentLine + word).length > maxWidth && currentLine.length > 0) {
                    lines.push(currentLine.trim());
                    currentLine = word + ' ';
                } else {
                    currentLine += word + ' ';
                }
            }
            lines.push(currentLine.trim());
            return lines;
        }
        
        const tooltipTitleCallback = (tooltipItems) => {
            const item = tooltipItems[0];
            let label = item.chart.data.labels[item.dataIndex];
            if (Array.isArray(label)) { 
                return label.join(' ');
            }
            const currentTrans = translations[currentLanguage];
            if (typeof label === 'string' && currentTrans[label]) { 
                 return currentTrans[label];
            }
            return label; 
        };
        
        function getChartConfig() {
            const currentTrans = translations[currentLanguage];
            const chartLabels = [
                currentTrans.chart_mcVsTd_label_bias,
                currentTrans.chart_mcVsTd_label_variance,
                wrapLabels(currentTrans.chart_mcVsTd_label_updateFreq, 16),
                wrapLabels(currentTrans.chart_mcVsTd_label_incompleteEp, 16),
                wrapLabels(currentTrans.chart_mcVsTd_label_initSens, 16)
            ];

            return {
                type: 'bar',
                data: {
                    labels: chartLabels,
                    datasets: [{
                        label: currentTrans.chart_mcVsTd_dataset_mc,
                        data: [1, 5, 1, 1, 1],
                        backgroundColor: 'rgba(0, 123, 255, 0.7)',
                        borderColor: 'rgba(0, 123, 255, 1)',
                        borderWidth: 1
                    }, {
                        label: currentTrans.chart_mcVsTd_dataset_td,
                        data: [3, 2, 5, 5, 4],
                        backgroundColor: 'rgba(255, 193, 7, 0.7)',
                        borderColor: 'rgba(255, 193, 7, 1)',
                        borderWidth: 1
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: {
                        title: {
                            display: true,
                            text: currentTrans.chart_mcVsTd_title,
                            font: { size: 18, weight: 'bold' },
                            padding: { top: 10, bottom: 20 },
                            color: '#0056B3'
                        },
                        tooltip: {
                            callbacks: {
                               title: tooltipTitleCallback,
                               label: function(context) {
                                    let label = context.dataset.label || '';
                                    if (label) {
                                        label += ': ';
                                    }
                                    const characteristicIndex = context.dataIndex;
                                    const datasetLabel = context.dataset.label;

                                    if (datasetLabel === currentTrans.chart_mcVsTd_dataset_mc) { // MC
                                        if (characteristicIndex === 0) label += currentTrans.tooltip_bias_low_mc;
                                        else if (characteristicIndex === 1) label += currentTrans.tooltip_variance_high_mc;
                                        else if (characteristicIndex === 2) label += currentTrans.tooltip_update_episode_mc;
                                        else if (characteristicIndex === 3) label += currentTrans.tooltip_incomplete_no_mc;
                                        else if (characteristicIndex === 4) label += currentTrans.tooltip_init_insensitive_mc;
                                    } else if (datasetLabel === currentTrans.chart_mcVsTd_dataset_td) { // TD
                                        if (characteristicIndex === 0) label += currentTrans.tooltip_bias_medium_td;
                                        else if (characteristicIndex === 1) label += currentTrans.tooltip_variance_low_td;
                                        else if (characteristicIndex === 2) label += currentTrans.tooltip_update_step_td;
                                        else if (characteristicIndex === 3) label += currentTrans.tooltip_incomplete_yes_td;
                                        else if (characteristicIndex === 4) label += currentTrans.tooltip_init_sensitive_td;
                                    } else {
                                        label += context.raw; 
                                    }
                                    return label;
                                }
                            }
                        }
                    },
                    scales: {
                        y: {
                            beginAtZero: true,
                            max: 5.5,
                            ticks: {
                                stepSize: 1,
                                callback: function(value) {
                                    const currentTrans = translations[currentLanguage];
                                    switch(value) {
                                        case 1: return currentTrans.chart_mcVsTd_yAxis_lowNo;
                                        case 3: return currentTrans.chart_mcVsTd_yAxis_medium;
                                        case 5: return currentTrans.chart_mcVsTd_yAxis_highYes;
                                        default: return value === 0 || value === 2 || value === 4 ? String(value) : '';
                                    }
                                },
                               color: '#0056B3',
                               font: { weight: '600'}
                            },
                            grid: {
                                color: 'rgba(0, 86, 179, 0.15)'
                            }
                        },
                        x: {
                             ticks: { color: '#0056B3', font: { weight: '600'}},
                             grid: { display: false }
                        }
                    },
                    animation: {
                        duration: 1000,
                        easing: 'easeInOutQuart'
                    }
                }
            }
        };
        
        function updateChartLanguage() {
            if (chartInstance) {
                const config = getChartConfig();
                chartInstance.data.labels = config.data.labels;
                chartInstance.data.datasets[0].label = config.data.datasets[0].label;
                chartInstance.data.datasets[1].label = config.data.datasets[1].label;
                chartInstance.options.plugins.title.text = config.options.plugins.title.text;
                chartInstance.update();
            }
        }

        const mcVsTdCtx = document.getElementById('mcVsTdChart');
        if (mcVsTdCtx) {
            chartInstance = new Chart(mcVsTdCtx.getContext('2d'), getChartConfig());
        }


        // --- Quiz Logic ---
        const questionTextEl = document.getElementById('question-text');
        const optionsContainerEl = document.getElementById('options-container');
        const feedbackAreaEl = document.getElementById('quiz-feedback-area');
        const nextQuestionBtn = document.getElementById('next-question-btn');
        const quizResultsAreaEl = document.getElementById('quiz-results-area');
        const scoreTextEl = document.getElementById('score-text');
        const restartQuizBtn = document.getElementById('restart-quiz-btn');
        const questionCounterEl = document.getElementById('question-counter');
        const quizQuestionContainerEl = document.getElementById('quiz-question-container');

        let currentQuestionIndex = 0;
        let score = 0;
        let selectedOptionElement = null;
        let answerSubmitted = false;

        function loadQuestion() {
            answerSubmitted = false;
            selectedOptionElement = null;
            const currentQuestion = currentQuizData[currentQuestionIndex];
            questionTextEl.innerHTML = currentQuestion.question; 
            
            optionsContainerEl.innerHTML = '';
            currentQuestion.options.forEach((option, index) => {
                const optionEl = document.createElement('button');
                optionEl.classList.add('quiz-option');
                optionEl.innerHTML = option; 
                optionEl.dataset.index = index;
                optionEl.addEventListener('click', selectOption);
                optionsContainerEl.appendChild(optionEl);
            });

            feedbackAreaEl.style.display = 'none';
            nextQuestionBtn.disabled = true;
            nextQuestionBtn.classList.add('opacity-50', 'cursor-not-allowed');
            nextQuestionBtn.classList.remove('opacity-100');
            questionCounterEl.textContent = `${translations[currentLanguage].quiz_questionCounterPrefix} ${currentQuestionIndex + 1} ${translations[currentLanguage].quiz_questionCounterOf} ${currentQuizData.length}`;
            nextQuestionBtn.textContent = translations[currentLanguage].quiz_nextButton;


            if (window.MathJax && window.MathJax.typesetPromise) {
                window.MathJax.typesetPromise([questionTextEl, optionsContainerEl]).catch(function (err) { console.error('MathJax typesetting error:', err); });
            }
        }

        function selectOption(event) {
            if (answerSubmitted) return;

            const currentlySelected = optionsContainerEl.querySelector('.quiz-option.selected');
            if (currentlySelected) {
                currentlySelected.classList.remove('selected');
            }

            selectedOptionElement = event.target.closest('.quiz-option');
            selectedOptionElement.classList.add('selected');
            
            submitAnswer(); 
        }

        function submitAnswer() {
            if (!selectedOptionElement || answerSubmitted) return;
            answerSubmitted = true;

            const selectedAnswerIndex = parseInt(selectedOptionElement.dataset.index);
            const correctAnswerIndex = currentQuizData[currentQuestionIndex].answer;

            optionsContainerEl.childNodes.forEach(child => {
                child.disabled = true; 
                child.classList.remove('selected'); 
            });


            if (selectedAnswerIndex === correctAnswerIndex) {
                score++;
                feedbackAreaEl.textContent = translations[currentLanguage].quiz_feedbackCorrect;
                feedbackAreaEl.className = 'quiz-feedback feedback-correct';
                selectedOptionElement.classList.add('correct');
            } else {
                feedbackAreaEl.innerHTML = `${translations[currentLanguage].quiz_feedbackIncorrectPrefix} "${currentQuizData[currentQuestionIndex].options[correctAnswerIndex]}"`;
                feedbackAreaEl.className = 'quiz-feedback feedback-incorrect';
                selectedOptionElement.classList.add('incorrect');
                optionsContainerEl.childNodes[correctAnswerIndex].classList.add('correct');
            }
            feedbackAreaEl.style.display = 'block';
            if (window.MathJax && window.MathJax.typesetPromise) { 
                 window.MathJax.typesetPromise([feedbackAreaEl]).catch(function (err) { console.error('MathJax typesetting error:', err); });
            }

            nextQuestionBtn.disabled = false;
            nextQuestionBtn.classList.remove('opacity-50', 'cursor-not-allowed');
            nextQuestionBtn.classList.add('opacity-100');

            if (currentQuestionIndex === currentQuizData.length - 1) {
                nextQuestionBtn.textContent = translations[currentLanguage].quiz_resultsButton;
            }
        }


        nextQuestionBtn.addEventListener('click', () => {
            currentQuestionIndex++;
            if (currentQuestionIndex < currentQuizData.length) {
                loadQuestion();
            } else {
                showResults();
            }
        });

        restartQuizBtn.addEventListener('click', () => {
            currentQuestionIndex = 0;
            score = 0;
            quizResultsAreaEl.style.display = 'none';
            quizQuestionContainerEl.style.display = 'block';
            document.getElementById('quiz-navigation').style.display = 'flex'; 
            nextQuestionBtn.textContent = translations[currentLanguage].quiz_nextButton;
            loadQuestion();
        });

        function showResults() {
            quizQuestionContainerEl.style.display = 'none';
            feedbackAreaEl.style.display = 'none';
            document.getElementById('quiz-navigation').style.display = 'none'; 
            quizResultsAreaEl.style.display = 'block';
            
            const scoreSuffix = score === 1 ? translations[currentLanguage].quiz_scoreSuffix_singular : translations[currentLanguage].quiz_scoreSuffix_plural;
            scoreTextEl.textContent = `${score} ${translations[currentLanguage].quiz_scoreOutOf} ${currentQuizData.length} (${((score / currentQuizData.length) * 100).toFixed(0)}%)`;
        }

        document.addEventListener('DOMContentLoaded', () => {
            setLanguage(currentLanguage); 
            if (document.getElementById('quiz-area')) {
                 loadQuestion();
            }
        });

    </script>

</body>
</html>
